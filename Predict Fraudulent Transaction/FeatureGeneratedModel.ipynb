{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dj/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import random\n",
    "import operator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn import preprocessing, metrics, ensemble, neighbors, linear_model, tree, model_selection\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn import manifold, decomposition\n",
    "from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train=pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['transaction_id', 'num_var_1', 'num_var_2', 'num_var_3', 'num_var_4',\n",
       "       'num_var_5', 'num_var_6', 'num_var_7', 'cat_var_1', 'cat_var_2',\n",
       "       'cat_var_3', 'cat_var_4', 'cat_var_5', 'cat_var_6', 'cat_var_7',\n",
       "       'cat_var_8', 'cat_var_9', 'cat_var_10', 'cat_var_11', 'cat_var_12',\n",
       "       'cat_var_13', 'cat_var_14', 'cat_var_15', 'cat_var_16', 'cat_var_17',\n",
       "       'cat_var_18', 'cat_var_19', 'cat_var_20', 'cat_var_21', 'cat_var_22',\n",
       "       'cat_var_23', 'cat_var_24', 'cat_var_25', 'cat_var_26', 'cat_var_27',\n",
       "       'cat_var_28', 'cat_var_29', 'cat_var_30', 'cat_var_31', 'cat_var_32',\n",
       "       'cat_var_33', 'cat_var_34', 'cat_var_35', 'cat_var_36', 'cat_var_37',\n",
       "       'cat_var_38', 'cat_var_39', 'cat_var_40', 'cat_var_41', 'cat_var_42',\n",
       "       'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transaction_id    348978\n",
       "num_var_1          13385\n",
       "num_var_2           5550\n",
       "num_var_3              8\n",
       "num_var_4           1006\n",
       "num_var_5           4622\n",
       "num_var_6          11827\n",
       "num_var_7          26213\n",
       "cat_var_1            534\n",
       "cat_var_2             60\n",
       "cat_var_3            616\n",
       "cat_var_4              2\n",
       "cat_var_5              2\n",
       "cat_var_6            518\n",
       "cat_var_7             20\n",
       "cat_var_8            462\n",
       "cat_var_9              5\n",
       "cat_var_10            23\n",
       "cat_var_11             5\n",
       "cat_var_12             5\n",
       "cat_var_13            52\n",
       "cat_var_14            12\n",
       "cat_var_15             2\n",
       "cat_var_16             2\n",
       "cat_var_17             2\n",
       "cat_var_18             2\n",
       "cat_var_19             2\n",
       "cat_var_20             2\n",
       "cat_var_21             2\n",
       "cat_var_22             2\n",
       "cat_var_23             2\n",
       "cat_var_24             2\n",
       "cat_var_25             2\n",
       "cat_var_26             2\n",
       "cat_var_27             2\n",
       "cat_var_28             2\n",
       "cat_var_29             2\n",
       "cat_var_30             2\n",
       "cat_var_31             1\n",
       "cat_var_32             2\n",
       "cat_var_33             2\n",
       "cat_var_34             2\n",
       "cat_var_35             1\n",
       "cat_var_36             1\n",
       "cat_var_37             1\n",
       "cat_var_38             1\n",
       "cat_var_39             2\n",
       "cat_var_40             1\n",
       "cat_var_41             2\n",
       "cat_var_42             1\n",
       "target                 2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_y=train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat((train.loc[:,'num_var_1':'cat_var_42'],\n",
    "                      test.loc[:,'num_var_1':'cat_var_42']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#g = sns.distplot(all_data[\"num_var_7\"], color=\"m\", label=\"Skewness : %.2f\"%(all_data[\"num_var_7\"].skew()))\n",
    "#g.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ep    497666\n",
       "tn    374778\n",
       "Name: cat_var_18, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.cat_var_18.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_to_drop=['cat_var_24', 'cat_var_25', 'cat_var_26', 'cat_var_27', 'cat_var_28', 'cat_var_29', 'cat_var_30', \n",
    "             'cat_var_31', 'cat_var_32', 'cat_var_33', 'cat_var_34', 'cat_var_35', 'cat_var_36', 'cat_var_37', \n",
    "             'cat_var_38', 'cat_var_39', 'cat_var_40', 'cat_var_41','cat_var_42', 'num_var_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data=all_data.drop(col_to_drop, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['num_var_1', 'num_var_2', 'num_var_4', 'num_var_5', 'num_var_6',\n",
       "       'num_var_7', 'cat_var_1', 'cat_var_2', 'cat_var_3', 'cat_var_4',\n",
       "       'cat_var_5', 'cat_var_6', 'cat_var_7', 'cat_var_8', 'cat_var_9',\n",
       "       'cat_var_10', 'cat_var_11', 'cat_var_12', 'cat_var_13', 'cat_var_14',\n",
       "       'cat_var_15', 'cat_var_16', 'cat_var_17', 'cat_var_18', 'cat_var_19',\n",
       "       'cat_var_20', 'cat_var_21', 'cat_var_22', 'cat_var_23'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def na_checker(data):   \n",
    "    '''Check all columns in a dataframe and returns a list of columns to discard'''\n",
    "    #Initialize vectors\n",
    "    col_to_discard = []\n",
    "    col_to_fill = []\n",
    "    # Set conditions\n",
    "    for i in data.columns:\n",
    "        p = round(1-(float(data[i].count())/float(len(data[i]))),3) \n",
    "        if p > 0.75:\n",
    "            col_to_discard.append((p,i))\n",
    "        elif p <= 0.75 and p > 0:\n",
    "            col_to_fill.append((p,i))\n",
    "    # Print report\n",
    "    print(\"Discard following columns: \", col_to_discard)\n",
    "    print(\"Consider filling those columns: \", col_to_fill)\n",
    "    \n",
    "    # Return\n",
    "    col = list(map(lambda x: x[1],col_to_discard))\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def no_of_category(data):\n",
    "    no_of_category = []\n",
    "    for i in data.columns:\n",
    "        if(data.dtypes[i]=='O'):\n",
    "            no_of_category.append((i,len(data[i].unique())))\n",
    "            \n",
    "    print(pd.DataFrame(no_of_category))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data['cat_var_1']=all_data['cat_var_1'].fillna('Nan')\n",
    "all_data['cat_var_3']=all_data['cat_var_3'].fillna('Nan')\n",
    "all_data['cat_var_6']=all_data['cat_var_6'].fillna('Nan')\n",
    "all_data['cat_var_8']=all_data['cat_var_8'].fillna('Nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder=all_data.groupby('cat_var_1').size()\n",
    "encoder=encoder/len(all_data)\n",
    "all_data['cat_var_1_freq']=all_data.cat_var_1.map(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder=all_data.groupby('cat_var_2').size()\n",
    "encoder=encoder/len(all_data)\n",
    "all_data['cat_var_2_freq']=all_data.cat_var_2.map(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder=all_data.groupby('cat_var_3').size()\n",
    "encoder=encoder/len(all_data)\n",
    "all_data['cat_var_3_freq']=all_data.cat_var_3.map(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder=all_data.groupby('cat_var_6').size()\n",
    "encoder=encoder/len(all_data)\n",
    "all_data['cat_var_6_freq']=all_data.cat_var_6.map(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder=all_data.groupby('cat_var_7').size()\n",
    "encoder=encoder/len(all_data)\n",
    "all_data['cat_var_7_freq']=all_data.cat_var_7.map(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder=all_data.groupby('cat_var_8').size()\n",
    "encoder=encoder/len(all_data)\n",
    "all_data['cat_var_8_freq']=all_data.cat_var_8.map(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder=all_data.groupby('cat_var_10').size()\n",
    "encoder=encoder/len(all_data)\n",
    "all_data['cat_var_10_freq']=all_data.cat_var_10.map(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder=all_data.groupby('cat_var_13').size()\n",
    "encoder=encoder/len(all_data)\n",
    "all_data['cat_var_13_freq']=all_data.cat_var_13.map(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder=all_data.groupby('cat_var_14').size()\n",
    "encoder=encoder/len(all_data)\n",
    "all_data['cat_var_14_freq']=all_data.cat_var_14.map(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_var_1</th>\n",
       "      <th>num_var_2</th>\n",
       "      <th>num_var_4</th>\n",
       "      <th>num_var_5</th>\n",
       "      <th>num_var_6</th>\n",
       "      <th>num_var_7</th>\n",
       "      <th>cat_var_1</th>\n",
       "      <th>cat_var_2</th>\n",
       "      <th>cat_var_3</th>\n",
       "      <th>cat_var_4</th>\n",
       "      <th>...</th>\n",
       "      <th>cat_var_23</th>\n",
       "      <th>cat_var_1_freq</th>\n",
       "      <th>cat_var_2_freq</th>\n",
       "      <th>cat_var_3_freq</th>\n",
       "      <th>cat_var_6_freq</th>\n",
       "      <th>cat_var_7_freq</th>\n",
       "      <th>cat_var_8_freq</th>\n",
       "      <th>cat_var_10_freq</th>\n",
       "      <th>cat_var_13_freq</th>\n",
       "      <th>cat_var_14_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.302632e-08</td>\n",
       "      <td>0.040182</td>\n",
       "      <td>1.800000e-07</td>\n",
       "      <td>2.302632e-08</td>\n",
       "      <td>2.368421e-08</td>\n",
       "      <td>1.115205e-08</td>\n",
       "      <td>Nan</td>\n",
       "      <td>ce</td>\n",
       "      <td>db</td>\n",
       "      <td>ep</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.039514</td>\n",
       "      <td>0.667156</td>\n",
       "      <td>0.023058</td>\n",
       "      <td>0.016274</td>\n",
       "      <td>0.998761</td>\n",
       "      <td>0.135110</td>\n",
       "      <td>0.030682</td>\n",
       "      <td>0.025584</td>\n",
       "      <td>0.711649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.965789e-06</td>\n",
       "      <td>0.157872</td>\n",
       "      <td>2.105000e-06</td>\n",
       "      <td>2.769737e-07</td>\n",
       "      <td>7.965789e-06</td>\n",
       "      <td>2.433058e-06</td>\n",
       "      <td>da</td>\n",
       "      <td>tn</td>\n",
       "      <td>zl</td>\n",
       "      <td>tn</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.295931</td>\n",
       "      <td>0.009840</td>\n",
       "      <td>0.289718</td>\n",
       "      <td>0.998761</td>\n",
       "      <td>0.135110</td>\n",
       "      <td>0.036715</td>\n",
       "      <td>0.299744</td>\n",
       "      <td>0.711649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.828947e-08</td>\n",
       "      <td>0.089140</td>\n",
       "      <td>3.550000e-07</td>\n",
       "      <td>4.671053e-08</td>\n",
       "      <td>1.052632e-07</td>\n",
       "      <td>4.276014e-07</td>\n",
       "      <td>gf</td>\n",
       "      <td>ce</td>\n",
       "      <td>gs</td>\n",
       "      <td>tn</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.379122</td>\n",
       "      <td>0.667156</td>\n",
       "      <td>0.004132</td>\n",
       "      <td>0.378800</td>\n",
       "      <td>0.998761</td>\n",
       "      <td>0.379122</td>\n",
       "      <td>0.092515</td>\n",
       "      <td>0.380945</td>\n",
       "      <td>0.711649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.894737e-08</td>\n",
       "      <td>0.227239</td>\n",
       "      <td>1.050000e-06</td>\n",
       "      <td>1.381579e-07</td>\n",
       "      <td>2.190789e-07</td>\n",
       "      <td>1.848054e-08</td>\n",
       "      <td>Nan</td>\n",
       "      <td>ce</td>\n",
       "      <td>fy</td>\n",
       "      <td>ep</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.039514</td>\n",
       "      <td>0.667156</td>\n",
       "      <td>0.002698</td>\n",
       "      <td>0.005679</td>\n",
       "      <td>0.998761</td>\n",
       "      <td>0.019988</td>\n",
       "      <td>0.029980</td>\n",
       "      <td>0.016051</td>\n",
       "      <td>0.034106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.321053e-06</td>\n",
       "      <td>0.160410</td>\n",
       "      <td>2.105000e-06</td>\n",
       "      <td>2.769737e-07</td>\n",
       "      <td>3.340789e-06</td>\n",
       "      <td>2.152983e-06</td>\n",
       "      <td>da</td>\n",
       "      <td>tn</td>\n",
       "      <td>zn</td>\n",
       "      <td>tn</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.290023</td>\n",
       "      <td>0.295931</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.289718</td>\n",
       "      <td>0.998761</td>\n",
       "      <td>0.135110</td>\n",
       "      <td>0.049561</td>\n",
       "      <td>0.299744</td>\n",
       "      <td>0.711649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      num_var_1  num_var_2     num_var_4     num_var_5     num_var_6  \\\n",
       "0  2.302632e-08   0.040182  1.800000e-07  2.302632e-08  2.368421e-08   \n",
       "1  7.965789e-06   0.157872  2.105000e-06  2.769737e-07  7.965789e-06   \n",
       "2  7.828947e-08   0.089140  3.550000e-07  4.671053e-08  1.052632e-07   \n",
       "3  7.894737e-08   0.227239  1.050000e-06  1.381579e-07  2.190789e-07   \n",
       "4  3.321053e-06   0.160410  2.105000e-06  2.769737e-07  3.340789e-06   \n",
       "\n",
       "      num_var_7 cat_var_1 cat_var_2 cat_var_3 cat_var_4       ...        \\\n",
       "0  1.115205e-08       Nan        ce        db        ep       ...         \n",
       "1  2.433058e-06        da        tn        zl        tn       ...         \n",
       "2  4.276014e-07        gf        ce        gs        tn       ...         \n",
       "3  1.848054e-08       Nan        ce        fy        ep       ...         \n",
       "4  2.152983e-06        da        tn        zn        tn       ...         \n",
       "\n",
       "  cat_var_23 cat_var_1_freq cat_var_2_freq cat_var_3_freq cat_var_6_freq  \\\n",
       "0          0       0.039514       0.667156       0.023058       0.016274   \n",
       "1          0       0.290023       0.295931       0.009840       0.289718   \n",
       "2          0       0.379122       0.667156       0.004132       0.378800   \n",
       "3          0       0.039514       0.667156       0.002698       0.005679   \n",
       "4          0       0.290023       0.295931       0.000298       0.289718   \n",
       "\n",
       "  cat_var_7_freq cat_var_8_freq cat_var_10_freq cat_var_13_freq  \\\n",
       "0       0.998761       0.135110        0.030682        0.025584   \n",
       "1       0.998761       0.135110        0.036715        0.299744   \n",
       "2       0.998761       0.379122        0.092515        0.380945   \n",
       "3       0.998761       0.019988        0.029980        0.016051   \n",
       "4       0.998761       0.135110        0.049561        0.299744   \n",
       "\n",
       "  cat_var_14_freq  \n",
       "0        0.711649  \n",
       "1        0.711649  \n",
       "2        0.711649  \n",
       "3        0.034106  \n",
       "4        0.711649  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder(data):\n",
    "    '''Map the categorical variables to numbers to work with scikit learn'''\n",
    "    for col in data.columns:\n",
    "        if data.dtypes[col] == \"O\":\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            data[col]=le.fit_transform(data[col])\n",
    "            #data[col] = le.transform(data[col])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data=encoder(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = all_data[:len(train)]\n",
    "test_X = all_data[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_id=test['transaction_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_feature_map(features):\n",
    "    outfile = open('xgb.fmap', 'w')\n",
    "    for i, feat in enumerate(features):\n",
    "        outfile.write('{0}\\t{1}\\tq\\n'.format(i,feat))\n",
    "    outfile.close()\n",
    "\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, feature_names=None, seed_val=0, rounds=500, dep=10, eta=0.2):\n",
    "    params = {}\n",
    "    params[\"objective\"] = \"binary:logistic\"\n",
    "    params['eval_metric'] = 'auc'\n",
    "    params[\"eta\"] = eta\n",
    "    params[\"subsample\"] = 0.7\n",
    "    params[\"min_child_weight\"] = 1\n",
    "    params[\"colsample_bytree\"] = 0.7\n",
    "    params[\"max_depth\"] = dep\n",
    "\n",
    "    params[\"silent\"] = 1\n",
    "    params[\"seed\"] = seed_val\n",
    "    #params[\"max_delta_step\"] = 2\n",
    "    #params[\"gamma\"] = 0.5\n",
    "    num_rounds = rounds\n",
    "\n",
    "    plst = list(params.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=100, verbose_eval=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    if feature_names is not None:\n",
    "        create_feature_map(feature_names)\n",
    "        model.dump_model('xgbmodel.txt', 'xgb.fmap', with_stats=True)\n",
    "        importance = model.get_fscore(fmap='xgb.fmap')\n",
    "        importance = sorted(importance.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        imp_df = pd.DataFrame(importance, columns=['feature','fscore'])\n",
    "        imp_df['fscore'] = imp_df['fscore'] / imp_df['fscore'].sum()\n",
    "        imp_df.to_csv(\"imp_feat.txt\", index=False)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest, ntree_limit=model.best_ntree_limit)\n",
    "    pred_test_y2 = model.predict(xgb.DMatrix(test_X2), ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "    loss = 0\n",
    "    if test_y is not None:\n",
    "        loss = metrics.roc_auc_score(test_y, pred_test_y)\n",
    "        return pred_test_y, loss, pred_test_y2\n",
    "    else:\n",
    "        return pred_test_y, loss, pred_test_y2\n",
    "\n",
    "def runLGB(train_X, train_y, test_X, test_y=None, test_X2=None, feature_names=None, seed_val=0, rounds=500, dep=10, eta=0.2):\n",
    "    params = {}\n",
    "    params[\"objective\"] = \"binary\"\n",
    "    params['metric'] = 'auc'\n",
    "    params[\"max_depth\"] = dep\n",
    "    params[\"min_data_in_leaf\"] = 64\n",
    "    params[\"learning_rate\"] = eta\n",
    "    params[\"bagging_fraction\"] = 0.7\n",
    "    params[\"feature_fraction\"] = 0.7\n",
    "    params[\"bagging_freq\"] = 5\n",
    "    params[\"bagging_seed\"] = seed_val\n",
    "    params[\"verbosity\"] = 0\n",
    "    #params[\"reg_alpha\"] = 20\n",
    "    #params[\"reg_lambda\"] = 20\n",
    "    num_rounds = rounds\n",
    "\n",
    "    plst = list(params.items())\n",
    "    lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        lgtest = lgb.Dataset(test_X, label=test_y)\n",
    "        model = lgb.train(params, lgtrain, num_rounds, valid_sets=[lgtest], early_stopping_rounds=100, verbose_eval=20)\n",
    "    else:\n",
    "        lgtest = lgb.DMatrix(test_X)\n",
    "        model = lgb.train(params, lgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n",
    "\n",
    "    loss = 0\n",
    "    if test_y is not None:\n",
    "        loss = metrics.roc_auc_score(test_y, pred_test_y)\n",
    "        print(loss)\n",
    "        return pred_test_y, loss, pred_test_y2\n",
    "    else:\n",
    "        return pred_test_y, loss, pred_test_y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model building..\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[20]\tvalid_0's auc: 0.729034\n",
      "[40]\tvalid_0's auc: 0.731389\n",
      "[60]\tvalid_0's auc: 0.730362\n",
      "[80]\tvalid_0's auc: 0.730175\n",
      "[100]\tvalid_0's auc: 0.733212\n",
      "[120]\tvalid_0's auc: 0.73054\n",
      "[140]\tvalid_0's auc: 0.730753\n",
      "[160]\tvalid_0's auc: 0.7314\n",
      "[180]\tvalid_0's auc: 0.73138\n",
      "[200]\tvalid_0's auc: 0.730942\n",
      "Early stopping, best iteration is:\n",
      "[100]\tvalid_0's auc: 0.733212\n",
      "0.733211934982\n",
      "[0.73321193498178339]\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[20]\tvalid_0's auc: 0.728689\n",
      "[40]\tvalid_0's auc: 0.728316\n",
      "[60]\tvalid_0's auc: 0.730217\n",
      "[80]\tvalid_0's auc: 0.729941\n",
      "[100]\tvalid_0's auc: 0.732037\n",
      "[120]\tvalid_0's auc: 0.731101\n",
      "[140]\tvalid_0's auc: 0.730531\n",
      "[160]\tvalid_0's auc: 0.730056\n",
      "[180]\tvalid_0's auc: 0.728113\n",
      "[200]\tvalid_0's auc: 0.728602\n",
      "Early stopping, best iteration is:\n",
      "[102]\tvalid_0's auc: 0.732155\n",
      "0.732154923006\n",
      "[0.73321193498178339, 0.73215492300611085]\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[20]\tvalid_0's auc: 0.730343\n",
      "[40]\tvalid_0's auc: 0.731455\n",
      "[60]\tvalid_0's auc: 0.731163\n",
      "[80]\tvalid_0's auc: 0.730735\n",
      "[100]\tvalid_0's auc: 0.730819\n",
      "[120]\tvalid_0's auc: 0.728884\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid_0's auc: 0.731924\n",
      "0.73192449098\n",
      "[0.73321193498178339, 0.73215492300611085, 0.73192449097974577]\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[20]\tvalid_0's auc: 0.724908\n",
      "[40]\tvalid_0's auc: 0.726384\n",
      "[60]\tvalid_0's auc: 0.725625\n",
      "[80]\tvalid_0's auc: 0.724748\n",
      "[100]\tvalid_0's auc: 0.724177\n",
      "[120]\tvalid_0's auc: 0.724067\n",
      "Early stopping, best iteration is:\n",
      "[31]\tvalid_0's auc: 0.727064\n",
      "0.727063758811\n",
      "[0.73321193498178339, 0.73215492300611085, 0.73192449097974577, 0.72706375881083929]\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[20]\tvalid_0's auc: 0.725213\n",
      "[40]\tvalid_0's auc: 0.726224\n",
      "[60]\tvalid_0's auc: 0.726396\n",
      "[80]\tvalid_0's auc: 0.725069\n",
      "[100]\tvalid_0's auc: 0.725755\n",
      "[120]\tvalid_0's auc: 0.726596\n",
      "[140]\tvalid_0's auc: 0.725765\n",
      "[160]\tvalid_0's auc: 0.725391\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid_0's auc: 0.727367\n",
      "0.727367162715\n",
      "[0.73321193498178339, 0.73215492300611085, 0.73192449097974577, 0.72706375881083929, 0.72736716271514135]\n",
      "0.730443845627\n",
      "Model building..\n",
      "[0]\ttrain-auc:0.701043\ttest-auc:0.699736\n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 100 rounds.\n",
      "[20]\ttrain-auc:0.782834\ttest-auc:0.734017\n",
      "[40]\ttrain-auc:0.823356\ttest-auc:0.731565\n",
      "[60]\ttrain-auc:0.854091\ttest-auc:0.729969\n",
      "[80]\ttrain-auc:0.881931\ttest-auc:0.731874\n",
      "[100]\ttrain-auc:0.900413\ttest-auc:0.731868\n",
      "[120]\ttrain-auc:0.912594\ttest-auc:0.732639\n",
      "Stopping. Best iteration:\n",
      "[21]\ttrain-auc:0.785302\ttest-auc:0.734223\n",
      "\n",
      "[0.73422323475143214]\n",
      "[0]\ttrain-auc:0.701608\ttest-auc:0.697055\n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 100 rounds.\n",
      "[20]\ttrain-auc:0.788083\ttest-auc:0.725993\n",
      "[40]\ttrain-auc:0.829006\ttest-auc:0.727389\n",
      "[60]\ttrain-auc:0.856776\ttest-auc:0.726857\n",
      "[80]\ttrain-auc:0.881837\ttest-auc:0.723453\n",
      "[100]\ttrain-auc:0.901503\ttest-auc:0.723794\n",
      "[120]\ttrain-auc:0.914995\ttest-auc:0.72286\n",
      "Stopping. Best iteration:\n",
      "[38]\ttrain-auc:0.825089\ttest-auc:0.72805\n",
      "\n",
      "[0.73422323475143214, 0.72805025565902404]\n",
      "[0]\ttrain-auc:0.702897\ttest-auc:0.701667\n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 100 rounds.\n",
      "[20]\ttrain-auc:0.789764\ttest-auc:0.731697\n",
      "[40]\ttrain-auc:0.831072\ttest-auc:0.730788\n",
      "[60]\ttrain-auc:0.864245\ttest-auc:0.731978\n",
      "[80]\ttrain-auc:0.890757\ttest-auc:0.732016\n",
      "[100]\ttrain-auc:0.906525\ttest-auc:0.733708\n",
      "[120]\ttrain-auc:0.917869\ttest-auc:0.732238\n",
      "[140]\ttrain-auc:0.929226\ttest-auc:0.731451\n",
      "[160]\ttrain-auc:0.93802\ttest-auc:0.731026\n",
      "[180]\ttrain-auc:0.945548\ttest-auc:0.730329\n",
      "Stopping. Best iteration:\n",
      "[91]\ttrain-auc:0.90049\ttest-auc:0.734168\n",
      "\n",
      "[0.73422323475143214, 0.72805025565902404, 0.73416825129206942]\n",
      "[0]\ttrain-auc:0.72547\ttest-auc:0.719744\n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 100 rounds.\n",
      "[20]\ttrain-auc:0.788389\ttest-auc:0.726653\n",
      "[40]\ttrain-auc:0.83726\ttest-auc:0.727685\n",
      "[60]\ttrain-auc:0.868771\ttest-auc:0.728289\n",
      "[80]\ttrain-auc:0.888755\ttest-auc:0.728833\n",
      "[100]\ttrain-auc:0.904963\ttest-auc:0.730071\n",
      "[120]\ttrain-auc:0.916363\ttest-auc:0.728283\n",
      "[140]\ttrain-auc:0.928425\ttest-auc:0.727873\n",
      "[160]\ttrain-auc:0.940425\ttest-auc:0.72646\n",
      "[180]\ttrain-auc:0.947916\ttest-auc:0.726259\n",
      "Stopping. Best iteration:\n",
      "[94]\ttrain-auc:0.899452\ttest-auc:0.730547\n",
      "\n",
      "[0.73422323475143214, 0.72805025565902404, 0.73416825129206942, 0.73054696467631342]\n",
      "[0]\ttrain-auc:0.72556\ttest-auc:0.721139\n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 100 rounds.\n",
      "[20]\ttrain-auc:0.788573\ttest-auc:0.727386\n",
      "[40]\ttrain-auc:0.82954\ttest-auc:0.728553\n",
      "[60]\ttrain-auc:0.859626\ttest-auc:0.729217\n",
      "[80]\ttrain-auc:0.884548\ttest-auc:0.729917\n",
      "[100]\ttrain-auc:0.900504\ttest-auc:0.728188\n",
      "[120]\ttrain-auc:0.914652\ttest-auc:0.728055\n",
      "[140]\ttrain-auc:0.926192\ttest-auc:0.727517\n",
      "[160]\ttrain-auc:0.936252\ttest-auc:0.726658\n",
      "Stopping. Best iteration:\n",
      "[64]\ttrain-auc:0.864898\ttest-auc:0.730972\n",
      "\n",
      "[0.73422323475143214, 0.72805025565902404, 0.73416825129206942, 0.73054696467631342, 0.73097247091121909]\n",
      "0.731138237255\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    #model_name = \"ET\"\n",
    "    for model_name in [\"LGB1\", \"XGB1\"]:\n",
    "\n",
    "        print(\"Model building..\")\n",
    "        kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "        cv_scores = []\n",
    "        pred_test_full = 0\n",
    "        pred_val_full = np.zeros(train_X.shape[0])\n",
    "        for dev_index, val_index in kf.split(train_X, train_y):\n",
    "            dev_X, val_X = train_X.iloc[dev_index,:], train_X.iloc[val_index,:]\n",
    "            dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "\n",
    "            if model_name == \"XGB1\":\n",
    "                pred_val, loss, pred_test = runXGB(dev_X, dev_y, val_X, val_y, test_X, rounds=5000, feature_names=dev_X.columns.tolist())\n",
    "            elif model_name == \"LGB1\":\n",
    "                pred_val, loss, pred_test = runLGB(dev_X, dev_y, val_X, val_y, test_X, rounds=5000)\n",
    "            pred_val_full[val_index] = pred_val\n",
    "            pred_test_full = pred_test_full + pred_test\n",
    "            cv_scores.append(loss)\n",
    "            print(cv_scores)\n",
    "        pred_test_full /= 5.\n",
    "        print(metrics.roc_auc_score(train_y, pred_val_full))\n",
    "\n",
    "        out_df = pd.DataFrame({\"transaction_id\":test_id})\n",
    "        out_df[\"target\"] = pred_test_full\n",
    "        out_df.to_csv(\"pred_test_v5_\"+model_name+\".csv\", index=False)\n",
    "\n",
    "        #out_df = pd.DataFrame({\"transaction_id\":train_id})\n",
    "        #out_df[\"target\"] = pred_val_full\n",
    "        #out_df.to_csv(\"pred_val_v5_\"+model_name+\".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#XGB 0.73633  0.731481622839              0.73463  0.731768245692\n",
    "params[\"objective\"] = \"binary:logistic\"\n",
    "params['eval_metric'] = 'auc'\n",
    "params[\"eta\"] = 0.2\n",
    "params[\"subsample\"] = 0.7\n",
    "params[\"min_child_weight\"] = 1\n",
    "params[\"colsample_bytree\"] = 0.7\n",
    "params[\"max_depth\"] = 10\n",
    "params[\"silent\"] = 1\n",
    "#XGB 0.73594  0.730887790006\n",
    "params[\"objective\"] = \"binary:logistic\"\n",
    "params['eval_metric'] = 'auc'\n",
    "params[\"eta\"] = 0.1\n",
    "params[\"subsample\"] = 0.7\n",
    "params[\"min_child_weight\"] = 1\n",
    "params[\"colsample_bytree\"] = 0.7\n",
    "params[\"max_depth\"] = 8\n",
    "params[\"silent\"] = 1\n",
    "params[\"seed\"] = seed_val\n",
    "#LGM 0.73414  0.731143283655\n",
    "params[\"objective\"] = \"binary\"\n",
    "params['metric'] = 'auc'\n",
    "params[\"max_depth\"] = 8\n",
    "params[\"min_data_in_leaf\"] = 128\n",
    "params[\"learning_rate\"] = 0.1\n",
    "params[\"bagging_fraction\"] = 0.7\n",
    "params[\"feature_fraction\"] = 0.7\n",
    "params[\"bagging_freq\"] = 5\n",
    "params[\"bagging_seed\"] = seed_val\n",
    "#LGM 0.73422  0.731471583671\n",
    "params[\"objective\"] = \"binary\"\n",
    "params['metric'] = 'auc'\n",
    "params[\"max_depth\"] = 10\n",
    "params[\"min_data_in_leaf\"] = 128\n",
    "params[\"learning_rate\"] = 0.1\n",
    "params[\"bagging_fraction\"] = 0.7\n",
    "params[\"feature_fraction\"] = 0.7\n",
    "params[\"bagging_freq\"] = 5\n",
    "params[\"bagging_seed\"] = seed_val\n",
    "#LGM   0.731282373295\n",
    "params[\"objective\"] = \"binary\"\n",
    "params['metric'] = 'auc'\n",
    "params[\"max_depth\"] = 10\n",
    "params[\"min_data_in_leaf\"] = 128\n",
    "params[\"learning_rate\"] = 0.2\n",
    "params[\"bagging_fraction\"] = 0.7\n",
    "params[\"feature_fraction\"] = 0.7\n",
    "params[\"bagging_freq\"] = 5\n",
    "params[\"bagging_seed\"] = seed_val\n",
    "#LGM   0.732104622404                        0.73373   0.730939456999\n",
    "params[\"objective\"] = \"binary\"\n",
    "params['metric'] = 'auc'\n",
    "params[\"max_depth\"] = 10\n",
    "params[\"min_data_in_leaf\"] = 64\n",
    "params[\"learning_rate\"] = 0.1\n",
    "params[\"bagging_fraction\"] = 0.7\n",
    "params[\"feature_fraction\"] = 0.7\n",
    "params[\"bagging_freq\"] = 5\n",
    "params[\"bagging_seed\"] = seed_val\n",
    "#LGM                                         0.73439      0.732129309669\n",
    "params[\"objective\"] = \"binary\"\n",
    "params['metric'] = 'auc'\n",
    "params[\"max_depth\"] = 10\n",
    "params[\"min_data_in_leaf\"] = 64\n",
    "params[\"learning_rate\"] = 0.2\n",
    "params[\"bagging_fraction\"] = 0.7\n",
    "params[\"feature_fraction\"] = 0.7\n",
    "params[\"bagging_freq\"] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dj/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.4 of module '_catboost' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "def runCatB(train_X, train_y, test_X, test_y=None, test_X2=None, depth=8):\n",
    "    model = CatBoostClassifier(\n",
    "                                iterations = 1000,\n",
    "                                learning_rate = 0.2,\n",
    "                                depth = depth,\n",
    "                                od_type='Iter',\n",
    "                                od_wait=100,\n",
    "                                rsm=0.7,\n",
    "                                l2_leaf_reg=3,\n",
    "                                eval_metric = 'AUC', \n",
    "                                verbose=True,\n",
    "                                random_seed=42)\n",
    "    \n",
    "    model.fit(train_X, train_y, eval_set=(test_X, test_y))#, plot=True)\n",
    "    train_preds = model.predict_proba(train_X)[:,1]\n",
    "    test_preds = model.predict_proba(test_X)[:,1]\n",
    "    test_preds2 = model.predict_proba(test_X2)[:,1]\n",
    "    test_loss = 0\n",
    "    if test_y is not None:\n",
    "        train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "        test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "        print(\"Depth : \", depth)\n",
    "        print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dj/anaconda3/lib/python3.6/site-packages/catboost/core.py:1240: FutureWarning: The 'verbose' parameter is deprecated, use 'logging_level' parameter instead (posible values: 'Silent', 'Verbose', 'Info', 'Debug').\n",
      "  super(CatBoostClassifier, self).__init__(params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: learn: 0.690253\ttest: 0.6872842\tbestTest: 0.6872842 (0)\ttotal: 2.6s\tremaining: 43m 12s\n",
      "1: learn: 0.7067968\ttest: 0.705925\tbestTest: 0.705925 (1)\ttotal: 3.1s\tremaining: 25m 45s\n",
      "2: learn: 0.7127781\ttest: 0.7098438\tbestTest: 0.7098438 (2)\ttotal: 3.63s\tremaining: 20m 5s\n",
      "3: learn: 0.7132002\ttest: 0.7113407\tbestTest: 0.7113407 (3)\ttotal: 4.27s\tremaining: 17m 43s\n",
      "4: learn: 0.7166818\ttest: 0.7145716\tbestTest: 0.7145716 (4)\ttotal: 4.86s\tremaining: 16m 7s\n",
      "5: learn: 0.7215131\ttest: 0.7192641\tbestTest: 0.7192641 (5)\ttotal: 5.42s\tremaining: 14m 57s\n",
      "6: learn: 0.7220094\ttest: 0.7192931\tbestTest: 0.7192931 (6)\ttotal: 6.1s\tremaining: 14m 25s\n",
      "7: learn: 0.7235972\ttest: 0.7200475\tbestTest: 0.7200475 (7)\ttotal: 6.78s\tremaining: 14m\n",
      "8: learn: 0.7268306\ttest: 0.7220191\tbestTest: 0.7220191 (8)\ttotal: 7.35s\tremaining: 13m 29s\n",
      "9: learn: 0.7278768\ttest: 0.724376\tbestTest: 0.724376 (9)\ttotal: 8.02s\tremaining: 13m 14s\n",
      "10: learn: 0.7282133\ttest: 0.724835\tbestTest: 0.724835 (10)\ttotal: 8.69s\tremaining: 13m 1s\n",
      "11: learn: 0.7289802\ttest: 0.7257961\tbestTest: 0.7257961 (11)\ttotal: 9.33s\tremaining: 12m 48s\n",
      "12: learn: 0.7296082\ttest: 0.7265681\tbestTest: 0.7265681 (12)\ttotal: 9.91s\tremaining: 12m 32s\n",
      "13: learn: 0.7298311\ttest: 0.726608\tbestTest: 0.726608 (13)\ttotal: 10.5s\tremaining: 12m 17s\n",
      "14: learn: 0.7300517\ttest: 0.7265715\tbestTest: 0.726608 (13)\ttotal: 11s\tremaining: 12m 1s\n",
      "15: learn: 0.7302551\ttest: 0.7263861\tbestTest: 0.726608 (13)\ttotal: 11.6s\tremaining: 11m 56s\n",
      "16: learn: 0.7303524\ttest: 0.7263423\tbestTest: 0.726608 (13)\ttotal: 12.2s\tremaining: 11m 43s\n",
      "17: learn: 0.73037\ttest: 0.7266641\tbestTest: 0.7266641 (17)\ttotal: 12.8s\tremaining: 11m 36s\n",
      "18: learn: 0.7310452\ttest: 0.7270644\tbestTest: 0.7270644 (18)\ttotal: 13.4s\tremaining: 11m 33s\n",
      "19: learn: 0.7311887\ttest: 0.7273822\tbestTest: 0.7273822 (19)\ttotal: 13.9s\tremaining: 11m 20s\n",
      "20: learn: 0.7313523\ttest: 0.7273883\tbestTest: 0.7273883 (20)\ttotal: 14.5s\tremaining: 11m 16s\n",
      "21: learn: 0.7314224\ttest: 0.7270604\tbestTest: 0.7273883 (20)\ttotal: 15s\tremaining: 11m 7s\n",
      "22: learn: 0.7316053\ttest: 0.7272711\tbestTest: 0.7273883 (20)\ttotal: 15.6s\tremaining: 11m 2s\n",
      "23: learn: 0.7318699\ttest: 0.727739\tbestTest: 0.727739 (23)\ttotal: 16.2s\tremaining: 10m 57s\n",
      "24: learn: 0.7319974\ttest: 0.7277024\tbestTest: 0.727739 (23)\ttotal: 16.8s\tremaining: 10m 55s\n",
      "25: learn: 0.7322334\ttest: 0.7277051\tbestTest: 0.727739 (23)\ttotal: 17.3s\tremaining: 10m 48s\n",
      "26: learn: 0.7325602\ttest: 0.727711\tbestTest: 0.727739 (23)\ttotal: 17.9s\tremaining: 10m 44s\n",
      "27: learn: 0.7335414\ttest: 0.7284639\tbestTest: 0.7284639 (27)\ttotal: 18.4s\tremaining: 10m 40s\n",
      "28: learn: 0.7341675\ttest: 0.7288082\tbestTest: 0.7288082 (28)\ttotal: 19s\tremaining: 10m 37s\n",
      "29: learn: 0.7347548\ttest: 0.72907\tbestTest: 0.72907 (29)\ttotal: 19.6s\tremaining: 10m 33s\n",
      "30: learn: 0.7350057\ttest: 0.7294696\tbestTest: 0.7294696 (30)\ttotal: 20.2s\tremaining: 10m 32s\n",
      "31: learn: 0.7353878\ttest: 0.7295935\tbestTest: 0.7295935 (31)\ttotal: 20.8s\tremaining: 10m 29s\n",
      "32: learn: 0.7359434\ttest: 0.7302558\tbestTest: 0.7302558 (32)\ttotal: 21.3s\tremaining: 10m 25s\n",
      "33: learn: 0.7359943\ttest: 0.7303804\tbestTest: 0.7303804 (33)\ttotal: 21.9s\tremaining: 10m 22s\n",
      "34: learn: 0.7363175\ttest: 0.7302318\tbestTest: 0.7303804 (33)\ttotal: 22.5s\tremaining: 10m 20s\n",
      "35: learn: 0.7370195\ttest: 0.7297837\tbestTest: 0.7303804 (33)\ttotal: 23.1s\tremaining: 10m 19s\n",
      "36: learn: 0.7370819\ttest: 0.7297235\tbestTest: 0.7303804 (33)\ttotal: 23.6s\tremaining: 10m 14s\n",
      "37: learn: 0.7373316\ttest: 0.7297125\tbestTest: 0.7303804 (33)\ttotal: 24.3s\tremaining: 10m 14s\n",
      "38: learn: 0.7375246\ttest: 0.7299564\tbestTest: 0.7303804 (33)\ttotal: 24.8s\tremaining: 10m 11s\n",
      "39: learn: 0.737622\ttest: 0.7302238\tbestTest: 0.7303804 (33)\ttotal: 25.3s\tremaining: 10m 6s\n",
      "40: learn: 0.7375504\ttest: 0.730276\tbestTest: 0.7303804 (33)\ttotal: 25.8s\tremaining: 10m 3s\n",
      "41: learn: 0.7377361\ttest: 0.7301281\tbestTest: 0.7303804 (33)\ttotal: 26.4s\tremaining: 10m 2s\n",
      "42: learn: 0.7379109\ttest: 0.7305093\tbestTest: 0.7305093 (42)\ttotal: 27s\tremaining: 10m\n",
      "43: learn: 0.7380461\ttest: 0.730479\tbestTest: 0.7305093 (42)\ttotal: 27.5s\tremaining: 9m 56s\n",
      "44: learn: 0.7383792\ttest: 0.730509\tbestTest: 0.7305093 (42)\ttotal: 28s\tremaining: 9m 55s\n",
      "45: learn: 0.7386956\ttest: 0.7310805\tbestTest: 0.7310805 (45)\ttotal: 28.6s\tremaining: 9m 53s\n",
      "46: learn: 0.7388152\ttest: 0.7311193\tbestTest: 0.7311193 (46)\ttotal: 29.2s\tremaining: 9m 52s\n",
      "47: learn: 0.7391213\ttest: 0.7311151\tbestTest: 0.7311193 (46)\ttotal: 29.9s\tremaining: 9m 52s\n",
      "48: learn: 0.7393396\ttest: 0.7313865\tbestTest: 0.7313865 (48)\ttotal: 30.5s\tremaining: 9m 51s\n",
      "49: learn: 0.7395259\ttest: 0.7317091\tbestTest: 0.7317091 (49)\ttotal: 31.1s\tremaining: 9m 50s\n",
      "50: learn: 0.7398272\ttest: 0.7317615\tbestTest: 0.7317615 (50)\ttotal: 31.6s\tremaining: 9m 48s\n",
      "51: learn: 0.7399491\ttest: 0.7317555\tbestTest: 0.7317615 (50)\ttotal: 32.2s\tremaining: 9m 46s\n",
      "52: learn: 0.7399571\ttest: 0.7317701\tbestTest: 0.7317701 (52)\ttotal: 32.8s\tremaining: 9m 45s\n",
      "53: learn: 0.7400809\ttest: 0.7318189\tbestTest: 0.7318189 (53)\ttotal: 33.3s\tremaining: 9m 43s\n",
      "54: learn: 0.7402183\ttest: 0.731644\tbestTest: 0.7318189 (53)\ttotal: 33.8s\tremaining: 9m 41s\n",
      "55: learn: 0.7402882\ttest: 0.7316918\tbestTest: 0.7318189 (53)\ttotal: 34.4s\tremaining: 9m 39s\n",
      "56: learn: 0.740262\ttest: 0.7314987\tbestTest: 0.7318189 (53)\ttotal: 34.7s\tremaining: 9m 34s\n",
      "57: learn: 0.74046\ttest: 0.7317386\tbestTest: 0.7318189 (53)\ttotal: 35.3s\tremaining: 9m 32s\n",
      "58: learn: 0.7406384\ttest: 0.7317305\tbestTest: 0.7318189 (53)\ttotal: 35.9s\tremaining: 9m 32s\n",
      "59: learn: 0.7407513\ttest: 0.7317083\tbestTest: 0.7318189 (53)\ttotal: 36.4s\tremaining: 9m 30s\n",
      "60: learn: 0.7407831\ttest: 0.7315272\tbestTest: 0.7318189 (53)\ttotal: 37.1s\tremaining: 9m 31s\n",
      "61: learn: 0.7411242\ttest: 0.7316795\tbestTest: 0.7318189 (53)\ttotal: 37.6s\tremaining: 9m 29s\n",
      "62: learn: 0.7412864\ttest: 0.7315273\tbestTest: 0.7318189 (53)\ttotal: 38.2s\tremaining: 9m 27s\n",
      "63: learn: 0.7415688\ttest: 0.7309202\tbestTest: 0.7318189 (53)\ttotal: 38.7s\tremaining: 9m 26s\n",
      "64: learn: 0.7419957\ttest: 0.7311169\tbestTest: 0.7318189 (53)\ttotal: 39.2s\tremaining: 9m 24s\n",
      "65: learn: 0.7422665\ttest: 0.7310701\tbestTest: 0.7318189 (53)\ttotal: 39.8s\tremaining: 9m 23s\n",
      "66: learn: 0.7423783\ttest: 0.7310062\tbestTest: 0.7318189 (53)\ttotal: 40.4s\tremaining: 9m 22s\n",
      "67: learn: 0.742806\ttest: 0.730946\tbestTest: 0.7318189 (53)\ttotal: 41s\tremaining: 9m 22s\n",
      "68: learn: 0.7436786\ttest: 0.7311683\tbestTest: 0.7318189 (53)\ttotal: 41.6s\tremaining: 9m 20s\n",
      "69: learn: 0.7437445\ttest: 0.7312548\tbestTest: 0.7318189 (53)\ttotal: 42.2s\tremaining: 9m 21s\n",
      "70: learn: 0.7438918\ttest: 0.7311137\tbestTest: 0.7318189 (53)\ttotal: 42.9s\tremaining: 9m 20s\n",
      "71: learn: 0.7440462\ttest: 0.7310645\tbestTest: 0.7318189 (53)\ttotal: 43.3s\tremaining: 9m 18s\n",
      "72: learn: 0.7442499\ttest: 0.7313557\tbestTest: 0.7318189 (53)\ttotal: 43.9s\tremaining: 9m 17s\n",
      "73: learn: 0.7442794\ttest: 0.731375\tbestTest: 0.7318189 (53)\ttotal: 44.5s\tremaining: 9m 16s\n",
      "74: learn: 0.7443583\ttest: 0.7314632\tbestTest: 0.7318189 (53)\ttotal: 45.1s\tremaining: 9m 15s\n",
      "75: learn: 0.7443734\ttest: 0.7317002\tbestTest: 0.7318189 (53)\ttotal: 45.7s\tremaining: 9m 15s\n",
      "76: learn: 0.7446687\ttest: 0.7316885\tbestTest: 0.7318189 (53)\ttotal: 46.3s\tremaining: 9m 14s\n",
      "77: learn: 0.7446664\ttest: 0.7316871\tbestTest: 0.7318189 (53)\ttotal: 46.9s\tremaining: 9m 14s\n",
      "78: learn: 0.7449921\ttest: 0.7317131\tbestTest: 0.7318189 (53)\ttotal: 47.4s\tremaining: 9m 12s\n",
      "79: learn: 0.7450462\ttest: 0.7315922\tbestTest: 0.7318189 (53)\ttotal: 47.9s\tremaining: 9m 10s\n",
      "80: learn: 0.7452105\ttest: 0.7311613\tbestTest: 0.7318189 (53)\ttotal: 48.4s\tremaining: 9m 9s\n",
      "81: learn: 0.7452893\ttest: 0.7313566\tbestTest: 0.7318189 (53)\ttotal: 49.1s\tremaining: 9m 9s\n",
      "82: learn: 0.745361\ttest: 0.7315657\tbestTest: 0.7318189 (53)\ttotal: 49.7s\tremaining: 9m 9s\n",
      "83: learn: 0.7456693\ttest: 0.7315015\tbestTest: 0.7318189 (53)\ttotal: 50.3s\tremaining: 9m 7s\n",
      "84: learn: 0.7463839\ttest: 0.7320487\tbestTest: 0.7320487 (84)\ttotal: 50.9s\tremaining: 9m 7s\n",
      "85: learn: 0.7465937\ttest: 0.7321036\tbestTest: 0.7321036 (85)\ttotal: 51.4s\tremaining: 9m 6s\n",
      "86: learn: 0.7465513\ttest: 0.7319789\tbestTest: 0.7321036 (85)\ttotal: 52s\tremaining: 9m 5s\n",
      "87: learn: 0.7466159\ttest: 0.7320539\tbestTest: 0.7321036 (85)\ttotal: 52.6s\tremaining: 9m 5s\n",
      "88: learn: 0.7467067\ttest: 0.7319554\tbestTest: 0.7321036 (85)\ttotal: 53.2s\tremaining: 9m 4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89: learn: 0.7467302\ttest: 0.7323623\tbestTest: 0.7323623 (89)\ttotal: 53.8s\tremaining: 9m 3s\n",
      "90: learn: 0.746786\ttest: 0.7323457\tbestTest: 0.7323623 (89)\ttotal: 54.4s\tremaining: 9m 3s\n",
      "91: learn: 0.7468901\ttest: 0.7316069\tbestTest: 0.7323623 (89)\ttotal: 55s\tremaining: 9m 2s\n",
      "92: learn: 0.7470834\ttest: 0.7318884\tbestTest: 0.7323623 (89)\ttotal: 55.7s\tremaining: 9m 2s\n",
      "93: learn: 0.7471847\ttest: 0.7317612\tbestTest: 0.7323623 (89)\ttotal: 56.4s\tremaining: 9m 3s\n",
      "94: learn: 0.7474569\ttest: 0.7319369\tbestTest: 0.7323623 (89)\ttotal: 57s\tremaining: 9m 3s\n",
      "95: learn: 0.74752\ttest: 0.7318505\tbestTest: 0.7323623 (89)\ttotal: 57.8s\tremaining: 9m 3s\n",
      "96: learn: 0.7475676\ttest: 0.7319119\tbestTest: 0.7323623 (89)\ttotal: 58.5s\tremaining: 9m 4s\n",
      "97: learn: 0.7476581\ttest: 0.7320423\tbestTest: 0.7323623 (89)\ttotal: 59.1s\tremaining: 9m 4s\n",
      "98: learn: 0.7479456\ttest: 0.7323656\tbestTest: 0.7323656 (98)\ttotal: 59.7s\tremaining: 9m 3s\n",
      "99: learn: 0.7481616\ttest: 0.7326338\tbestTest: 0.7326338 (99)\ttotal: 1m\tremaining: 9m 3s\n",
      "100: learn: 0.7481876\ttest: 0.7323983\tbestTest: 0.7326338 (99)\ttotal: 1m\tremaining: 9m 2s\n",
      "101: learn: 0.7483341\ttest: 0.7321273\tbestTest: 0.7326338 (99)\ttotal: 1m 1s\tremaining: 9m 1s\n",
      "102: learn: 0.7484457\ttest: 0.7320233\tbestTest: 0.7326338 (99)\ttotal: 1m 2s\tremaining: 9m\n",
      "103: learn: 0.7485993\ttest: 0.7320561\tbestTest: 0.7326338 (99)\ttotal: 1m 2s\tremaining: 8m 59s\n",
      "104: learn: 0.7490855\ttest: 0.7324786\tbestTest: 0.7326338 (99)\ttotal: 1m 3s\tremaining: 8m 58s\n",
      "105: learn: 0.7490987\ttest: 0.7324456\tbestTest: 0.7326338 (99)\ttotal: 1m 3s\tremaining: 8m 57s\n",
      "106: learn: 0.7492377\ttest: 0.7324441\tbestTest: 0.7326338 (99)\ttotal: 1m 4s\tremaining: 8m 57s\n",
      "107: learn: 0.7493216\ttest: 0.7325079\tbestTest: 0.7326338 (99)\ttotal: 1m 4s\tremaining: 8m 55s\n",
      "108: learn: 0.7494685\ttest: 0.7328937\tbestTest: 0.7328937 (108)\ttotal: 1m 5s\tremaining: 8m 55s\n",
      "109: learn: 0.7496394\ttest: 0.7327266\tbestTest: 0.7328937 (108)\ttotal: 1m 6s\tremaining: 8m 54s\n",
      "110: learn: 0.7497565\ttest: 0.7327371\tbestTest: 0.7328937 (108)\ttotal: 1m 6s\tremaining: 8m 54s\n",
      "111: learn: 0.7499031\ttest: 0.7326054\tbestTest: 0.7328937 (108)\ttotal: 1m 7s\tremaining: 8m 53s\n",
      "112: learn: 0.7503228\ttest: 0.7326198\tbestTest: 0.7328937 (108)\ttotal: 1m 7s\tremaining: 8m 52s\n",
      "113: learn: 0.7503295\ttest: 0.7326599\tbestTest: 0.7328937 (108)\ttotal: 1m 8s\tremaining: 8m 51s\n",
      "114: learn: 0.7504103\ttest: 0.7325488\tbestTest: 0.7328937 (108)\ttotal: 1m 8s\tremaining: 8m 50s\n",
      "115: learn: 0.7510763\ttest: 0.7321499\tbestTest: 0.7328937 (108)\ttotal: 1m 9s\tremaining: 8m 50s\n",
      "116: learn: 0.7513373\ttest: 0.7316981\tbestTest: 0.7328937 (108)\ttotal: 1m 10s\tremaining: 8m 49s\n",
      "117: learn: 0.751762\ttest: 0.732012\tbestTest: 0.7328937 (108)\ttotal: 1m 10s\tremaining: 8m 48s\n",
      "118: learn: 0.7519222\ttest: 0.7317142\tbestTest: 0.7328937 (108)\ttotal: 1m 11s\tremaining: 8m 47s\n",
      "119: learn: 0.7519597\ttest: 0.7316095\tbestTest: 0.7328937 (108)\ttotal: 1m 11s\tremaining: 8m 46s\n",
      "120: learn: 0.7521535\ttest: 0.7316577\tbestTest: 0.7328937 (108)\ttotal: 1m 12s\tremaining: 8m 45s\n",
      "121: learn: 0.7523671\ttest: 0.731657\tbestTest: 0.7328937 (108)\ttotal: 1m 12s\tremaining: 8m 43s\n",
      "122: learn: 0.7525677\ttest: 0.7315867\tbestTest: 0.7328937 (108)\ttotal: 1m 13s\tremaining: 8m 42s\n",
      "123: learn: 0.7526082\ttest: 0.7314671\tbestTest: 0.7328937 (108)\ttotal: 1m 13s\tremaining: 8m 42s\n",
      "124: learn: 0.7526281\ttest: 0.7314911\tbestTest: 0.7328937 (108)\ttotal: 1m 14s\tremaining: 8m 41s\n",
      "125: learn: 0.7528063\ttest: 0.731593\tbestTest: 0.7328937 (108)\ttotal: 1m 15s\tremaining: 8m 40s\n",
      "126: learn: 0.7527968\ttest: 0.7315733\tbestTest: 0.7328937 (108)\ttotal: 1m 15s\tremaining: 8m 39s\n",
      "127: learn: 0.7530496\ttest: 0.7314467\tbestTest: 0.7328937 (108)\ttotal: 1m 16s\tremaining: 8m 39s\n",
      "128: learn: 0.7531128\ttest: 0.7314217\tbestTest: 0.7328937 (108)\ttotal: 1m 16s\tremaining: 8m 38s\n",
      "129: learn: 0.7531792\ttest: 0.7313062\tbestTest: 0.7328937 (108)\ttotal: 1m 17s\tremaining: 8m 37s\n",
      "130: learn: 0.7534726\ttest: 0.7314796\tbestTest: 0.7328937 (108)\ttotal: 1m 17s\tremaining: 8m 37s\n",
      "131: learn: 0.7534925\ttest: 0.7314341\tbestTest: 0.7328937 (108)\ttotal: 1m 18s\tremaining: 8m 37s\n",
      "132: learn: 0.7536681\ttest: 0.7314899\tbestTest: 0.7328937 (108)\ttotal: 1m 19s\tremaining: 8m 36s\n",
      "133: learn: 0.7538592\ttest: 0.7317655\tbestTest: 0.7328937 (108)\ttotal: 1m 19s\tremaining: 8m 36s\n",
      "134: learn: 0.7540222\ttest: 0.7314798\tbestTest: 0.7328937 (108)\ttotal: 1m 20s\tremaining: 8m 35s\n",
      "135: learn: 0.7541776\ttest: 0.7313783\tbestTest: 0.7328937 (108)\ttotal: 1m 21s\tremaining: 8m 35s\n",
      "136: learn: 0.7541735\ttest: 0.7313755\tbestTest: 0.7328937 (108)\ttotal: 1m 21s\tremaining: 8m 34s\n",
      "137: learn: 0.754322\ttest: 0.7312996\tbestTest: 0.7328937 (108)\ttotal: 1m 22s\tremaining: 8m 34s\n",
      "138: learn: 0.7549608\ttest: 0.7313662\tbestTest: 0.7328937 (108)\ttotal: 1m 23s\tremaining: 8m 34s\n",
      "139: learn: 0.7549569\ttest: 0.7313617\tbestTest: 0.7328937 (108)\ttotal: 1m 23s\tremaining: 8m 33s\n",
      "140: learn: 0.7549567\ttest: 0.7313617\tbestTest: 0.7328937 (108)\ttotal: 1m 24s\tremaining: 8m 34s\n",
      "141: learn: 0.7549871\ttest: 0.7315867\tbestTest: 0.7328937 (108)\ttotal: 1m 24s\tremaining: 8m 33s\n",
      "142: learn: 0.7550327\ttest: 0.731737\tbestTest: 0.7328937 (108)\ttotal: 1m 25s\tremaining: 8m 32s\n",
      "143: learn: 0.7551913\ttest: 0.7316033\tbestTest: 0.7328937 (108)\ttotal: 1m 26s\tremaining: 8m 31s\n",
      "144: learn: 0.7554312\ttest: 0.7315626\tbestTest: 0.7328937 (108)\ttotal: 1m 26s\tremaining: 8m 30s\n",
      "145: learn: 0.7554428\ttest: 0.7315693\tbestTest: 0.7328937 (108)\ttotal: 1m 27s\tremaining: 8m 30s\n",
      "146: learn: 0.7556638\ttest: 0.7312762\tbestTest: 0.7328937 (108)\ttotal: 1m 27s\tremaining: 8m 29s\n",
      "147: learn: 0.7559466\ttest: 0.7311714\tbestTest: 0.7328937 (108)\ttotal: 1m 28s\tremaining: 8m 28s\n",
      "148: learn: 0.7560961\ttest: 0.7310418\tbestTest: 0.7328937 (108)\ttotal: 1m 28s\tremaining: 8m 28s\n",
      "149: learn: 0.7561541\ttest: 0.7311206\tbestTest: 0.7328937 (108)\ttotal: 1m 29s\tremaining: 8m 27s\n",
      "150: learn: 0.7562276\ttest: 0.7312083\tbestTest: 0.7328937 (108)\ttotal: 1m 30s\tremaining: 8m 26s\n",
      "151: learn: 0.7562407\ttest: 0.7312491\tbestTest: 0.7328937 (108)\ttotal: 1m 30s\tremaining: 8m 26s\n",
      "152: learn: 0.7563883\ttest: 0.7312071\tbestTest: 0.7328937 (108)\ttotal: 1m 31s\tremaining: 8m 26s\n",
      "153: learn: 0.7565926\ttest: 0.7311291\tbestTest: 0.7328937 (108)\ttotal: 1m 32s\tremaining: 8m 25s\n",
      "154: learn: 0.7565876\ttest: 0.7311343\tbestTest: 0.7328937 (108)\ttotal: 1m 32s\tremaining: 8m 24s\n",
      "155: learn: 0.7568167\ttest: 0.7311374\tbestTest: 0.7328937 (108)\ttotal: 1m 33s\tremaining: 8m 24s\n",
      "156: learn: 0.7569714\ttest: 0.7312924\tbestTest: 0.7328937 (108)\ttotal: 1m 33s\tremaining: 8m 24s\n",
      "157: learn: 0.7572921\ttest: 0.7311655\tbestTest: 0.7328937 (108)\ttotal: 1m 34s\tremaining: 8m 23s\n",
      "158: learn: 0.7574495\ttest: 0.7312128\tbestTest: 0.7328937 (108)\ttotal: 1m 35s\tremaining: 8m 23s\n",
      "159: learn: 0.7575514\ttest: 0.7313283\tbestTest: 0.7328937 (108)\ttotal: 1m 35s\tremaining: 8m 23s\n",
      "160: learn: 0.7576391\ttest: 0.7313335\tbestTest: 0.7328937 (108)\ttotal: 1m 36s\tremaining: 8m 24s\n",
      "161: learn: 0.7576431\ttest: 0.7312836\tbestTest: 0.7328937 (108)\ttotal: 1m 37s\tremaining: 8m 23s\n",
      "162: learn: 0.7580104\ttest: 0.7316004\tbestTest: 0.7328937 (108)\ttotal: 1m 38s\tremaining: 8m 23s\n",
      "163: learn: 0.758031\ttest: 0.7315599\tbestTest: 0.7328937 (108)\ttotal: 1m 38s\tremaining: 8m 22s\n",
      "164: learn: 0.7588243\ttest: 0.731787\tbestTest: 0.7328937 (108)\ttotal: 1m 39s\tremaining: 8m 22s\n",
      "165: learn: 0.7591321\ttest: 0.7315825\tbestTest: 0.7328937 (108)\ttotal: 1m 39s\tremaining: 8m 22s\n",
      "166: learn: 0.7591658\ttest: 0.7316664\tbestTest: 0.7328937 (108)\ttotal: 1m 40s\tremaining: 8m 20s\n",
      "167: learn: 0.7591534\ttest: 0.7316323\tbestTest: 0.7328937 (108)\ttotal: 1m 41s\tremaining: 8m 20s\n",
      "168: learn: 0.7593794\ttest: 0.731483\tbestTest: 0.7328937 (108)\ttotal: 1m 41s\tremaining: 8m 19s\n",
      "169: learn: 0.7598609\ttest: 0.7314818\tbestTest: 0.7328937 (108)\ttotal: 1m 42s\tremaining: 8m 19s\n",
      "170: learn: 0.7604359\ttest: 0.7315706\tbestTest: 0.7328937 (108)\ttotal: 1m 42s\tremaining: 8m 18s\n",
      "171: learn: 0.7608225\ttest: 0.7317474\tbestTest: 0.7328937 (108)\ttotal: 1m 43s\tremaining: 8m 17s\n",
      "172: learn: 0.7610612\ttest: 0.7315511\tbestTest: 0.7328937 (108)\ttotal: 1m 43s\tremaining: 8m 17s\n",
      "173: learn: 0.7611688\ttest: 0.7316159\tbestTest: 0.7328937 (108)\ttotal: 1m 44s\tremaining: 8m 16s\n",
      "174: learn: 0.7613442\ttest: 0.7315588\tbestTest: 0.7328937 (108)\ttotal: 1m 45s\tremaining: 8m 15s\n",
      "175: learn: 0.7613404\ttest: 0.7315541\tbestTest: 0.7328937 (108)\ttotal: 1m 45s\tremaining: 8m 15s\n",
      "176: learn: 0.7613588\ttest: 0.7315713\tbestTest: 0.7328937 (108)\ttotal: 1m 46s\tremaining: 8m 14s\n",
      "177: learn: 0.7613455\ttest: 0.7315279\tbestTest: 0.7328937 (108)\ttotal: 1m 46s\tremaining: 8m 13s\n",
      "178: learn: 0.7614037\ttest: 0.7315637\tbestTest: 0.7328937 (108)\ttotal: 1m 47s\tremaining: 8m 12s\n",
      "179: learn: 0.7615001\ttest: 0.7315128\tbestTest: 0.7328937 (108)\ttotal: 1m 47s\tremaining: 8m 11s\n",
      "180: learn: 0.7615698\ttest: 0.7315124\tbestTest: 0.7328937 (108)\ttotal: 1m 48s\tremaining: 8m 10s\n",
      "181: learn: 0.7616123\ttest: 0.7316133\tbestTest: 0.7328937 (108)\ttotal: 1m 49s\tremaining: 8m 10s\n",
      "182: learn: 0.761613\ttest: 0.7315779\tbestTest: 0.7328937 (108)\ttotal: 1m 49s\tremaining: 8m 9s\n",
      "183: learn: 0.7616084\ttest: 0.7315824\tbestTest: 0.7328937 (108)\ttotal: 1m 50s\tremaining: 8m 9s\n",
      "184: learn: 0.7616249\ttest: 0.7315736\tbestTest: 0.7328937 (108)\ttotal: 1m 50s\tremaining: 8m 7s\n",
      "185: learn: 0.761659\ttest: 0.7316176\tbestTest: 0.7328937 (108)\ttotal: 1m 51s\tremaining: 8m 7s\n",
      "186: learn: 0.761921\ttest: 0.7317319\tbestTest: 0.7328937 (108)\ttotal: 1m 52s\tremaining: 8m 7s\n",
      "187: learn: 0.7620997\ttest: 0.7317831\tbestTest: 0.7328937 (108)\ttotal: 1m 52s\tremaining: 8m 6s\n",
      "188: learn: 0.7621468\ttest: 0.7317719\tbestTest: 0.7328937 (108)\ttotal: 1m 53s\tremaining: 8m 6s\n",
      "189: learn: 0.7622513\ttest: 0.7317712\tbestTest: 0.7328937 (108)\ttotal: 1m 53s\tremaining: 8m 5s\n",
      "190: learn: 0.762588\ttest: 0.731789\tbestTest: 0.7328937 (108)\ttotal: 1m 54s\tremaining: 8m 5s\n",
      "191: learn: 0.7626724\ttest: 0.7317407\tbestTest: 0.7328937 (108)\ttotal: 1m 55s\tremaining: 8m 4s\n",
      "192: learn: 0.7627759\ttest: 0.7317513\tbestTest: 0.7328937 (108)\ttotal: 1m 55s\tremaining: 8m 3s\n",
      "193: learn: 0.762993\ttest: 0.7317475\tbestTest: 0.7328937 (108)\ttotal: 1m 56s\tremaining: 8m 3s\n",
      "194: learn: 0.7630272\ttest: 0.7317253\tbestTest: 0.7328937 (108)\ttotal: 1m 56s\tremaining: 8m 2s\n",
      "195: learn: 0.7633113\ttest: 0.731633\tbestTest: 0.7328937 (108)\ttotal: 1m 57s\tremaining: 8m 1s\n",
      "196: learn: 0.7634643\ttest: 0.7318285\tbestTest: 0.7328937 (108)\ttotal: 1m 57s\tremaining: 8m\n",
      "197: learn: 0.7634859\ttest: 0.7319059\tbestTest: 0.7328937 (108)\ttotal: 1m 58s\tremaining: 8m\n",
      "198: learn: 0.7635754\ttest: 0.73195\tbestTest: 0.7328937 (108)\ttotal: 1m 59s\tremaining: 7m 59s\n",
      "199: learn: 0.7636152\ttest: 0.7318997\tbestTest: 0.7328937 (108)\ttotal: 1m 59s\tremaining: 7m 59s\n",
      "200: learn: 0.7640273\ttest: 0.7316161\tbestTest: 0.7328937 (108)\ttotal: 2m\tremaining: 7m 58s\n",
      "201: learn: 0.7641953\ttest: 0.7316025\tbestTest: 0.7328937 (108)\ttotal: 2m 1s\tremaining: 7m 58s\n",
      "202: learn: 0.7641927\ttest: 0.7316064\tbestTest: 0.7328937 (108)\ttotal: 2m 1s\tremaining: 7m 57s\n",
      "203: learn: 0.7641827\ttest: 0.7315967\tbestTest: 0.7328937 (108)\ttotal: 2m 2s\tremaining: 7m 56s\n",
      "204: learn: 0.7641665\ttest: 0.7316365\tbestTest: 0.7328937 (108)\ttotal: 2m 2s\tremaining: 7m 56s\n",
      "205: learn: 0.7641488\ttest: 0.7316319\tbestTest: 0.7328937 (108)\ttotal: 2m 3s\tremaining: 7m 56s\n",
      "206: learn: 0.7641898\ttest: 0.73168\tbestTest: 0.7328937 (108)\ttotal: 2m 4s\tremaining: 7m 56s\n",
      "207: learn: 0.7642284\ttest: 0.7317035\tbestTest: 0.7328937 (108)\ttotal: 2m 4s\tremaining: 7m 55s\n",
      "208: learn: 0.7642527\ttest: 0.7316681\tbestTest: 0.7328937 (108)\ttotal: 2m 5s\tremaining: 7m 55s\n",
      "209: learn: 0.7642858\ttest: 0.7317302\tbestTest: 0.7328937 (108)\ttotal: 2m 6s\tremaining: 7m 54s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.7328936509\n",
      "bestIteration = 108\n",
      "\n",
      "Depth :  8\n",
      "Train and Test loss :  0.7642857746 0.731730244429\n",
      "[0.73173024442866696]\n",
      "0: learn: 0.69015\ttest: 0.6885734\tbestTest: 0.6885734 (0)\ttotal: 617ms\tremaining: 10m 15s\n",
      "1: learn: 0.7032844\ttest: 0.7010828\tbestTest: 0.7010828 (1)\ttotal: 1.19s\tremaining: 9m 56s\n",
      "2: learn: 0.7098355\ttest: 0.7075884\tbestTest: 0.7075884 (2)\ttotal: 1.75s\tremaining: 9m 41s\n",
      "3: learn: 0.7104362\ttest: 0.707953\tbestTest: 0.707953 (3)\ttotal: 2.17s\tremaining: 9m\n",
      "4: learn: 0.7151004\ttest: 0.7128021\tbestTest: 0.7128021 (4)\ttotal: 2.75s\tremaining: 9m 7s\n",
      "5: learn: 0.7155261\ttest: 0.7122455\tbestTest: 0.7128021 (4)\ttotal: 3.28s\tremaining: 9m 3s\n",
      "6: learn: 0.71843\ttest: 0.7153974\tbestTest: 0.7153974 (6)\ttotal: 3.86s\tremaining: 9m 7s\n",
      "7: learn: 0.7218657\ttest: 0.7180554\tbestTest: 0.7180554 (7)\ttotal: 4.48s\tremaining: 9m 16s\n",
      "8: learn: 0.7219181\ttest: 0.7176874\tbestTest: 0.7180554 (7)\ttotal: 5.08s\tremaining: 9m 19s\n",
      "9: learn: 0.7241621\ttest: 0.7185958\tbestTest: 0.7185958 (9)\ttotal: 5.61s\tremaining: 9m 15s\n",
      "10: learn: 0.7250472\ttest: 0.718974\tbestTest: 0.718974 (10)\ttotal: 6.25s\tremaining: 9m 22s\n",
      "11: learn: 0.7252389\ttest: 0.7192957\tbestTest: 0.7192957 (11)\ttotal: 6.82s\tremaining: 9m 21s\n",
      "12: learn: 0.7270787\ttest: 0.7210618\tbestTest: 0.7210618 (12)\ttotal: 7.34s\tremaining: 9m 17s\n",
      "13: learn: 0.7275912\ttest: 0.7218797\tbestTest: 0.7218797 (13)\ttotal: 7.93s\tremaining: 9m 18s\n",
      "14: learn: 0.728472\ttest: 0.7220195\tbestTest: 0.7220195 (14)\ttotal: 8.65s\tremaining: 9m 28s\n",
      "15: learn: 0.7287222\ttest: 0.7220463\tbestTest: 0.7220463 (15)\ttotal: 9.32s\tremaining: 9m 33s\n",
      "16: learn: 0.7295101\ttest: 0.7225631\tbestTest: 0.7225631 (16)\ttotal: 9.96s\tremaining: 9m 36s\n",
      "17: learn: 0.7298924\ttest: 0.7230093\tbestTest: 0.7230093 (17)\ttotal: 10.5s\tremaining: 9m 35s\n",
      "18: learn: 0.7299244\ttest: 0.722708\tbestTest: 0.7230093 (17)\ttotal: 11.2s\tremaining: 9m 36s\n",
      "19: learn: 0.730433\ttest: 0.7226339\tbestTest: 0.7230093 (17)\ttotal: 11.7s\tremaining: 9m 35s\n",
      "20: learn: 0.7313857\ttest: 0.7228381\tbestTest: 0.7230093 (17)\ttotal: 12.4s\tremaining: 9m 37s\n",
      "21: learn: 0.7315709\ttest: 0.723294\tbestTest: 0.723294 (21)\ttotal: 13.1s\tremaining: 9m 40s\n",
      "22: learn: 0.7318159\ttest: 0.7232518\tbestTest: 0.723294 (21)\ttotal: 13.6s\tremaining: 9m 36s\n",
      "23: learn: 0.7316666\ttest: 0.7238099\tbestTest: 0.7238099 (23)\ttotal: 14.2s\tremaining: 9m 36s\n",
      "24: learn: 0.7334131\ttest: 0.7240518\tbestTest: 0.7240518 (24)\ttotal: 14.7s\tremaining: 9m 34s\n",
      "25: learn: 0.7340835\ttest: 0.7236864\tbestTest: 0.7240518 (24)\ttotal: 15.3s\tremaining: 9m 33s\n",
      "26: learn: 0.7342271\ttest: 0.7240357\tbestTest: 0.7240518 (24)\ttotal: 16.1s\tremaining: 9m 40s\n",
      "27: learn: 0.7348567\ttest: 0.7241789\tbestTest: 0.7241789 (27)\ttotal: 16.7s\tremaining: 9m 39s\n",
      "28: learn: 0.7350973\ttest: 0.7246569\tbestTest: 0.7246569 (28)\ttotal: 17.4s\tremaining: 9m 42s\n",
      "29: learn: 0.7353514\ttest: 0.7246884\tbestTest: 0.7246884 (29)\ttotal: 17.9s\tremaining: 9m 37s\n",
      "30: learn: 0.7355706\ttest: 0.7248255\tbestTest: 0.7248255 (30)\ttotal: 18.5s\tremaining: 9m 36s\n",
      "31: learn: 0.7357634\ttest: 0.72476\tbestTest: 0.7248255 (30)\ttotal: 19.1s\tremaining: 9m 37s\n",
      "32: learn: 0.7358338\ttest: 0.7249616\tbestTest: 0.7249616 (32)\ttotal: 19.7s\tremaining: 9m 36s\n",
      "33: learn: 0.7361486\ttest: 0.7255299\tbestTest: 0.7255299 (33)\ttotal: 20.3s\tremaining: 9m 38s\n",
      "34: learn: 0.7363447\ttest: 0.7249138\tbestTest: 0.7255299 (33)\ttotal: 20.9s\tremaining: 9m 35s\n",
      "35: learn: 0.736554\ttest: 0.7249746\tbestTest: 0.7255299 (33)\ttotal: 21.5s\tremaining: 9m 34s\n",
      "36: learn: 0.7366939\ttest: 0.7250132\tbestTest: 0.7255299 (33)\ttotal: 22.3s\tremaining: 9m 39s\n",
      "37: learn: 0.7373588\ttest: 0.7247073\tbestTest: 0.7255299 (33)\ttotal: 23s\tremaining: 9m 42s\n",
      "38: learn: 0.7378773\ttest: 0.7248001\tbestTest: 0.7255299 (33)\ttotal: 23.6s\tremaining: 9m 41s\n",
      "39: learn: 0.737928\ttest: 0.7247247\tbestTest: 0.7255299 (33)\ttotal: 24.2s\tremaining: 9m 41s\n",
      "40: learn: 0.7386141\ttest: 0.7245094\tbestTest: 0.7255299 (33)\ttotal: 24.8s\tremaining: 9m 39s\n",
      "41: learn: 0.7386292\ttest: 0.7244373\tbestTest: 0.7255299 (33)\ttotal: 25.4s\tremaining: 9m 39s\n",
      "42: learn: 0.7386822\ttest: 0.7244807\tbestTest: 0.7255299 (33)\ttotal: 25.9s\tremaining: 9m 37s\n",
      "43: learn: 0.738874\ttest: 0.7249397\tbestTest: 0.7255299 (33)\ttotal: 26.5s\tremaining: 9m 34s\n",
      "44: learn: 0.7390224\ttest: 0.7248842\tbestTest: 0.7255299 (33)\ttotal: 27s\tremaining: 9m 33s\n",
      "45: learn: 0.7391769\ttest: 0.7250478\tbestTest: 0.7255299 (33)\ttotal: 27.6s\tremaining: 9m 31s\n",
      "46: learn: 0.7398481\ttest: 0.7247878\tbestTest: 0.7255299 (33)\ttotal: 28.1s\tremaining: 9m 29s\n",
      "47: learn: 0.7406304\ttest: 0.7246807\tbestTest: 0.7255299 (33)\ttotal: 28.6s\tremaining: 9m 28s\n",
      "48: learn: 0.7410377\ttest: 0.7250497\tbestTest: 0.7255299 (33)\ttotal: 29.2s\tremaining: 9m 26s\n",
      "49: learn: 0.7412179\ttest: 0.7250321\tbestTest: 0.7255299 (33)\ttotal: 29.9s\tremaining: 9m 27s\n",
      "50: learn: 0.7412233\ttest: 0.7249129\tbestTest: 0.7255299 (33)\ttotal: 30.5s\tremaining: 9m 27s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51: learn: 0.7414301\ttest: 0.7251147\tbestTest: 0.7255299 (33)\ttotal: 31s\tremaining: 9m 25s\n",
      "52: learn: 0.7416182\ttest: 0.7252331\tbestTest: 0.7255299 (33)\ttotal: 31.6s\tremaining: 9m 24s\n",
      "53: learn: 0.7422431\ttest: 0.72513\tbestTest: 0.7255299 (33)\ttotal: 32.2s\tremaining: 9m 24s\n",
      "54: learn: 0.7427678\ttest: 0.7256929\tbestTest: 0.7256929 (54)\ttotal: 32.9s\tremaining: 9m 24s\n",
      "55: learn: 0.7426857\ttest: 0.7256654\tbestTest: 0.7256929 (54)\ttotal: 33.5s\tremaining: 9m 24s\n",
      "56: learn: 0.7428418\ttest: 0.7256831\tbestTest: 0.7256929 (54)\ttotal: 34s\tremaining: 9m 22s\n",
      "57: learn: 0.7430866\ttest: 0.7257468\tbestTest: 0.7257468 (57)\ttotal: 34.6s\tremaining: 9m 22s\n",
      "58: learn: 0.7433008\ttest: 0.7257694\tbestTest: 0.7257694 (58)\ttotal: 35.3s\tremaining: 9m 22s\n",
      "59: learn: 0.7435062\ttest: 0.7253945\tbestTest: 0.7257694 (58)\ttotal: 35.8s\tremaining: 9m 21s\n",
      "60: learn: 0.7437025\ttest: 0.7254518\tbestTest: 0.7257694 (58)\ttotal: 36.5s\tremaining: 9m 22s\n",
      "61: learn: 0.7440862\ttest: 0.7257143\tbestTest: 0.7257694 (58)\ttotal: 37s\tremaining: 9m 20s\n",
      "62: learn: 0.7449798\ttest: 0.7260037\tbestTest: 0.7260037 (62)\ttotal: 37.7s\tremaining: 9m 20s\n",
      "63: learn: 0.7451795\ttest: 0.7255913\tbestTest: 0.7260037 (62)\ttotal: 38.4s\tremaining: 9m 21s\n",
      "64: learn: 0.745498\ttest: 0.7256304\tbestTest: 0.7260037 (62)\ttotal: 39.1s\tremaining: 9m 21s\n",
      "65: learn: 0.7454675\ttest: 0.7257564\tbestTest: 0.7260037 (62)\ttotal: 39.6s\tremaining: 9m 20s\n",
      "66: learn: 0.7462372\ttest: 0.7252164\tbestTest: 0.7260037 (62)\ttotal: 40.2s\tremaining: 9m 19s\n",
      "67: learn: 0.746325\ttest: 0.7254414\tbestTest: 0.7260037 (62)\ttotal: 40.9s\tremaining: 9m 19s\n",
      "68: learn: 0.7463527\ttest: 0.7255165\tbestTest: 0.7260037 (62)\ttotal: 41.4s\tremaining: 9m 19s\n",
      "69: learn: 0.7466258\ttest: 0.7256197\tbestTest: 0.7260037 (62)\ttotal: 42.1s\tremaining: 9m 18s\n",
      "70: learn: 0.7467518\ttest: 0.7256323\tbestTest: 0.7260037 (62)\ttotal: 42.6s\tremaining: 9m 17s\n",
      "71: learn: 0.7468394\ttest: 0.7256802\tbestTest: 0.7260037 (62)\ttotal: 43.3s\tremaining: 9m 17s\n",
      "72: learn: 0.7470879\ttest: 0.7255484\tbestTest: 0.7260037 (62)\ttotal: 43.9s\tremaining: 9m 16s\n",
      "73: learn: 0.7473201\ttest: 0.725727\tbestTest: 0.7260037 (62)\ttotal: 44.6s\tremaining: 9m 18s\n",
      "74: learn: 0.7474473\ttest: 0.7257584\tbestTest: 0.7260037 (62)\ttotal: 45.1s\tremaining: 9m 16s\n",
      "75: learn: 0.7476113\ttest: 0.725743\tbestTest: 0.7260037 (62)\ttotal: 45.7s\tremaining: 9m 15s\n",
      "76: learn: 0.747653\ttest: 0.7257873\tbestTest: 0.7260037 (62)\ttotal: 46.3s\tremaining: 9m 14s\n",
      "77: learn: 0.7480672\ttest: 0.725555\tbestTest: 0.7260037 (62)\ttotal: 46.8s\tremaining: 9m 12s\n",
      "78: learn: 0.7484104\ttest: 0.7254447\tbestTest: 0.7260037 (62)\ttotal: 47.3s\tremaining: 9m 11s\n",
      "79: learn: 0.7486558\ttest: 0.7255242\tbestTest: 0.7260037 (62)\ttotal: 47.8s\tremaining: 9m 9s\n",
      "80: learn: 0.7490277\ttest: 0.7254414\tbestTest: 0.7260037 (62)\ttotal: 48.4s\tremaining: 9m 8s\n",
      "81: learn: 0.7491748\ttest: 0.7252819\tbestTest: 0.7260037 (62)\ttotal: 49s\tremaining: 9m 9s\n",
      "82: learn: 0.7495342\ttest: 0.7250911\tbestTest: 0.7260037 (62)\ttotal: 49.7s\tremaining: 9m 9s\n",
      "83: learn: 0.7496836\ttest: 0.7247722\tbestTest: 0.7260037 (62)\ttotal: 50.4s\tremaining: 9m 9s\n",
      "84: learn: 0.7497791\ttest: 0.7247678\tbestTest: 0.7260037 (62)\ttotal: 50.9s\tremaining: 9m 8s\n",
      "85: learn: 0.7501167\ttest: 0.7248263\tbestTest: 0.7260037 (62)\ttotal: 51.6s\tremaining: 9m 8s\n",
      "86: learn: 0.7503849\ttest: 0.7248588\tbestTest: 0.7260037 (62)\ttotal: 52.2s\tremaining: 9m 7s\n",
      "87: learn: 0.7503817\ttest: 0.7249869\tbestTest: 0.7260037 (62)\ttotal: 52.8s\tremaining: 9m 6s\n",
      "88: learn: 0.7506268\ttest: 0.7253086\tbestTest: 0.7260037 (62)\ttotal: 53.4s\tremaining: 9m 6s\n",
      "89: learn: 0.750783\ttest: 0.7255189\tbestTest: 0.7260037 (62)\ttotal: 54.1s\tremaining: 9m 7s\n",
      "90: learn: 0.7511076\ttest: 0.725589\tbestTest: 0.7260037 (62)\ttotal: 54.7s\tremaining: 9m 6s\n",
      "91: learn: 0.7511841\ttest: 0.7256245\tbestTest: 0.7260037 (62)\ttotal: 55.3s\tremaining: 9m 5s\n",
      "92: learn: 0.7511786\ttest: 0.7255816\tbestTest: 0.7260037 (62)\ttotal: 55.9s\tremaining: 9m 4s\n",
      "93: learn: 0.7513908\ttest: 0.7257353\tbestTest: 0.7260037 (62)\ttotal: 56.4s\tremaining: 9m 3s\n",
      "94: learn: 0.7516104\ttest: 0.7256084\tbestTest: 0.7260037 (62)\ttotal: 57s\tremaining: 9m 3s\n",
      "95: learn: 0.7519405\ttest: 0.7256337\tbestTest: 0.7260037 (62)\ttotal: 57.7s\tremaining: 9m 3s\n",
      "96: learn: 0.752171\ttest: 0.725637\tbestTest: 0.7260037 (62)\ttotal: 58.4s\tremaining: 9m 3s\n",
      "97: learn: 0.7525959\ttest: 0.7257547\tbestTest: 0.7260037 (62)\ttotal: 58.9s\tremaining: 9m 2s\n",
      "98: learn: 0.7527359\ttest: 0.7257045\tbestTest: 0.7260037 (62)\ttotal: 59.4s\tremaining: 9m\n",
      "99: learn: 0.7532838\ttest: 0.7257474\tbestTest: 0.7260037 (62)\ttotal: 1m\tremaining: 9m\n",
      "100: learn: 0.7534153\ttest: 0.7259846\tbestTest: 0.7260037 (62)\ttotal: 1m\tremaining: 8m 59s\n",
      "101: learn: 0.7534618\ttest: 0.7259858\tbestTest: 0.7260037 (62)\ttotal: 1m 1s\tremaining: 8m 57s\n",
      "102: learn: 0.753586\ttest: 0.7260131\tbestTest: 0.7260131 (102)\ttotal: 1m 1s\tremaining: 8m 57s\n",
      "103: learn: 0.7535946\ttest: 0.7260987\tbestTest: 0.7260987 (103)\ttotal: 1m 2s\tremaining: 8m 57s\n",
      "104: learn: 0.7543742\ttest: 0.7262118\tbestTest: 0.7262118 (104)\ttotal: 1m 3s\tremaining: 8m 57s\n",
      "105: learn: 0.7546432\ttest: 0.7261823\tbestTest: 0.7262118 (104)\ttotal: 1m 3s\tremaining: 8m 56s\n",
      "106: learn: 0.7547392\ttest: 0.7262403\tbestTest: 0.7262403 (106)\ttotal: 1m 4s\tremaining: 8m 57s\n",
      "107: learn: 0.7549807\ttest: 0.7260373\tbestTest: 0.7262403 (106)\ttotal: 1m 5s\tremaining: 8m 57s\n",
      "108: learn: 0.7557227\ttest: 0.7261432\tbestTest: 0.7262403 (106)\ttotal: 1m 5s\tremaining: 8m 56s\n",
      "109: learn: 0.7558442\ttest: 0.726127\tbestTest: 0.7262403 (106)\ttotal: 1m 6s\tremaining: 8m 56s\n",
      "110: learn: 0.7560898\ttest: 0.7260453\tbestTest: 0.7262403 (106)\ttotal: 1m 6s\tremaining: 8m 56s\n",
      "111: learn: 0.7562628\ttest: 0.7260712\tbestTest: 0.7262403 (106)\ttotal: 1m 7s\tremaining: 8m 56s\n",
      "112: learn: 0.7563919\ttest: 0.7259792\tbestTest: 0.7262403 (106)\ttotal: 1m 8s\tremaining: 8m 56s\n",
      "113: learn: 0.7566048\ttest: 0.7258573\tbestTest: 0.7262403 (106)\ttotal: 1m 9s\tremaining: 8m 56s\n",
      "114: learn: 0.756975\ttest: 0.7256187\tbestTest: 0.7262403 (106)\ttotal: 1m 9s\tremaining: 8m 56s\n",
      "115: learn: 0.7570758\ttest: 0.7257111\tbestTest: 0.7262403 (106)\ttotal: 1m 10s\tremaining: 8m 56s\n",
      "116: learn: 0.7572729\ttest: 0.7257898\tbestTest: 0.7262403 (106)\ttotal: 1m 11s\tremaining: 8m 56s\n",
      "117: learn: 0.7574941\ttest: 0.7257087\tbestTest: 0.7262403 (106)\ttotal: 1m 11s\tremaining: 8m 55s\n",
      "118: learn: 0.7576327\ttest: 0.7255892\tbestTest: 0.7262403 (106)\ttotal: 1m 12s\tremaining: 8m 54s\n",
      "119: learn: 0.757856\ttest: 0.7258108\tbestTest: 0.7262403 (106)\ttotal: 1m 12s\tremaining: 8m 53s\n",
      "120: learn: 0.7580185\ttest: 0.7258254\tbestTest: 0.7262403 (106)\ttotal: 1m 13s\tremaining: 8m 53s\n",
      "121: learn: 0.7582261\ttest: 0.7258002\tbestTest: 0.7262403 (106)\ttotal: 1m 14s\tremaining: 8m 52s\n",
      "122: learn: 0.7584932\ttest: 0.7258763\tbestTest: 0.7262403 (106)\ttotal: 1m 14s\tremaining: 8m 52s\n",
      "123: learn: 0.7592461\ttest: 0.726075\tbestTest: 0.7262403 (106)\ttotal: 1m 15s\tremaining: 8m 51s\n",
      "124: learn: 0.7594006\ttest: 0.7260246\tbestTest: 0.7262403 (106)\ttotal: 1m 15s\tremaining: 8m 50s\n",
      "125: learn: 0.7598271\ttest: 0.7260232\tbestTest: 0.7262403 (106)\ttotal: 1m 16s\tremaining: 8m 49s\n",
      "126: learn: 0.7602412\ttest: 0.7260151\tbestTest: 0.7262403 (106)\ttotal: 1m 16s\tremaining: 8m 48s\n",
      "127: learn: 0.7603754\ttest: 0.725999\tbestTest: 0.7262403 (106)\ttotal: 1m 17s\tremaining: 8m 47s\n",
      "128: learn: 0.7604714\ttest: 0.7262024\tbestTest: 0.7262403 (106)\ttotal: 1m 18s\tremaining: 8m 47s\n",
      "129: learn: 0.7606193\ttest: 0.7261566\tbestTest: 0.7262403 (106)\ttotal: 1m 18s\tremaining: 8m 47s\n",
      "130: learn: 0.7607577\ttest: 0.7263234\tbestTest: 0.7263234 (130)\ttotal: 1m 19s\tremaining: 8m 46s\n",
      "131: learn: 0.7607826\ttest: 0.7263385\tbestTest: 0.7263385 (131)\ttotal: 1m 19s\tremaining: 8m 45s\n",
      "132: learn: 0.7609452\ttest: 0.726459\tbestTest: 0.726459 (132)\ttotal: 1m 20s\tremaining: 8m 44s\n",
      "133: learn: 0.7612307\ttest: 0.7263224\tbestTest: 0.726459 (132)\ttotal: 1m 21s\tremaining: 8m 45s\n",
      "134: learn: 0.7612672\ttest: 0.7263962\tbestTest: 0.726459 (132)\ttotal: 1m 21s\tremaining: 8m 43s\n",
      "135: learn: 0.7615057\ttest: 0.7263976\tbestTest: 0.726459 (132)\ttotal: 1m 22s\tremaining: 8m 43s\n",
      "136: learn: 0.7616253\ttest: 0.7265131\tbestTest: 0.7265131 (136)\ttotal: 1m 22s\tremaining: 8m 42s\n",
      "137: learn: 0.7617226\ttest: 0.7267604\tbestTest: 0.7267604 (137)\ttotal: 1m 23s\tremaining: 8m 42s\n",
      "138: learn: 0.761703\ttest: 0.7267625\tbestTest: 0.7267625 (138)\ttotal: 1m 24s\tremaining: 8m 42s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139: learn: 0.7621605\ttest: 0.7265708\tbestTest: 0.7267625 (138)\ttotal: 1m 24s\tremaining: 8m 41s\n",
      "140: learn: 0.7627892\ttest: 0.7265833\tbestTest: 0.7267625 (138)\ttotal: 1m 25s\tremaining: 8m 41s\n",
      "141: learn: 0.7629334\ttest: 0.7267415\tbestTest: 0.7267625 (138)\ttotal: 1m 26s\tremaining: 8m 39s\n",
      "142: learn: 0.7630229\ttest: 0.7269216\tbestTest: 0.7269216 (142)\ttotal: 1m 26s\tremaining: 8m 38s\n",
      "143: learn: 0.7633951\ttest: 0.7270695\tbestTest: 0.7270695 (143)\ttotal: 1m 27s\tremaining: 8m 38s\n",
      "144: learn: 0.7634076\ttest: 0.7270697\tbestTest: 0.7270697 (144)\ttotal: 1m 27s\tremaining: 8m 37s\n",
      "145: learn: 0.763466\ttest: 0.7269557\tbestTest: 0.7270697 (144)\ttotal: 1m 28s\tremaining: 8m 36s\n",
      "146: learn: 0.7635444\ttest: 0.7268789\tbestTest: 0.7270697 (144)\ttotal: 1m 29s\tremaining: 8m 36s\n",
      "147: learn: 0.7636646\ttest: 0.726879\tbestTest: 0.7270697 (144)\ttotal: 1m 29s\tremaining: 8m 36s\n",
      "148: learn: 0.7642893\ttest: 0.7269747\tbestTest: 0.7270697 (144)\ttotal: 1m 30s\tremaining: 8m 36s\n",
      "149: learn: 0.7643894\ttest: 0.7269856\tbestTest: 0.7270697 (144)\ttotal: 1m 31s\tremaining: 8m 35s\n",
      "150: learn: 0.7645221\ttest: 0.7268618\tbestTest: 0.7270697 (144)\ttotal: 1m 31s\tremaining: 8m 35s\n",
      "151: learn: 0.7645794\ttest: 0.726958\tbestTest: 0.7270697 (144)\ttotal: 1m 32s\tremaining: 8m 34s\n",
      "152: learn: 0.7647328\ttest: 0.7270189\tbestTest: 0.7270697 (144)\ttotal: 1m 32s\tremaining: 8m 33s\n",
      "153: learn: 0.7651238\ttest: 0.7271023\tbestTest: 0.7271023 (153)\ttotal: 1m 33s\tremaining: 8m 33s\n",
      "154: learn: 0.7651147\ttest: 0.7270934\tbestTest: 0.7271023 (153)\ttotal: 1m 34s\tremaining: 8m 32s\n",
      "155: learn: 0.765111\ttest: 0.7270911\tbestTest: 0.7271023 (153)\ttotal: 1m 34s\tremaining: 8m 32s\n",
      "156: learn: 0.7651256\ttest: 0.7270992\tbestTest: 0.7271023 (153)\ttotal: 1m 35s\tremaining: 8m 32s\n",
      "157: learn: 0.7655425\ttest: 0.7269895\tbestTest: 0.7271023 (153)\ttotal: 1m 35s\tremaining: 8m 31s\n",
      "158: learn: 0.7655741\ttest: 0.7271221\tbestTest: 0.7271221 (158)\ttotal: 1m 36s\tremaining: 8m 30s\n",
      "159: learn: 0.7656026\ttest: 0.7272594\tbestTest: 0.7272594 (159)\ttotal: 1m 37s\tremaining: 8m 29s\n",
      "160: learn: 0.7655971\ttest: 0.7271839\tbestTest: 0.7272594 (159)\ttotal: 1m 37s\tremaining: 8m 29s\n",
      "161: learn: 0.7656459\ttest: 0.727201\tbestTest: 0.7272594 (159)\ttotal: 1m 38s\tremaining: 8m 28s\n",
      "162: learn: 0.7658094\ttest: 0.7271624\tbestTest: 0.7272594 (159)\ttotal: 1m 38s\tremaining: 8m 27s\n",
      "163: learn: 0.7658378\ttest: 0.7271917\tbestTest: 0.7272594 (159)\ttotal: 1m 39s\tremaining: 8m 26s\n",
      "164: learn: 0.7659496\ttest: 0.7271722\tbestTest: 0.7272594 (159)\ttotal: 1m 39s\tremaining: 8m 25s\n",
      "165: learn: 0.7660619\ttest: 0.7273168\tbestTest: 0.7273168 (165)\ttotal: 1m 40s\tremaining: 8m 25s\n",
      "166: learn: 0.7663446\ttest: 0.7276222\tbestTest: 0.7276222 (166)\ttotal: 1m 41s\tremaining: 8m 24s\n",
      "167: learn: 0.76647\ttest: 0.7280236\tbestTest: 0.7280236 (167)\ttotal: 1m 41s\tremaining: 8m 23s\n",
      "168: learn: 0.766483\ttest: 0.7279987\tbestTest: 0.7280236 (167)\ttotal: 1m 42s\tremaining: 8m 22s\n",
      "169: learn: 0.7671068\ttest: 0.7276344\tbestTest: 0.7280236 (167)\ttotal: 1m 42s\tremaining: 8m 22s\n",
      "170: learn: 0.7675523\ttest: 0.7278249\tbestTest: 0.7280236 (167)\ttotal: 1m 43s\tremaining: 8m 21s\n",
      "171: learn: 0.7676054\ttest: 0.7278827\tbestTest: 0.7280236 (167)\ttotal: 1m 43s\tremaining: 8m 20s\n",
      "172: learn: 0.7677354\ttest: 0.7277851\tbestTest: 0.7280236 (167)\ttotal: 1m 44s\tremaining: 8m 19s\n",
      "173: learn: 0.7680464\ttest: 0.7276966\tbestTest: 0.7280236 (167)\ttotal: 1m 45s\tremaining: 8m 19s\n",
      "174: learn: 0.7680458\ttest: 0.7277621\tbestTest: 0.7280236 (167)\ttotal: 1m 45s\tremaining: 8m 18s\n",
      "175: learn: 0.7680529\ttest: 0.7276824\tbestTest: 0.7280236 (167)\ttotal: 1m 46s\tremaining: 8m 17s\n",
      "176: learn: 0.7680538\ttest: 0.727617\tbestTest: 0.7280236 (167)\ttotal: 1m 46s\tremaining: 8m 17s\n",
      "177: learn: 0.7680624\ttest: 0.7276293\tbestTest: 0.7280236 (167)\ttotal: 1m 47s\tremaining: 8m 16s\n",
      "178: learn: 0.7682312\ttest: 0.7275527\tbestTest: 0.7280236 (167)\ttotal: 1m 48s\tremaining: 8m 15s\n",
      "179: learn: 0.7685761\ttest: 0.7276903\tbestTest: 0.7280236 (167)\ttotal: 1m 48s\tremaining: 8m 14s\n",
      "180: learn: 0.7687179\ttest: 0.7274889\tbestTest: 0.7280236 (167)\ttotal: 1m 49s\tremaining: 8m 14s\n",
      "181: learn: 0.7687305\ttest: 0.7274937\tbestTest: 0.7280236 (167)\ttotal: 1m 49s\tremaining: 8m 13s\n",
      "182: learn: 0.768726\ttest: 0.7275742\tbestTest: 0.7280236 (167)\ttotal: 1m 50s\tremaining: 8m 12s\n",
      "183: learn: 0.7689595\ttest: 0.7278775\tbestTest: 0.7280236 (167)\ttotal: 1m 50s\tremaining: 8m 12s\n",
      "184: learn: 0.7690491\ttest: 0.7278936\tbestTest: 0.7280236 (167)\ttotal: 1m 51s\tremaining: 8m 11s\n",
      "185: learn: 0.7691128\ttest: 0.72786\tbestTest: 0.7280236 (167)\ttotal: 1m 52s\tremaining: 8m 10s\n",
      "186: learn: 0.769195\ttest: 0.7277841\tbestTest: 0.7280236 (167)\ttotal: 1m 52s\tremaining: 8m 10s\n",
      "187: learn: 0.7695415\ttest: 0.7277742\tbestTest: 0.7280236 (167)\ttotal: 1m 53s\tremaining: 8m 9s\n",
      "188: learn: 0.7696669\ttest: 0.7277845\tbestTest: 0.7280236 (167)\ttotal: 1m 53s\tremaining: 8m 8s\n",
      "189: learn: 0.7698941\ttest: 0.7279413\tbestTest: 0.7280236 (167)\ttotal: 1m 54s\tremaining: 8m 8s\n",
      "190: learn: 0.7698598\ttest: 0.7279861\tbestTest: 0.7280236 (167)\ttotal: 1m 55s\tremaining: 8m 7s\n",
      "191: learn: 0.7699705\ttest: 0.7280376\tbestTest: 0.7280376 (191)\ttotal: 1m 55s\tremaining: 8m 7s\n",
      "192: learn: 0.7701648\ttest: 0.7277734\tbestTest: 0.7280376 (191)\ttotal: 1m 56s\tremaining: 8m 6s\n",
      "193: learn: 0.7702346\ttest: 0.7277316\tbestTest: 0.7280376 (191)\ttotal: 1m 57s\tremaining: 8m 6s\n",
      "194: learn: 0.7703129\ttest: 0.7280849\tbestTest: 0.7280849 (194)\ttotal: 1m 57s\tremaining: 8m 5s\n",
      "195: learn: 0.7703942\ttest: 0.7280424\tbestTest: 0.7280849 (194)\ttotal: 1m 58s\tremaining: 8m 5s\n",
      "196: learn: 0.7706621\ttest: 0.7282957\tbestTest: 0.7282957 (196)\ttotal: 1m 58s\tremaining: 8m 4s\n",
      "197: learn: 0.77071\ttest: 0.7283767\tbestTest: 0.7283767 (197)\ttotal: 1m 59s\tremaining: 8m 4s\n",
      "198: learn: 0.770822\ttest: 0.7283035\tbestTest: 0.7283767 (197)\ttotal: 2m\tremaining: 8m 3s\n",
      "199: learn: 0.7709255\ttest: 0.728312\tbestTest: 0.7283767 (197)\ttotal: 2m\tremaining: 8m 2s\n",
      "200: learn: 0.7709293\ttest: 0.7283721\tbestTest: 0.7283767 (197)\ttotal: 2m 1s\tremaining: 8m 1s\n",
      "201: learn: 0.7709468\ttest: 0.7284171\tbestTest: 0.7284171 (201)\ttotal: 2m 1s\tremaining: 8m 1s\n",
      "202: learn: 0.7711316\ttest: 0.7284059\tbestTest: 0.7284171 (201)\ttotal: 2m 2s\tremaining: 8m\n",
      "203: learn: 0.7711694\ttest: 0.7281352\tbestTest: 0.7284171 (201)\ttotal: 2m 2s\tremaining: 7m 59s\n",
      "204: learn: 0.7714604\ttest: 0.7282948\tbestTest: 0.7284171 (201)\ttotal: 2m 3s\tremaining: 7m 59s\n",
      "205: learn: 0.771468\ttest: 0.728268\tbestTest: 0.7284171 (201)\ttotal: 2m 4s\tremaining: 7m 58s\n",
      "206: learn: 0.7715487\ttest: 0.7281363\tbestTest: 0.7284171 (201)\ttotal: 2m 4s\tremaining: 7m 58s\n",
      "207: learn: 0.7716821\ttest: 0.7282262\tbestTest: 0.7284171 (201)\ttotal: 2m 5s\tremaining: 7m 57s\n",
      "208: learn: 0.7719085\ttest: 0.7282521\tbestTest: 0.7284171 (201)\ttotal: 2m 5s\tremaining: 7m 56s\n",
      "209: learn: 0.7718961\ttest: 0.7283039\tbestTest: 0.7284171 (201)\ttotal: 2m 6s\tremaining: 7m 56s\n",
      "210: learn: 0.7718991\ttest: 0.7282714\tbestTest: 0.7284171 (201)\ttotal: 2m 7s\tremaining: 7m 55s\n",
      "211: learn: 0.7718885\ttest: 0.7282767\tbestTest: 0.7284171 (201)\ttotal: 2m 7s\tremaining: 7m 54s\n",
      "212: learn: 0.77197\ttest: 0.7282872\tbestTest: 0.7284171 (201)\ttotal: 2m 8s\tremaining: 7m 54s\n",
      "213: learn: 0.77232\ttest: 0.7285484\tbestTest: 0.7285484 (213)\ttotal: 2m 8s\tremaining: 7m 53s\n",
      "214: learn: 0.7723969\ttest: 0.7283981\tbestTest: 0.7285484 (213)\ttotal: 2m 9s\tremaining: 7m 52s\n",
      "215: learn: 0.7724787\ttest: 0.7282419\tbestTest: 0.7285484 (213)\ttotal: 2m 9s\tremaining: 7m 51s\n",
      "216: learn: 0.7725327\ttest: 0.7280774\tbestTest: 0.7285484 (213)\ttotal: 2m 10s\tremaining: 7m 50s\n",
      "217: learn: 0.772935\ttest: 0.7281283\tbestTest: 0.7285484 (213)\ttotal: 2m 11s\tremaining: 7m 50s\n",
      "218: learn: 0.7729425\ttest: 0.7281149\tbestTest: 0.7285484 (213)\ttotal: 2m 11s\tremaining: 7m 49s\n",
      "219: learn: 0.7729804\ttest: 0.7282003\tbestTest: 0.7285484 (213)\ttotal: 2m 12s\tremaining: 7m 49s\n",
      "220: learn: 0.7730905\ttest: 0.7282363\tbestTest: 0.7285484 (213)\ttotal: 2m 12s\tremaining: 7m 48s\n",
      "221: learn: 0.7731956\ttest: 0.727938\tbestTest: 0.7285484 (213)\ttotal: 2m 13s\tremaining: 7m 47s\n",
      "222: learn: 0.7732315\ttest: 0.7279653\tbestTest: 0.7285484 (213)\ttotal: 2m 13s\tremaining: 7m 46s\n",
      "223: learn: 0.7735323\ttest: 0.7279049\tbestTest: 0.7285484 (213)\ttotal: 2m 14s\tremaining: 7m 46s\n",
      "224: learn: 0.7735571\ttest: 0.7280198\tbestTest: 0.7285484 (213)\ttotal: 2m 15s\tremaining: 7m 45s\n",
      "225: learn: 0.7736827\ttest: 0.7281678\tbestTest: 0.7285484 (213)\ttotal: 2m 15s\tremaining: 7m 44s\n",
      "226: learn: 0.7740126\ttest: 0.7282803\tbestTest: 0.7285484 (213)\ttotal: 2m 16s\tremaining: 7m 44s\n",
      "227: learn: 0.7740185\ttest: 0.7282668\tbestTest: 0.7285484 (213)\ttotal: 2m 17s\tremaining: 7m 43s\n",
      "228: learn: 0.7741066\ttest: 0.7282666\tbestTest: 0.7285484 (213)\ttotal: 2m 17s\tremaining: 7m 43s\n",
      "229: learn: 0.7741017\ttest: 0.7282692\tbestTest: 0.7285484 (213)\ttotal: 2m 18s\tremaining: 7m 42s\n",
      "230: learn: 0.7742204\ttest: 0.7282399\tbestTest: 0.7285484 (213)\ttotal: 2m 18s\tremaining: 7m 41s\n",
      "231: learn: 0.7744706\ttest: 0.7285225\tbestTest: 0.7285484 (213)\ttotal: 2m 19s\tremaining: 7m 41s\n",
      "232: learn: 0.7745081\ttest: 0.728475\tbestTest: 0.7285484 (213)\ttotal: 2m 19s\tremaining: 7m 40s\n",
      "233: learn: 0.7745515\ttest: 0.7284538\tbestTest: 0.7285484 (213)\ttotal: 2m 20s\tremaining: 7m 39s\n",
      "234: learn: 0.7746221\ttest: 0.728363\tbestTest: 0.7285484 (213)\ttotal: 2m 21s\tremaining: 7m 39s\n",
      "235: learn: 0.7746235\ttest: 0.7283631\tbestTest: 0.7285484 (213)\ttotal: 2m 21s\tremaining: 7m 38s\n",
      "236: learn: 0.7747987\ttest: 0.7283596\tbestTest: 0.7285484 (213)\ttotal: 2m 22s\tremaining: 7m 37s\n",
      "237: learn: 0.7748985\ttest: 0.728106\tbestTest: 0.7285484 (213)\ttotal: 2m 22s\tremaining: 7m 36s\n",
      "238: learn: 0.7748976\ttest: 0.7281077\tbestTest: 0.7285484 (213)\ttotal: 2m 23s\tremaining: 7m 36s\n",
      "239: learn: 0.7750956\ttest: 0.7282371\tbestTest: 0.7285484 (213)\ttotal: 2m 23s\tremaining: 7m 35s\n",
      "240: learn: 0.7753132\ttest: 0.7283024\tbestTest: 0.7285484 (213)\ttotal: 2m 24s\tremaining: 7m 34s\n",
      "241: learn: 0.7753171\ttest: 0.7282685\tbestTest: 0.7285484 (213)\ttotal: 2m 25s\tremaining: 7m 34s\n",
      "242: learn: 0.7753159\ttest: 0.7282494\tbestTest: 0.7285484 (213)\ttotal: 2m 25s\tremaining: 7m 33s\n",
      "243: learn: 0.7753812\ttest: 0.728218\tbestTest: 0.7285484 (213)\ttotal: 2m 26s\tremaining: 7m 33s\n",
      "244: learn: 0.7756154\ttest: 0.728377\tbestTest: 0.7285484 (213)\ttotal: 2m 26s\tremaining: 7m 32s\n",
      "245: learn: 0.776081\ttest: 0.728217\tbestTest: 0.7285484 (213)\ttotal: 2m 27s\tremaining: 7m 31s\n",
      "246: learn: 0.7761601\ttest: 0.7280782\tbestTest: 0.7285484 (213)\ttotal: 2m 28s\tremaining: 7m 31s\n",
      "247: learn: 0.7761538\ttest: 0.7280726\tbestTest: 0.7285484 (213)\ttotal: 2m 28s\tremaining: 7m 30s\n",
      "248: learn: 0.7762497\ttest: 0.7281885\tbestTest: 0.7285484 (213)\ttotal: 2m 29s\tremaining: 7m 30s\n",
      "249: learn: 0.7762968\ttest: 0.7282658\tbestTest: 0.7285484 (213)\ttotal: 2m 29s\tremaining: 7m 29s\n",
      "250: learn: 0.7762889\ttest: 0.7282803\tbestTest: 0.7285484 (213)\ttotal: 2m 30s\tremaining: 7m 28s\n",
      "251: learn: 0.776352\ttest: 0.7283689\tbestTest: 0.7285484 (213)\ttotal: 2m 30s\tremaining: 7m 27s\n",
      "252: learn: 0.7763346\ttest: 0.7283489\tbestTest: 0.7285484 (213)\ttotal: 2m 31s\tremaining: 7m 27s\n",
      "253: learn: 0.7763917\ttest: 0.7283465\tbestTest: 0.7285484 (213)\ttotal: 2m 32s\tremaining: 7m 26s\n",
      "254: learn: 0.7763758\ttest: 0.7283275\tbestTest: 0.7285484 (213)\ttotal: 2m 32s\tremaining: 7m 26s\n",
      "255: learn: 0.7763752\ttest: 0.7283285\tbestTest: 0.7285484 (213)\ttotal: 2m 33s\tremaining: 7m 24s\n",
      "256: learn: 0.7766361\ttest: 0.7282744\tbestTest: 0.7285484 (213)\ttotal: 2m 33s\tremaining: 7m 24s\n",
      "257: learn: 0.776705\ttest: 0.7281844\tbestTest: 0.7285484 (213)\ttotal: 2m 34s\tremaining: 7m 23s\n",
      "258: learn: 0.7769324\ttest: 0.7282351\tbestTest: 0.7285484 (213)\ttotal: 2m 34s\tremaining: 7m 22s\n",
      "259: learn: 0.7770285\ttest: 0.7282424\tbestTest: 0.7285484 (213)\ttotal: 2m 35s\tremaining: 7m 22s\n",
      "260: learn: 0.7770353\ttest: 0.7281984\tbestTest: 0.7285484 (213)\ttotal: 2m 35s\tremaining: 7m 21s\n",
      "261: learn: 0.7770358\ttest: 0.728192\tbestTest: 0.7285484 (213)\ttotal: 2m 36s\tremaining: 7m 20s\n",
      "262: learn: 0.777244\ttest: 0.7281412\tbestTest: 0.7285484 (213)\ttotal: 2m 37s\tremaining: 7m 20s\n",
      "263: learn: 0.7774871\ttest: 0.7284973\tbestTest: 0.7285484 (213)\ttotal: 2m 37s\tremaining: 7m 19s\n",
      "264: learn: 0.777771\ttest: 0.7285192\tbestTest: 0.7285484 (213)\ttotal: 2m 38s\tremaining: 7m 18s\n",
      "265: learn: 0.7777884\ttest: 0.7285817\tbestTest: 0.7285817 (265)\ttotal: 2m 38s\tremaining: 7m 18s\n",
      "266: learn: 0.7778045\ttest: 0.7285778\tbestTest: 0.7285817 (265)\ttotal: 2m 39s\tremaining: 7m 17s\n",
      "267: learn: 0.7780685\ttest: 0.7288821\tbestTest: 0.7288821 (267)\ttotal: 2m 39s\tremaining: 7m 16s\n",
      "268: learn: 0.7781987\ttest: 0.7287395\tbestTest: 0.7288821 (267)\ttotal: 2m 40s\tremaining: 7m 16s\n",
      "269: learn: 0.7781926\ttest: 0.7286928\tbestTest: 0.7288821 (267)\ttotal: 2m 41s\tremaining: 7m 15s\n",
      "270: learn: 0.7782555\ttest: 0.7285356\tbestTest: 0.7288821 (267)\ttotal: 2m 41s\tremaining: 7m 14s\n",
      "271: learn: 0.7783101\ttest: 0.7285609\tbestTest: 0.7288821 (267)\ttotal: 2m 42s\tremaining: 7m 14s\n",
      "272: learn: 0.7783532\ttest: 0.7284894\tbestTest: 0.7288821 (267)\ttotal: 2m 42s\tremaining: 7m 13s\n",
      "273: learn: 0.7783484\ttest: 0.7284459\tbestTest: 0.7288821 (267)\ttotal: 2m 43s\tremaining: 7m 12s\n",
      "274: learn: 0.7784624\ttest: 0.7284564\tbestTest: 0.7288821 (267)\ttotal: 2m 43s\tremaining: 7m 12s\n",
      "275: learn: 0.7786383\ttest: 0.7286264\tbestTest: 0.7288821 (267)\ttotal: 2m 44s\tremaining: 7m 11s\n",
      "276: learn: 0.778743\ttest: 0.7284359\tbestTest: 0.7288821 (267)\ttotal: 2m 44s\tremaining: 7m 10s\n",
      "277: learn: 0.7787658\ttest: 0.7283597\tbestTest: 0.7288821 (267)\ttotal: 2m 45s\tremaining: 7m 9s\n",
      "278: learn: 0.7790308\ttest: 0.728379\tbestTest: 0.7288821 (267)\ttotal: 2m 46s\tremaining: 7m 9s\n",
      "279: learn: 0.7791167\ttest: 0.7283679\tbestTest: 0.7288821 (267)\ttotal: 2m 46s\tremaining: 7m 8s\n",
      "280: learn: 0.7792648\ttest: 0.7284813\tbestTest: 0.7288821 (267)\ttotal: 2m 47s\tremaining: 7m 7s\n",
      "281: learn: 0.7792745\ttest: 0.7284784\tbestTest: 0.7288821 (267)\ttotal: 2m 47s\tremaining: 7m 7s\n",
      "282: learn: 0.779446\ttest: 0.7282316\tbestTest: 0.7288821 (267)\ttotal: 2m 48s\tremaining: 7m 6s\n",
      "283: learn: 0.779752\ttest: 0.7283979\tbestTest: 0.7288821 (267)\ttotal: 2m 48s\tremaining: 7m 6s\n",
      "284: learn: 0.7798784\ttest: 0.7283787\tbestTest: 0.7288821 (267)\ttotal: 2m 49s\tremaining: 7m 5s\n",
      "285: learn: 0.7799747\ttest: 0.728409\tbestTest: 0.7288821 (267)\ttotal: 2m 50s\tremaining: 7m 4s\n",
      "286: learn: 0.7800234\ttest: 0.7284233\tbestTest: 0.7288821 (267)\ttotal: 2m 50s\tremaining: 7m 4s\n",
      "287: learn: 0.7801177\ttest: 0.7283681\tbestTest: 0.7288821 (267)\ttotal: 2m 51s\tremaining: 7m 3s\n",
      "288: learn: 0.7801089\ttest: 0.7284154\tbestTest: 0.7288821 (267)\ttotal: 2m 51s\tremaining: 7m 2s\n",
      "289: learn: 0.7801869\ttest: 0.7284365\tbestTest: 0.7288821 (267)\ttotal: 2m 52s\tremaining: 7m 2s\n",
      "290: learn: 0.7802873\ttest: 0.7282597\tbestTest: 0.7288821 (267)\ttotal: 2m 53s\tremaining: 7m 1s\n",
      "291: learn: 0.7804438\ttest: 0.7281515\tbestTest: 0.7288821 (267)\ttotal: 2m 53s\tremaining: 7m\n",
      "292: learn: 0.7805539\ttest: 0.7283161\tbestTest: 0.7288821 (267)\ttotal: 2m 54s\tremaining: 7m\n",
      "293: learn: 0.7806701\ttest: 0.7283612\tbestTest: 0.7288821 (267)\ttotal: 2m 54s\tremaining: 6m 59s\n",
      "294: learn: 0.7807799\ttest: 0.72837\tbestTest: 0.7288821 (267)\ttotal: 2m 55s\tremaining: 6m 58s\n",
      "295: learn: 0.7808268\ttest: 0.728327\tbestTest: 0.7288821 (267)\ttotal: 2m 55s\tremaining: 6m 58s\n",
      "296: learn: 0.780924\ttest: 0.7283012\tbestTest: 0.7288821 (267)\ttotal: 2m 56s\tremaining: 6m 57s\n",
      "297: learn: 0.7809952\ttest: 0.7282453\tbestTest: 0.7288821 (267)\ttotal: 2m 57s\tremaining: 6m 57s\n",
      "298: learn: 0.7810594\ttest: 0.728249\tbestTest: 0.7288821 (267)\ttotal: 2m 57s\tremaining: 6m 56s\n",
      "299: learn: 0.7810746\ttest: 0.7282696\tbestTest: 0.7288821 (267)\ttotal: 2m 58s\tremaining: 6m 55s\n",
      "300: learn: 0.7810788\ttest: 0.728299\tbestTest: 0.7288821 (267)\ttotal: 2m 58s\tremaining: 6m 55s\n",
      "301: learn: 0.7812285\ttest: 0.7282306\tbestTest: 0.7288821 (267)\ttotal: 2m 59s\tremaining: 6m 54s\n",
      "302: learn: 0.7812745\ttest: 0.728116\tbestTest: 0.7288821 (267)\ttotal: 2m 59s\tremaining: 6m 53s\n",
      "303: learn: 0.7814762\ttest: 0.7278904\tbestTest: 0.7288821 (267)\ttotal: 3m\tremaining: 6m 53s\n",
      "304: learn: 0.7815431\ttest: 0.7278819\tbestTest: 0.7288821 (267)\ttotal: 3m 1s\tremaining: 6m 52s\n",
      "305: learn: 0.7815753\ttest: 0.727825\tbestTest: 0.7288821 (267)\ttotal: 3m 1s\tremaining: 6m 52s\n",
      "306: learn: 0.7815771\ttest: 0.7278141\tbestTest: 0.7288821 (267)\ttotal: 3m 2s\tremaining: 6m 52s\n",
      "307: learn: 0.7815766\ttest: 0.7278143\tbestTest: 0.7288821 (267)\ttotal: 3m 3s\tremaining: 6m 52s\n",
      "308: learn: 0.7816217\ttest: 0.7276364\tbestTest: 0.7288821 (267)\ttotal: 3m 4s\tremaining: 6m 51s\n",
      "309: learn: 0.7816217\ttest: 0.7276615\tbestTest: 0.7288821 (267)\ttotal: 3m 4s\tremaining: 6m 51s\n",
      "310: learn: 0.7816163\ttest: 0.7276596\tbestTest: 0.7288821 (267)\ttotal: 3m 5s\tremaining: 6m 51s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311: learn: 0.7816293\ttest: 0.727635\tbestTest: 0.7288821 (267)\ttotal: 3m 6s\tremaining: 6m 50s\n",
      "312: learn: 0.7818114\ttest: 0.7275406\tbestTest: 0.7288821 (267)\ttotal: 3m 6s\tremaining: 6m 49s\n",
      "313: learn: 0.7821909\ttest: 0.7276107\tbestTest: 0.7288821 (267)\ttotal: 3m 7s\tremaining: 6m 49s\n",
      "314: learn: 0.7822152\ttest: 0.7277021\tbestTest: 0.7288821 (267)\ttotal: 3m 7s\tremaining: 6m 48s\n",
      "315: learn: 0.7823568\ttest: 0.7277569\tbestTest: 0.7288821 (267)\ttotal: 3m 8s\tremaining: 6m 48s\n",
      "316: learn: 0.782554\ttest: 0.7277114\tbestTest: 0.7288821 (267)\ttotal: 3m 9s\tremaining: 6m 47s\n",
      "317: learn: 0.7827244\ttest: 0.7275747\tbestTest: 0.7288821 (267)\ttotal: 3m 9s\tremaining: 6m 46s\n",
      "318: learn: 0.7827492\ttest: 0.7275525\tbestTest: 0.7288821 (267)\ttotal: 3m 10s\tremaining: 6m 46s\n",
      "319: learn: 0.782795\ttest: 0.7275181\tbestTest: 0.7288821 (267)\ttotal: 3m 10s\tremaining: 6m 45s\n",
      "320: learn: 0.78286\ttest: 0.727485\tbestTest: 0.7288821 (267)\ttotal: 3m 11s\tremaining: 6m 44s\n",
      "321: learn: 0.7830644\ttest: 0.7274365\tbestTest: 0.7288821 (267)\ttotal: 3m 12s\tremaining: 6m 44s\n",
      "322: learn: 0.7831378\ttest: 0.7274958\tbestTest: 0.7288821 (267)\ttotal: 3m 12s\tremaining: 6m 43s\n",
      "323: learn: 0.7832275\ttest: 0.727505\tbestTest: 0.7288821 (267)\ttotal: 3m 13s\tremaining: 6m 43s\n",
      "324: learn: 0.7833813\ttest: 0.7275957\tbestTest: 0.7288821 (267)\ttotal: 3m 13s\tremaining: 6m 42s\n",
      "325: learn: 0.7834043\ttest: 0.7276195\tbestTest: 0.7288821 (267)\ttotal: 3m 14s\tremaining: 6m 42s\n",
      "326: learn: 0.7837507\ttest: 0.7274427\tbestTest: 0.7288821 (267)\ttotal: 3m 15s\tremaining: 6m 41s\n",
      "327: learn: 0.7839215\ttest: 0.7273391\tbestTest: 0.7288821 (267)\ttotal: 3m 15s\tremaining: 6m 41s\n",
      "328: learn: 0.7839983\ttest: 0.7272018\tbestTest: 0.7288821 (267)\ttotal: 3m 16s\tremaining: 6m 40s\n",
      "329: learn: 0.7839941\ttest: 0.7272452\tbestTest: 0.7288821 (267)\ttotal: 3m 17s\tremaining: 6m 40s\n",
      "330: learn: 0.7841239\ttest: 0.7270976\tbestTest: 0.7288821 (267)\ttotal: 3m 18s\tremaining: 6m 40s\n",
      "331: learn: 0.7841163\ttest: 0.7270522\tbestTest: 0.7288821 (267)\ttotal: 3m 19s\tremaining: 6m 40s\n",
      "332: learn: 0.7841065\ttest: 0.727025\tbestTest: 0.7288821 (267)\ttotal: 3m 19s\tremaining: 6m 40s\n",
      "333: learn: 0.7842562\ttest: 0.7271382\tbestTest: 0.7288821 (267)\ttotal: 3m 20s\tremaining: 6m 40s\n",
      "334: learn: 0.7842562\ttest: 0.7270697\tbestTest: 0.7288821 (267)\ttotal: 3m 21s\tremaining: 6m 39s\n",
      "335: learn: 0.7846972\ttest: 0.7271038\tbestTest: 0.7288821 (267)\ttotal: 3m 21s\tremaining: 6m 39s\n",
      "336: learn: 0.784878\ttest: 0.727371\tbestTest: 0.7288821 (267)\ttotal: 3m 22s\tremaining: 6m 39s\n",
      "337: learn: 0.7852664\ttest: 0.7273518\tbestTest: 0.7288821 (267)\ttotal: 3m 23s\tremaining: 6m 39s\n",
      "338: learn: 0.7853848\ttest: 0.7272747\tbestTest: 0.7288821 (267)\ttotal: 3m 24s\tremaining: 6m 39s\n",
      "339: learn: 0.7854971\ttest: 0.7273005\tbestTest: 0.7288821 (267)\ttotal: 3m 25s\tremaining: 6m 39s\n",
      "340: learn: 0.7856285\ttest: 0.727201\tbestTest: 0.7288821 (267)\ttotal: 3m 26s\tremaining: 6m 38s\n",
      "341: learn: 0.7856451\ttest: 0.7270479\tbestTest: 0.7288821 (267)\ttotal: 3m 26s\tremaining: 6m 38s\n",
      "342: learn: 0.7857782\ttest: 0.7271183\tbestTest: 0.7288821 (267)\ttotal: 3m 27s\tremaining: 6m 37s\n",
      "343: learn: 0.7858432\ttest: 0.7270756\tbestTest: 0.7288821 (267)\ttotal: 3m 28s\tremaining: 6m 37s\n",
      "344: learn: 0.78586\ttest: 0.7270128\tbestTest: 0.7288821 (267)\ttotal: 3m 28s\tremaining: 6m 36s\n",
      "345: learn: 0.7858569\ttest: 0.727016\tbestTest: 0.7288821 (267)\ttotal: 3m 29s\tremaining: 6m 35s\n",
      "346: learn: 0.7858605\ttest: 0.727027\tbestTest: 0.7288821 (267)\ttotal: 3m 29s\tremaining: 6m 34s\n",
      "347: learn: 0.7858735\ttest: 0.7270483\tbestTest: 0.7288821 (267)\ttotal: 3m 30s\tremaining: 6m 34s\n",
      "348: learn: 0.7859292\ttest: 0.7270152\tbestTest: 0.7288821 (267)\ttotal: 3m 31s\tremaining: 6m 33s\n",
      "349: learn: 0.7859274\ttest: 0.727016\tbestTest: 0.7288821 (267)\ttotal: 3m 31s\tremaining: 6m 33s\n",
      "350: learn: 0.7859217\ttest: 0.7270344\tbestTest: 0.7288821 (267)\ttotal: 3m 32s\tremaining: 6m 32s\n",
      "351: learn: 0.7861026\ttest: 0.7270453\tbestTest: 0.7288821 (267)\ttotal: 3m 32s\tremaining: 6m 31s\n",
      "352: learn: 0.786502\ttest: 0.7269683\tbestTest: 0.7288821 (267)\ttotal: 3m 33s\tremaining: 6m 31s\n",
      "353: learn: 0.7868254\ttest: 0.7269821\tbestTest: 0.7288821 (267)\ttotal: 3m 33s\tremaining: 6m 30s\n",
      "354: learn: 0.7868307\ttest: 0.7270193\tbestTest: 0.7288821 (267)\ttotal: 3m 34s\tremaining: 6m 29s\n",
      "355: learn: 0.7869041\ttest: 0.726979\tbestTest: 0.7288821 (267)\ttotal: 3m 35s\tremaining: 6m 29s\n",
      "356: learn: 0.7868958\ttest: 0.7269827\tbestTest: 0.7288821 (267)\ttotal: 3m 35s\tremaining: 6m 28s\n",
      "357: learn: 0.7869263\ttest: 0.7269362\tbestTest: 0.7288821 (267)\ttotal: 3m 36s\tremaining: 6m 27s\n",
      "358: learn: 0.7869791\ttest: 0.7272187\tbestTest: 0.7288821 (267)\ttotal: 3m 36s\tremaining: 6m 27s\n",
      "359: learn: 0.7869813\ttest: 0.7272133\tbestTest: 0.7288821 (267)\ttotal: 3m 37s\tremaining: 6m 26s\n",
      "360: learn: 0.7869831\ttest: 0.7272113\tbestTest: 0.7288821 (267)\ttotal: 3m 38s\tremaining: 6m 25s\n",
      "361: learn: 0.7872359\ttest: 0.7271603\tbestTest: 0.7288821 (267)\ttotal: 3m 38s\tremaining: 6m 25s\n",
      "362: learn: 0.7872826\ttest: 0.727239\tbestTest: 0.7288821 (267)\ttotal: 3m 39s\tremaining: 6m 24s\n",
      "363: learn: 0.7874146\ttest: 0.7270383\tbestTest: 0.7288821 (267)\ttotal: 3m 39s\tremaining: 6m 23s\n",
      "364: learn: 0.7874502\ttest: 0.7270505\tbestTest: 0.7288821 (267)\ttotal: 3m 40s\tremaining: 6m 23s\n",
      "365: learn: 0.7874801\ttest: 0.7272535\tbestTest: 0.7288821 (267)\ttotal: 3m 41s\tremaining: 6m 22s\n",
      "366: learn: 0.7874863\ttest: 0.7272251\tbestTest: 0.7288821 (267)\ttotal: 3m 41s\tremaining: 6m 22s\n",
      "367: learn: 0.7874873\ttest: 0.7272317\tbestTest: 0.7288821 (267)\ttotal: 3m 42s\tremaining: 6m 21s\n",
      "368: learn: 0.7875136\ttest: 0.7271284\tbestTest: 0.7288821 (267)\ttotal: 3m 42s\tremaining: 6m 20s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.7288820675\n",
      "bestIteration = 267\n",
      "\n",
      "Depth :  8\n",
      "Train and Test loss :  0.787513637797 0.727128424504\n",
      "[0.73173024442866696, 0.72712842450385662]\n",
      "0: learn: 0.6925513\ttest: 0.6980467\tbestTest: 0.6980467 (0)\ttotal: 1.64s\tremaining: 27m 23s\n",
      "1: learn: 0.7064116\ttest: 0.7120548\tbestTest: 0.7120548 (1)\ttotal: 2.35s\tremaining: 19m 31s\n",
      "2: learn: 0.7119586\ttest: 0.7170902\tbestTest: 0.7170902 (2)\ttotal: 3.07s\tremaining: 16m 59s\n",
      "3: learn: 0.7171015\ttest: 0.7215588\tbestTest: 0.7215588 (3)\ttotal: 3.9s\tremaining: 16m 11s\n",
      "4: learn: 0.7174062\ttest: 0.7216824\tbestTest: 0.7216824 (4)\ttotal: 4.65s\tremaining: 15m 26s\n",
      "5: learn: 0.7164448\ttest: 0.7200229\tbestTest: 0.7216824 (4)\ttotal: 5.34s\tremaining: 14m 45s\n",
      "6: learn: 0.7171196\ttest: 0.7207369\tbestTest: 0.7216824 (4)\ttotal: 6s\tremaining: 14m 11s\n",
      "7: learn: 0.7203516\ttest: 0.7244744\tbestTest: 0.7244744 (7)\ttotal: 6.67s\tremaining: 13m 47s\n",
      "8: learn: 0.7235798\ttest: 0.7266532\tbestTest: 0.7266532 (8)\ttotal: 7.27s\tremaining: 13m 20s\n",
      "9: learn: 0.7240451\ttest: 0.7265314\tbestTest: 0.7266532 (8)\ttotal: 8s\tremaining: 13m 12s\n",
      "10: learn: 0.7253255\ttest: 0.7273509\tbestTest: 0.7273509 (10)\ttotal: 8.66s\tremaining: 12m 58s\n",
      "11: learn: 0.7250201\ttest: 0.7273061\tbestTest: 0.7273509 (10)\ttotal: 9.24s\tremaining: 12m 40s\n",
      "12: learn: 0.7266911\ttest: 0.728869\tbestTest: 0.728869 (12)\ttotal: 9.85s\tremaining: 12m 27s\n",
      "13: learn: 0.7271721\ttest: 0.7292505\tbestTest: 0.7292505 (13)\ttotal: 10.6s\tremaining: 12m 23s\n",
      "14: learn: 0.7275099\ttest: 0.7294072\tbestTest: 0.7294072 (14)\ttotal: 11.2s\tremaining: 12m 12s\n",
      "15: learn: 0.7286158\ttest: 0.7293233\tbestTest: 0.7294072 (14)\ttotal: 11.8s\tremaining: 12m 4s\n",
      "16: learn: 0.7286607\ttest: 0.7287911\tbestTest: 0.7294072 (14)\ttotal: 12.4s\tremaining: 11m 55s\n",
      "17: learn: 0.7300926\ttest: 0.7296609\tbestTest: 0.7296609 (17)\ttotal: 13s\tremaining: 11m 50s\n",
      "18: learn: 0.7304524\ttest: 0.7297501\tbestTest: 0.7297501 (18)\ttotal: 13.6s\tremaining: 11m 39s\n",
      "19: learn: 0.7305984\ttest: 0.7298049\tbestTest: 0.7298049 (19)\ttotal: 14.1s\tremaining: 11m 31s\n",
      "20: learn: 0.7310004\ttest: 0.7299401\tbestTest: 0.7299401 (20)\ttotal: 14.8s\tremaining: 11m 29s\n",
      "21: learn: 0.7310493\ttest: 0.7305686\tbestTest: 0.7305686 (21)\ttotal: 15.4s\tremaining: 11m 24s\n",
      "22: learn: 0.7313608\ttest: 0.7302007\tbestTest: 0.7305686 (21)\ttotal: 16s\tremaining: 11m 20s\n",
      "23: learn: 0.7314587\ttest: 0.7300227\tbestTest: 0.7305686 (21)\ttotal: 16.6s\tremaining: 11m 14s\n",
      "24: learn: 0.7315549\ttest: 0.7301251\tbestTest: 0.7305686 (21)\ttotal: 17.1s\tremaining: 11m 6s\n",
      "25: learn: 0.731791\ttest: 0.730372\tbestTest: 0.7305686 (21)\ttotal: 17.6s\tremaining: 10m 59s\n",
      "26: learn: 0.7327259\ttest: 0.7299663\tbestTest: 0.7305686 (21)\ttotal: 18.3s\tremaining: 10m 59s\n",
      "27: learn: 0.7334258\ttest: 0.7294789\tbestTest: 0.7305686 (21)\ttotal: 18.9s\tremaining: 10m 55s\n",
      "28: learn: 0.7336575\ttest: 0.7295168\tbestTest: 0.7305686 (21)\ttotal: 19.4s\tremaining: 10m 50s\n",
      "29: learn: 0.7339532\ttest: 0.7297192\tbestTest: 0.7305686 (21)\ttotal: 20s\tremaining: 10m 45s\n",
      "30: learn: 0.7339825\ttest: 0.729795\tbestTest: 0.7305686 (21)\ttotal: 20.6s\tremaining: 10m 43s\n",
      "31: learn: 0.7341915\ttest: 0.7296155\tbestTest: 0.7305686 (21)\ttotal: 21.1s\tremaining: 10m 39s\n",
      "32: learn: 0.7347789\ttest: 0.7292845\tbestTest: 0.7305686 (21)\ttotal: 21.7s\tremaining: 10m 36s\n",
      "33: learn: 0.7348464\ttest: 0.7287613\tbestTest: 0.7305686 (21)\ttotal: 22.2s\tremaining: 10m 31s\n",
      "34: learn: 0.7350138\ttest: 0.7284957\tbestTest: 0.7305686 (21)\ttotal: 22.8s\tremaining: 10m 29s\n",
      "35: learn: 0.7351239\ttest: 0.7285006\tbestTest: 0.7305686 (21)\ttotal: 23.4s\tremaining: 10m 26s\n",
      "36: learn: 0.7353495\ttest: 0.7284475\tbestTest: 0.7305686 (21)\ttotal: 24s\tremaining: 10m 24s\n",
      "37: learn: 0.7353267\ttest: 0.7284198\tbestTest: 0.7305686 (21)\ttotal: 24.5s\tremaining: 10m 20s\n",
      "38: learn: 0.7358617\ttest: 0.7286988\tbestTest: 0.7305686 (21)\ttotal: 25.1s\tremaining: 10m 18s\n",
      "39: learn: 0.7361643\ttest: 0.7284065\tbestTest: 0.7305686 (21)\ttotal: 25.6s\tremaining: 10m 14s\n",
      "40: learn: 0.7362093\ttest: 0.7282825\tbestTest: 0.7305686 (21)\ttotal: 26.2s\tremaining: 10m 13s\n",
      "41: learn: 0.7364635\ttest: 0.7282074\tbestTest: 0.7305686 (21)\ttotal: 26.8s\tremaining: 10m 10s\n",
      "42: learn: 0.7373374\ttest: 0.7288303\tbestTest: 0.7305686 (21)\ttotal: 27.4s\tremaining: 10m 9s\n",
      "43: learn: 0.7374866\ttest: 0.7284995\tbestTest: 0.7305686 (21)\ttotal: 28s\tremaining: 10m 8s\n",
      "44: learn: 0.7374849\ttest: 0.7284655\tbestTest: 0.7305686 (21)\ttotal: 28.7s\tremaining: 10m 9s\n",
      "45: learn: 0.7376462\ttest: 0.7283597\tbestTest: 0.7305686 (21)\ttotal: 29.4s\tremaining: 10m 8s\n",
      "46: learn: 0.7376707\ttest: 0.7280244\tbestTest: 0.7305686 (21)\ttotal: 29.9s\tremaining: 10m 5s\n",
      "47: learn: 0.7379125\ttest: 0.7279728\tbestTest: 0.7305686 (21)\ttotal: 30.5s\tremaining: 10m 4s\n",
      "48: learn: 0.7381958\ttest: 0.7280302\tbestTest: 0.7305686 (21)\ttotal: 31.1s\tremaining: 10m 3s\n",
      "49: learn: 0.7383852\ttest: 0.7279844\tbestTest: 0.7305686 (21)\ttotal: 31.6s\tremaining: 10m\n",
      "50: learn: 0.7388844\ttest: 0.7281673\tbestTest: 0.7305686 (21)\ttotal: 32.2s\tremaining: 9m 58s\n",
      "51: learn: 0.7390238\ttest: 0.7284107\tbestTest: 0.7305686 (21)\ttotal: 32.8s\tremaining: 9m 57s\n",
      "52: learn: 0.7391235\ttest: 0.7284804\tbestTest: 0.7305686 (21)\ttotal: 33.4s\tremaining: 9m 57s\n",
      "53: learn: 0.7392457\ttest: 0.7284468\tbestTest: 0.7305686 (21)\ttotal: 33.9s\tremaining: 9m 53s\n",
      "54: learn: 0.7396239\ttest: 0.7284201\tbestTest: 0.7305686 (21)\ttotal: 34.6s\tremaining: 9m 53s\n",
      "55: learn: 0.7402028\ttest: 0.7286734\tbestTest: 0.7305686 (21)\ttotal: 35.1s\tremaining: 9m 52s\n",
      "56: learn: 0.7402837\ttest: 0.7285971\tbestTest: 0.7305686 (21)\ttotal: 35.7s\tremaining: 9m 50s\n",
      "57: learn: 0.7405842\ttest: 0.7286956\tbestTest: 0.7305686 (21)\ttotal: 36.3s\tremaining: 9m 49s\n",
      "58: learn: 0.7405402\ttest: 0.7289606\tbestTest: 0.7305686 (21)\ttotal: 36.9s\tremaining: 9m 49s\n",
      "59: learn: 0.7407061\ttest: 0.728838\tbestTest: 0.7305686 (21)\ttotal: 37.5s\tremaining: 9m 47s\n",
      "60: learn: 0.7408754\ttest: 0.7289544\tbestTest: 0.7305686 (21)\ttotal: 38.1s\tremaining: 9m 46s\n",
      "61: learn: 0.7409622\ttest: 0.7289521\tbestTest: 0.7305686 (21)\ttotal: 38.6s\tremaining: 9m 44s\n",
      "62: learn: 0.741816\ttest: 0.7286551\tbestTest: 0.7305686 (21)\ttotal: 39.2s\tremaining: 9m 42s\n",
      "63: learn: 0.7420636\ttest: 0.7288436\tbestTest: 0.7305686 (21)\ttotal: 39.7s\tremaining: 9m 41s\n",
      "64: learn: 0.7427023\ttest: 0.7289363\tbestTest: 0.7305686 (21)\ttotal: 40.4s\tremaining: 9m 41s\n",
      "65: learn: 0.7429894\ttest: 0.7286918\tbestTest: 0.7305686 (21)\ttotal: 41s\tremaining: 9m 40s\n",
      "66: learn: 0.7430372\ttest: 0.7287571\tbestTest: 0.7305686 (21)\ttotal: 41.5s\tremaining: 9m 38s\n",
      "67: learn: 0.7431905\ttest: 0.7289786\tbestTest: 0.7305686 (21)\ttotal: 42.2s\tremaining: 9m 37s\n",
      "68: learn: 0.7432244\ttest: 0.7289395\tbestTest: 0.7305686 (21)\ttotal: 42.7s\tremaining: 9m 36s\n",
      "69: learn: 0.7437263\ttest: 0.7290751\tbestTest: 0.7305686 (21)\ttotal: 43.3s\tremaining: 9m 35s\n",
      "70: learn: 0.7441493\ttest: 0.7291344\tbestTest: 0.7305686 (21)\ttotal: 43.9s\tremaining: 9m 33s\n",
      "71: learn: 0.7441706\ttest: 0.7292486\tbestTest: 0.7305686 (21)\ttotal: 44.4s\tremaining: 9m 32s\n",
      "72: learn: 0.7442922\ttest: 0.7290376\tbestTest: 0.7305686 (21)\ttotal: 45.1s\tremaining: 9m 33s\n",
      "73: learn: 0.7447779\ttest: 0.7290569\tbestTest: 0.7305686 (21)\ttotal: 45.7s\tremaining: 9m 31s\n",
      "74: learn: 0.7448082\ttest: 0.7290061\tbestTest: 0.7305686 (21)\ttotal: 46.3s\tremaining: 9m 31s\n",
      "75: learn: 0.7448163\ttest: 0.7291152\tbestTest: 0.7305686 (21)\ttotal: 46.9s\tremaining: 9m 29s\n",
      "76: learn: 0.7449839\ttest: 0.7289242\tbestTest: 0.7305686 (21)\ttotal: 47.5s\tremaining: 9m 29s\n",
      "77: learn: 0.745453\ttest: 0.7286537\tbestTest: 0.7305686 (21)\ttotal: 48.1s\tremaining: 9m 28s\n",
      "78: learn: 0.7464207\ttest: 0.728402\tbestTest: 0.7305686 (21)\ttotal: 48.6s\tremaining: 9m 27s\n",
      "79: learn: 0.7467233\ttest: 0.7286513\tbestTest: 0.7305686 (21)\ttotal: 49.3s\tremaining: 9m 26s\n",
      "80: learn: 0.7472418\ttest: 0.7286965\tbestTest: 0.7305686 (21)\ttotal: 49.9s\tremaining: 9m 25s\n",
      "81: learn: 0.747482\ttest: 0.7289088\tbestTest: 0.7305686 (21)\ttotal: 50.4s\tremaining: 9m 24s\n",
      "82: learn: 0.7484633\ttest: 0.7292014\tbestTest: 0.7305686 (21)\ttotal: 51.1s\tremaining: 9m 24s\n",
      "83: learn: 0.7487359\ttest: 0.7294092\tbestTest: 0.7305686 (21)\ttotal: 51.6s\tremaining: 9m 22s\n",
      "84: learn: 0.7491342\ttest: 0.729746\tbestTest: 0.7305686 (21)\ttotal: 52.2s\tremaining: 9m 22s\n",
      "85: learn: 0.749473\ttest: 0.7296465\tbestTest: 0.7305686 (21)\ttotal: 52.7s\tremaining: 9m 19s\n",
      "86: learn: 0.7498352\ttest: 0.7297358\tbestTest: 0.7305686 (21)\ttotal: 53.3s\tremaining: 9m 19s\n",
      "87: learn: 0.7500634\ttest: 0.7298562\tbestTest: 0.7305686 (21)\ttotal: 53.8s\tremaining: 9m 18s\n",
      "88: learn: 0.7501807\ttest: 0.7299112\tbestTest: 0.7305686 (21)\ttotal: 54.4s\tremaining: 9m 16s\n",
      "89: learn: 0.7504922\ttest: 0.7298406\tbestTest: 0.7305686 (21)\ttotal: 55s\tremaining: 9m 15s\n",
      "90: learn: 0.7509802\ttest: 0.730647\tbestTest: 0.730647 (90)\ttotal: 55.6s\tremaining: 9m 14s\n",
      "91: learn: 0.7509785\ttest: 0.7307031\tbestTest: 0.7307031 (91)\ttotal: 56.2s\tremaining: 9m 14s\n",
      "92: learn: 0.751092\ttest: 0.7307103\tbestTest: 0.7307103 (92)\ttotal: 56.7s\tremaining: 9m 12s\n",
      "93: learn: 0.7514556\ttest: 0.7305036\tbestTest: 0.7307103 (92)\ttotal: 57.2s\tremaining: 9m 11s\n",
      "94: learn: 0.7520089\ttest: 0.7303169\tbestTest: 0.7307103 (92)\ttotal: 57.8s\tremaining: 9m 10s\n",
      "95: learn: 0.7520535\ttest: 0.7301702\tbestTest: 0.7307103 (92)\ttotal: 58.5s\tremaining: 9m 10s\n",
      "96: learn: 0.7524027\ttest: 0.7302558\tbestTest: 0.7307103 (92)\ttotal: 59.1s\tremaining: 9m 9s\n",
      "97: learn: 0.7525378\ttest: 0.7304234\tbestTest: 0.7307103 (92)\ttotal: 59.6s\tremaining: 9m 8s\n",
      "98: learn: 0.7527989\ttest: 0.7302264\tbestTest: 0.7307103 (92)\ttotal: 1m\tremaining: 9m 7s\n",
      "99: learn: 0.7528006\ttest: 0.7302368\tbestTest: 0.7307103 (92)\ttotal: 1m\tremaining: 9m 5s\n",
      "100: learn: 0.7528932\ttest: 0.7303393\tbestTest: 0.7307103 (92)\ttotal: 1m 1s\tremaining: 9m 5s\n",
      "101: learn: 0.7531656\ttest: 0.7302215\tbestTest: 0.7307103 (92)\ttotal: 1m 1s\tremaining: 9m 3s\n",
      "102: learn: 0.7535843\ttest: 0.7300081\tbestTest: 0.7307103 (92)\ttotal: 1m 2s\tremaining: 9m 2s\n",
      "103: learn: 0.7537\ttest: 0.7299847\tbestTest: 0.7307103 (92)\ttotal: 1m 2s\tremaining: 9m 1s\n",
      "104: learn: 0.7539999\ttest: 0.7301315\tbestTest: 0.7307103 (92)\ttotal: 1m 3s\tremaining: 9m\n",
      "105: learn: 0.7542295\ttest: 0.7303152\tbestTest: 0.7307103 (92)\ttotal: 1m 3s\tremaining: 8m 59s\n",
      "106: learn: 0.7544591\ttest: 0.7303921\tbestTest: 0.7307103 (92)\ttotal: 1m 4s\tremaining: 8m 58s\n",
      "107: learn: 0.7545415\ttest: 0.7304133\tbestTest: 0.7307103 (92)\ttotal: 1m 5s\tremaining: 8m 57s\n",
      "108: learn: 0.7545532\ttest: 0.7303638\tbestTest: 0.7307103 (92)\ttotal: 1m 5s\tremaining: 8m 56s\n",
      "109: learn: 0.7547065\ttest: 0.7304026\tbestTest: 0.7307103 (92)\ttotal: 1m 6s\tremaining: 8m 55s\n",
      "110: learn: 0.7547049\ttest: 0.7304004\tbestTest: 0.7307103 (92)\ttotal: 1m 6s\tremaining: 8m 54s\n",
      "111: learn: 0.7548097\ttest: 0.7303123\tbestTest: 0.7307103 (92)\ttotal: 1m 7s\tremaining: 8m 52s\n",
      "112: learn: 0.7548598\ttest: 0.7304162\tbestTest: 0.7307103 (92)\ttotal: 1m 7s\tremaining: 8m 51s\n",
      "113: learn: 0.7549459\ttest: 0.7303313\tbestTest: 0.7307103 (92)\ttotal: 1m 8s\tremaining: 8m 50s\n",
      "114: learn: 0.7549966\ttest: 0.7302358\tbestTest: 0.7307103 (92)\ttotal: 1m 8s\tremaining: 8m 49s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115: learn: 0.7551141\ttest: 0.7301016\tbestTest: 0.7307103 (92)\ttotal: 1m 9s\tremaining: 8m 48s\n",
      "116: learn: 0.755217\ttest: 0.7299645\tbestTest: 0.7307103 (92)\ttotal: 1m 9s\tremaining: 8m 47s\n",
      "117: learn: 0.7556328\ttest: 0.7298972\tbestTest: 0.7307103 (92)\ttotal: 1m 10s\tremaining: 8m 46s\n",
      "118: learn: 0.7557918\ttest: 0.7298705\tbestTest: 0.7307103 (92)\ttotal: 1m 10s\tremaining: 8m 45s\n",
      "119: learn: 0.755969\ttest: 0.7297389\tbestTest: 0.7307103 (92)\ttotal: 1m 11s\tremaining: 8m 44s\n",
      "120: learn: 0.7561219\ttest: 0.730004\tbestTest: 0.7307103 (92)\ttotal: 1m 12s\tremaining: 8m 43s\n",
      "121: learn: 0.7563975\ttest: 0.7304945\tbestTest: 0.7307103 (92)\ttotal: 1m 12s\tremaining: 8m 43s\n",
      "122: learn: 0.7565139\ttest: 0.7304363\tbestTest: 0.7307103 (92)\ttotal: 1m 13s\tremaining: 8m 42s\n",
      "123: learn: 0.7565296\ttest: 0.7303374\tbestTest: 0.7307103 (92)\ttotal: 1m 13s\tremaining: 8m 42s\n",
      "124: learn: 0.756694\ttest: 0.730333\tbestTest: 0.7307103 (92)\ttotal: 1m 14s\tremaining: 8m 41s\n",
      "125: learn: 0.7567322\ttest: 0.7304609\tbestTest: 0.7307103 (92)\ttotal: 1m 15s\tremaining: 8m 40s\n",
      "126: learn: 0.7567793\ttest: 0.7303917\tbestTest: 0.7307103 (92)\ttotal: 1m 15s\tremaining: 8m 39s\n",
      "127: learn: 0.7568333\ttest: 0.7303971\tbestTest: 0.7307103 (92)\ttotal: 1m 16s\tremaining: 8m 39s\n",
      "128: learn: 0.7568935\ttest: 0.7303666\tbestTest: 0.7307103 (92)\ttotal: 1m 16s\tremaining: 8m 38s\n",
      "129: learn: 0.7569715\ttest: 0.7300388\tbestTest: 0.7307103 (92)\ttotal: 1m 17s\tremaining: 8m 37s\n",
      "130: learn: 0.7571306\ttest: 0.7302565\tbestTest: 0.7307103 (92)\ttotal: 1m 17s\tremaining: 8m 36s\n",
      "131: learn: 0.7575564\ttest: 0.7302039\tbestTest: 0.7307103 (92)\ttotal: 1m 18s\tremaining: 8m 35s\n",
      "132: learn: 0.7576133\ttest: 0.7301557\tbestTest: 0.7307103 (92)\ttotal: 1m 19s\tremaining: 8m 35s\n",
      "133: learn: 0.7580317\ttest: 0.7303076\tbestTest: 0.7307103 (92)\ttotal: 1m 19s\tremaining: 8m 34s\n",
      "134: learn: 0.7582023\ttest: 0.7303449\tbestTest: 0.7307103 (92)\ttotal: 1m 20s\tremaining: 8m 33s\n",
      "135: learn: 0.7584356\ttest: 0.730401\tbestTest: 0.7307103 (92)\ttotal: 1m 20s\tremaining: 8m 32s\n",
      "136: learn: 0.7585902\ttest: 0.7302432\tbestTest: 0.7307103 (92)\ttotal: 1m 21s\tremaining: 8m 32s\n",
      "137: learn: 0.7586216\ttest: 0.7302646\tbestTest: 0.7307103 (92)\ttotal: 1m 21s\tremaining: 8m 31s\n",
      "138: learn: 0.7586403\ttest: 0.7303327\tbestTest: 0.7307103 (92)\ttotal: 1m 22s\tremaining: 8m 31s\n",
      "139: learn: 0.7587447\ttest: 0.7303376\tbestTest: 0.7307103 (92)\ttotal: 1m 23s\tremaining: 8m 30s\n",
      "140: learn: 0.7595358\ttest: 0.7304826\tbestTest: 0.7307103 (92)\ttotal: 1m 23s\tremaining: 8m 29s\n",
      "141: learn: 0.7598425\ttest: 0.7305672\tbestTest: 0.7307103 (92)\ttotal: 1m 24s\tremaining: 8m 28s\n",
      "142: learn: 0.7603361\ttest: 0.7308645\tbestTest: 0.7308645 (142)\ttotal: 1m 24s\tremaining: 8m 28s\n",
      "143: learn: 0.7605706\ttest: 0.7306728\tbestTest: 0.7308645 (142)\ttotal: 1m 25s\tremaining: 8m 28s\n",
      "144: learn: 0.7607834\ttest: 0.7305153\tbestTest: 0.7308645 (142)\ttotal: 1m 26s\tremaining: 8m 27s\n",
      "145: learn: 0.7608342\ttest: 0.730537\tbestTest: 0.7308645 (142)\ttotal: 1m 26s\tremaining: 8m 26s\n",
      "146: learn: 0.7609713\ttest: 0.7305054\tbestTest: 0.7308645 (142)\ttotal: 1m 27s\tremaining: 8m 26s\n",
      "147: learn: 0.7610908\ttest: 0.7306206\tbestTest: 0.7308645 (142)\ttotal: 1m 27s\tremaining: 8m 25s\n",
      "148: learn: 0.7612903\ttest: 0.7305153\tbestTest: 0.7308645 (142)\ttotal: 1m 28s\tremaining: 8m 24s\n",
      "149: learn: 0.7615496\ttest: 0.7306317\tbestTest: 0.7308645 (142)\ttotal: 1m 29s\tremaining: 8m 24s\n",
      "150: learn: 0.7618696\ttest: 0.7304244\tbestTest: 0.7308645 (142)\ttotal: 1m 29s\tremaining: 8m 24s\n",
      "151: learn: 0.7619637\ttest: 0.7303172\tbestTest: 0.7308645 (142)\ttotal: 1m 30s\tremaining: 8m 23s\n",
      "152: learn: 0.7620693\ttest: 0.7303689\tbestTest: 0.7308645 (142)\ttotal: 1m 30s\tremaining: 8m 23s\n",
      "153: learn: 0.7622106\ttest: 0.7303806\tbestTest: 0.7308645 (142)\ttotal: 1m 31s\tremaining: 8m 22s\n",
      "154: learn: 0.7622577\ttest: 0.7303236\tbestTest: 0.7308645 (142)\ttotal: 1m 32s\tremaining: 8m 21s\n",
      "155: learn: 0.7623218\ttest: 0.7302833\tbestTest: 0.7308645 (142)\ttotal: 1m 32s\tremaining: 8m 20s\n",
      "156: learn: 0.7624875\ttest: 0.7302105\tbestTest: 0.7308645 (142)\ttotal: 1m 33s\tremaining: 8m 20s\n",
      "157: learn: 0.7626427\ttest: 0.7304397\tbestTest: 0.7308645 (142)\ttotal: 1m 33s\tremaining: 8m 19s\n",
      "158: learn: 0.762787\ttest: 0.7303341\tbestTest: 0.7308645 (142)\ttotal: 1m 34s\tremaining: 8m 18s\n",
      "159: learn: 0.7629873\ttest: 0.7305823\tbestTest: 0.7308645 (142)\ttotal: 1m 34s\tremaining: 8m 18s\n",
      "160: learn: 0.7633383\ttest: 0.7303812\tbestTest: 0.7308645 (142)\ttotal: 1m 35s\tremaining: 8m 17s\n",
      "161: learn: 0.7635772\ttest: 0.7306213\tbestTest: 0.7308645 (142)\ttotal: 1m 36s\tremaining: 8m 16s\n",
      "162: learn: 0.7635773\ttest: 0.7306362\tbestTest: 0.7308645 (142)\ttotal: 1m 36s\tremaining: 8m 16s\n",
      "163: learn: 0.7636585\ttest: 0.7307579\tbestTest: 0.7308645 (142)\ttotal: 1m 37s\tremaining: 8m 15s\n",
      "164: learn: 0.7646943\ttest: 0.7308075\tbestTest: 0.7308645 (142)\ttotal: 1m 37s\tremaining: 8m 14s\n",
      "165: learn: 0.7648693\ttest: 0.7307414\tbestTest: 0.7308645 (142)\ttotal: 1m 38s\tremaining: 8m 14s\n",
      "166: learn: 0.7649408\ttest: 0.7307818\tbestTest: 0.7308645 (142)\ttotal: 1m 38s\tremaining: 8m 13s\n",
      "167: learn: 0.7649753\ttest: 0.7308323\tbestTest: 0.7308645 (142)\ttotal: 1m 39s\tremaining: 8m 12s\n",
      "168: learn: 0.7650145\ttest: 0.7307834\tbestTest: 0.7308645 (142)\ttotal: 1m 40s\tremaining: 8m 11s\n",
      "169: learn: 0.7650166\ttest: 0.7306869\tbestTest: 0.7308645 (142)\ttotal: 1m 40s\tremaining: 8m 10s\n",
      "170: learn: 0.765054\ttest: 0.7306303\tbestTest: 0.7308645 (142)\ttotal: 1m 41s\tremaining: 8m 10s\n",
      "171: learn: 0.7652392\ttest: 0.730733\tbestTest: 0.7308645 (142)\ttotal: 1m 41s\tremaining: 8m 9s\n",
      "172: learn: 0.7655996\ttest: 0.7308027\tbestTest: 0.7308645 (142)\ttotal: 1m 42s\tremaining: 8m 8s\n",
      "173: learn: 0.7656214\ttest: 0.7308\tbestTest: 0.7308645 (142)\ttotal: 1m 42s\tremaining: 8m 8s\n",
      "174: learn: 0.7658996\ttest: 0.7304732\tbestTest: 0.7308645 (142)\ttotal: 1m 43s\tremaining: 8m 7s\n",
      "175: learn: 0.7659654\ttest: 0.7303752\tbestTest: 0.7308645 (142)\ttotal: 1m 43s\tremaining: 8m 6s\n",
      "176: learn: 0.7662038\ttest: 0.7303744\tbestTest: 0.7308645 (142)\ttotal: 1m 44s\tremaining: 8m 6s\n",
      "177: learn: 0.7662595\ttest: 0.7302171\tbestTest: 0.7308645 (142)\ttotal: 1m 45s\tremaining: 8m 5s\n",
      "178: learn: 0.7663377\ttest: 0.7302829\tbestTest: 0.7308645 (142)\ttotal: 1m 45s\tremaining: 8m 4s\n",
      "179: learn: 0.7663593\ttest: 0.7303032\tbestTest: 0.7308645 (142)\ttotal: 1m 46s\tremaining: 8m 4s\n",
      "180: learn: 0.7663624\ttest: 0.7303079\tbestTest: 0.7308645 (142)\ttotal: 1m 46s\tremaining: 8m 3s\n",
      "181: learn: 0.7664025\ttest: 0.73028\tbestTest: 0.7308645 (142)\ttotal: 1m 47s\tremaining: 8m 2s\n",
      "182: learn: 0.7667721\ttest: 0.7302667\tbestTest: 0.7308645 (142)\ttotal: 1m 47s\tremaining: 8m 1s\n",
      "183: learn: 0.7670068\ttest: 0.7303189\tbestTest: 0.7308645 (142)\ttotal: 1m 48s\tremaining: 8m 1s\n",
      "184: learn: 0.7671265\ttest: 0.7304323\tbestTest: 0.7308645 (142)\ttotal: 1m 49s\tremaining: 8m 1s\n",
      "185: learn: 0.7672196\ttest: 0.730428\tbestTest: 0.7308645 (142)\ttotal: 1m 49s\tremaining: 8m\n",
      "186: learn: 0.7673121\ttest: 0.7305587\tbestTest: 0.7308645 (142)\ttotal: 1m 50s\tremaining: 7m 59s\n",
      "187: learn: 0.7673978\ttest: 0.7305188\tbestTest: 0.7308645 (142)\ttotal: 1m 50s\tremaining: 7m 59s\n",
      "188: learn: 0.7675089\ttest: 0.7305135\tbestTest: 0.7308645 (142)\ttotal: 1m 51s\tremaining: 7m 58s\n",
      "189: learn: 0.7675515\ttest: 0.7305422\tbestTest: 0.7308645 (142)\ttotal: 1m 52s\tremaining: 7m 57s\n",
      "190: learn: 0.7676839\ttest: 0.7305265\tbestTest: 0.7308645 (142)\ttotal: 1m 52s\tremaining: 7m 56s\n",
      "191: learn: 0.7677111\ttest: 0.7305627\tbestTest: 0.7308645 (142)\ttotal: 1m 53s\tremaining: 7m 56s\n",
      "192: learn: 0.7677273\ttest: 0.7306034\tbestTest: 0.7308645 (142)\ttotal: 1m 53s\tremaining: 7m 55s\n",
      "193: learn: 0.7679515\ttest: 0.7305481\tbestTest: 0.7308645 (142)\ttotal: 1m 54s\tremaining: 7m 55s\n",
      "194: learn: 0.7679905\ttest: 0.7305735\tbestTest: 0.7308645 (142)\ttotal: 1m 55s\tremaining: 7m 55s\n",
      "195: learn: 0.7679964\ttest: 0.7305344\tbestTest: 0.7308645 (142)\ttotal: 1m 55s\tremaining: 7m 54s\n",
      "196: learn: 0.7681209\ttest: 0.7306335\tbestTest: 0.7308645 (142)\ttotal: 1m 56s\tremaining: 7m 53s\n",
      "197: learn: 0.7682444\ttest: 0.7306384\tbestTest: 0.7308645 (142)\ttotal: 1m 56s\tremaining: 7m 53s\n",
      "198: learn: 0.7682822\ttest: 0.7305107\tbestTest: 0.7308645 (142)\ttotal: 1m 57s\tremaining: 7m 52s\n",
      "199: learn: 0.7685334\ttest: 0.730527\tbestTest: 0.7308645 (142)\ttotal: 1m 57s\tremaining: 7m 51s\n",
      "200: learn: 0.768527\ttest: 0.7305793\tbestTest: 0.7308645 (142)\ttotal: 1m 58s\tremaining: 7m 50s\n",
      "201: learn: 0.7685903\ttest: 0.7306155\tbestTest: 0.7308645 (142)\ttotal: 1m 59s\tremaining: 7m 50s\n",
      "202: learn: 0.7687426\ttest: 0.7308261\tbestTest: 0.7308645 (142)\ttotal: 1m 59s\tremaining: 7m 50s\n",
      "203: learn: 0.7687339\ttest: 0.7310192\tbestTest: 0.7310192 (203)\ttotal: 2m\tremaining: 7m 49s\n",
      "204: learn: 0.768744\ttest: 0.7309553\tbestTest: 0.7310192 (203)\ttotal: 2m\tremaining: 7m 49s\n",
      "205: learn: 0.7689095\ttest: 0.7308636\tbestTest: 0.7310192 (203)\ttotal: 2m 1s\tremaining: 7m 48s\n",
      "206: learn: 0.7691723\ttest: 0.7306526\tbestTest: 0.7310192 (203)\ttotal: 2m 2s\tremaining: 7m 47s\n",
      "207: learn: 0.7693396\ttest: 0.7304025\tbestTest: 0.7310192 (203)\ttotal: 2m 2s\tremaining: 7m 47s\n",
      "208: learn: 0.7693451\ttest: 0.7303667\tbestTest: 0.7310192 (203)\ttotal: 2m 3s\tremaining: 7m 46s\n",
      "209: learn: 0.7698409\ttest: 0.7299015\tbestTest: 0.7310192 (203)\ttotal: 2m 3s\tremaining: 7m 45s\n",
      "210: learn: 0.7698267\ttest: 0.7299333\tbestTest: 0.7310192 (203)\ttotal: 2m 4s\tremaining: 7m 44s\n",
      "211: learn: 0.7699497\ttest: 0.7298372\tbestTest: 0.7310192 (203)\ttotal: 2m 4s\tremaining: 7m 44s\n",
      "212: learn: 0.7699544\ttest: 0.7298457\tbestTest: 0.7310192 (203)\ttotal: 2m 5s\tremaining: 7m 43s\n",
      "213: learn: 0.7701024\ttest: 0.7299924\tbestTest: 0.7310192 (203)\ttotal: 2m 6s\tremaining: 7m 43s\n",
      "214: learn: 0.7703198\ttest: 0.7300802\tbestTest: 0.7310192 (203)\ttotal: 2m 6s\tremaining: 7m 42s\n",
      "215: learn: 0.770445\ttest: 0.7300855\tbestTest: 0.7310192 (203)\ttotal: 2m 7s\tremaining: 7m 42s\n",
      "216: learn: 0.7704549\ttest: 0.7301094\tbestTest: 0.7310192 (203)\ttotal: 2m 7s\tremaining: 7m 41s\n",
      "217: learn: 0.7707817\ttest: 0.7304305\tbestTest: 0.7310192 (203)\ttotal: 2m 8s\tremaining: 7m 40s\n",
      "218: learn: 0.7709025\ttest: 0.7304052\tbestTest: 0.7310192 (203)\ttotal: 2m 9s\tremaining: 7m 40s\n",
      "219: learn: 0.7710413\ttest: 0.7303842\tbestTest: 0.7310192 (203)\ttotal: 2m 9s\tremaining: 7m 39s\n",
      "220: learn: 0.7710357\ttest: 0.7304403\tbestTest: 0.7310192 (203)\ttotal: 2m 10s\tremaining: 7m 39s\n",
      "221: learn: 0.7710814\ttest: 0.7303159\tbestTest: 0.7310192 (203)\ttotal: 2m 10s\tremaining: 7m 39s\n",
      "222: learn: 0.7710953\ttest: 0.7303228\tbestTest: 0.7310192 (203)\ttotal: 2m 11s\tremaining: 7m 38s\n",
      "223: learn: 0.7710856\ttest: 0.7302962\tbestTest: 0.7310192 (203)\ttotal: 2m 12s\tremaining: 7m 37s\n",
      "224: learn: 0.7712938\ttest: 0.7302825\tbestTest: 0.7310192 (203)\ttotal: 2m 12s\tremaining: 7m 37s\n",
      "225: learn: 0.7712985\ttest: 0.7302448\tbestTest: 0.7310192 (203)\ttotal: 2m 13s\tremaining: 7m 36s\n",
      "226: learn: 0.7717069\ttest: 0.7302506\tbestTest: 0.7310192 (203)\ttotal: 2m 13s\tremaining: 7m 36s\n",
      "227: learn: 0.7717965\ttest: 0.7304048\tbestTest: 0.7310192 (203)\ttotal: 2m 14s\tremaining: 7m 35s\n",
      "228: learn: 0.7719178\ttest: 0.7304502\tbestTest: 0.7310192 (203)\ttotal: 2m 15s\tremaining: 7m 35s\n",
      "229: learn: 0.7720438\ttest: 0.7303364\tbestTest: 0.7310192 (203)\ttotal: 2m 15s\tremaining: 7m 34s\n",
      "230: learn: 0.7720599\ttest: 0.7302755\tbestTest: 0.7310192 (203)\ttotal: 2m 16s\tremaining: 7m 34s\n",
      "231: learn: 0.7720551\ttest: 0.7302035\tbestTest: 0.7310192 (203)\ttotal: 2m 17s\tremaining: 7m 34s\n",
      "232: learn: 0.7721084\ttest: 0.7302627\tbestTest: 0.7310192 (203)\ttotal: 2m 17s\tremaining: 7m 33s\n",
      "233: learn: 0.7721065\ttest: 0.7302987\tbestTest: 0.7310192 (203)\ttotal: 2m 18s\tremaining: 7m 33s\n",
      "234: learn: 0.7721646\ttest: 0.730371\tbestTest: 0.7310192 (203)\ttotal: 2m 19s\tremaining: 7m 33s\n",
      "235: learn: 0.7721634\ttest: 0.7304105\tbestTest: 0.7310192 (203)\ttotal: 2m 19s\tremaining: 7m 32s\n",
      "236: learn: 0.7728453\ttest: 0.7305396\tbestTest: 0.7310192 (203)\ttotal: 2m 20s\tremaining: 7m 31s\n",
      "237: learn: 0.7728593\ttest: 0.7305604\tbestTest: 0.7310192 (203)\ttotal: 2m 20s\tremaining: 7m 31s\n",
      "238: learn: 0.7728879\ttest: 0.7306164\tbestTest: 0.7310192 (203)\ttotal: 2m 21s\tremaining: 7m 30s\n",
      "239: learn: 0.7728918\ttest: 0.7305752\tbestTest: 0.7310192 (203)\ttotal: 2m 22s\tremaining: 7m 30s\n",
      "240: learn: 0.7729919\ttest: 0.730709\tbestTest: 0.7310192 (203)\ttotal: 2m 23s\tremaining: 7m 30s\n",
      "241: learn: 0.7731497\ttest: 0.7307362\tbestTest: 0.7310192 (203)\ttotal: 2m 23s\tremaining: 7m 30s\n",
      "242: learn: 0.7732472\ttest: 0.7307803\tbestTest: 0.7310192 (203)\ttotal: 2m 24s\tremaining: 7m 31s\n",
      "243: learn: 0.773308\ttest: 0.7307031\tbestTest: 0.7310192 (203)\ttotal: 2m 25s\tremaining: 7m 30s\n",
      "244: learn: 0.7737946\ttest: 0.7306951\tbestTest: 0.7310192 (203)\ttotal: 2m 26s\tremaining: 7m 30s\n",
      "245: learn: 0.7738975\ttest: 0.7307121\tbestTest: 0.7310192 (203)\ttotal: 2m 26s\tremaining: 7m 29s\n",
      "246: learn: 0.7738905\ttest: 0.7306827\tbestTest: 0.7310192 (203)\ttotal: 2m 27s\tremaining: 7m 28s\n",
      "247: learn: 0.7739812\ttest: 0.7306852\tbestTest: 0.7310192 (203)\ttotal: 2m 27s\tremaining: 7m 28s\n",
      "248: learn: 0.7740958\ttest: 0.7304801\tbestTest: 0.7310192 (203)\ttotal: 2m 28s\tremaining: 7m 27s\n",
      "249: learn: 0.7741307\ttest: 0.730559\tbestTest: 0.7310192 (203)\ttotal: 2m 28s\tremaining: 7m 26s\n",
      "250: learn: 0.7743199\ttest: 0.7306471\tbestTest: 0.7310192 (203)\ttotal: 2m 29s\tremaining: 7m 25s\n",
      "251: learn: 0.774319\ttest: 0.7306497\tbestTest: 0.7310192 (203)\ttotal: 2m 29s\tremaining: 7m 24s\n",
      "252: learn: 0.7744168\ttest: 0.7306514\tbestTest: 0.7310192 (203)\ttotal: 2m 30s\tremaining: 7m 24s\n",
      "253: learn: 0.7744195\ttest: 0.7306501\tbestTest: 0.7310192 (203)\ttotal: 2m 31s\tremaining: 7m 24s\n",
      "254: learn: 0.7745592\ttest: 0.7308274\tbestTest: 0.7310192 (203)\ttotal: 2m 31s\tremaining: 7m 23s\n",
      "255: learn: 0.7746313\ttest: 0.7308898\tbestTest: 0.7310192 (203)\ttotal: 2m 32s\tremaining: 7m 23s\n",
      "256: learn: 0.7746471\ttest: 0.7308899\tbestTest: 0.7310192 (203)\ttotal: 2m 33s\tremaining: 7m 22s\n",
      "257: learn: 0.7746716\ttest: 0.7309385\tbestTest: 0.7310192 (203)\ttotal: 2m 33s\tremaining: 7m 21s\n",
      "258: learn: 0.7749107\ttest: 0.7311501\tbestTest: 0.7311501 (258)\ttotal: 2m 34s\tremaining: 7m 21s\n",
      "259: learn: 0.7749067\ttest: 0.7311557\tbestTest: 0.7311557 (259)\ttotal: 2m 34s\tremaining: 7m 20s\n",
      "260: learn: 0.774905\ttest: 0.7311351\tbestTest: 0.7311557 (259)\ttotal: 2m 35s\tremaining: 7m 19s\n",
      "261: learn: 0.7749335\ttest: 0.7311984\tbestTest: 0.7311984 (261)\ttotal: 2m 35s\tremaining: 7m 19s\n",
      "262: learn: 0.7749694\ttest: 0.7311536\tbestTest: 0.7311984 (261)\ttotal: 2m 36s\tremaining: 7m 18s\n",
      "263: learn: 0.7749865\ttest: 0.731165\tbestTest: 0.7311984 (261)\ttotal: 2m 36s\tremaining: 7m 17s\n",
      "264: learn: 0.7752427\ttest: 0.7311385\tbestTest: 0.7311984 (261)\ttotal: 2m 37s\tremaining: 7m 16s\n",
      "265: learn: 0.7755419\ttest: 0.7311308\tbestTest: 0.7311984 (261)\ttotal: 2m 38s\tremaining: 7m 16s\n",
      "266: learn: 0.7756425\ttest: 0.7311466\tbestTest: 0.7311984 (261)\ttotal: 2m 38s\tremaining: 7m 15s\n",
      "267: learn: 0.7756269\ttest: 0.7311376\tbestTest: 0.7311984 (261)\ttotal: 2m 39s\tremaining: 7m 14s\n",
      "268: learn: 0.7756288\ttest: 0.7311351\tbestTest: 0.7311984 (261)\ttotal: 2m 39s\tremaining: 7m 13s\n",
      "269: learn: 0.7758507\ttest: 0.7312488\tbestTest: 0.7312488 (269)\ttotal: 2m 40s\tremaining: 7m 13s\n",
      "270: learn: 0.7758363\ttest: 0.7313056\tbestTest: 0.7313056 (270)\ttotal: 2m 40s\tremaining: 7m 12s\n",
      "271: learn: 0.7758476\ttest: 0.7312845\tbestTest: 0.7313056 (270)\ttotal: 2m 41s\tremaining: 7m 11s\n",
      "272: learn: 0.775847\ttest: 0.7313228\tbestTest: 0.7313228 (272)\ttotal: 2m 41s\tremaining: 7m 11s\n",
      "273: learn: 0.7763211\ttest: 0.7314724\tbestTest: 0.7314724 (273)\ttotal: 2m 42s\tremaining: 7m 10s\n",
      "274: learn: 0.7763642\ttest: 0.7314982\tbestTest: 0.7314982 (274)\ttotal: 2m 43s\tremaining: 7m 10s\n",
      "275: learn: 0.7763773\ttest: 0.7315143\tbestTest: 0.7315143 (275)\ttotal: 2m 43s\tremaining: 7m 9s\n",
      "276: learn: 0.7763822\ttest: 0.731495\tbestTest: 0.7315143 (275)\ttotal: 2m 44s\tremaining: 7m 8s\n",
      "277: learn: 0.7769204\ttest: 0.7312185\tbestTest: 0.7315143 (275)\ttotal: 2m 44s\tremaining: 7m 8s\n",
      "278: learn: 0.7770488\ttest: 0.7312299\tbestTest: 0.7315143 (275)\ttotal: 2m 45s\tremaining: 7m 7s\n",
      "279: learn: 0.777156\ttest: 0.7310497\tbestTest: 0.7315143 (275)\ttotal: 2m 46s\tremaining: 7m 6s\n",
      "280: learn: 0.7771933\ttest: 0.7310909\tbestTest: 0.7315143 (275)\ttotal: 2m 46s\tremaining: 7m 6s\n",
      "281: learn: 0.7771879\ttest: 0.7311452\tbestTest: 0.7315143 (275)\ttotal: 2m 47s\tremaining: 7m 5s\n",
      "282: learn: 0.7771982\ttest: 0.7311647\tbestTest: 0.7315143 (275)\ttotal: 2m 47s\tremaining: 7m 5s\n",
      "283: learn: 0.7772243\ttest: 0.7312745\tbestTest: 0.7315143 (275)\ttotal: 2m 48s\tremaining: 7m 4s\n",
      "284: learn: 0.7771961\ttest: 0.7312898\tbestTest: 0.7315143 (275)\ttotal: 2m 48s\tremaining: 7m 3s\n",
      "285: learn: 0.777401\ttest: 0.7311956\tbestTest: 0.7315143 (275)\ttotal: 2m 49s\tremaining: 7m 3s\n",
      "286: learn: 0.7775818\ttest: 0.7311096\tbestTest: 0.7315143 (275)\ttotal: 2m 50s\tremaining: 7m 2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287: learn: 0.777647\ttest: 0.7311963\tbestTest: 0.7315143 (275)\ttotal: 2m 50s\tremaining: 7m 1s\n",
      "288: learn: 0.77766\ttest: 0.7312252\tbestTest: 0.7315143 (275)\ttotal: 2m 51s\tremaining: 7m 1s\n",
      "289: learn: 0.7777356\ttest: 0.7312965\tbestTest: 0.7315143 (275)\ttotal: 2m 51s\tremaining: 7m\n",
      "290: learn: 0.7777532\ttest: 0.7312642\tbestTest: 0.7315143 (275)\ttotal: 2m 52s\tremaining: 6m 59s\n",
      "291: learn: 0.7777566\ttest: 0.7312697\tbestTest: 0.7315143 (275)\ttotal: 2m 52s\tremaining: 6m 59s\n",
      "292: learn: 0.7777626\ttest: 0.7313002\tbestTest: 0.7315143 (275)\ttotal: 2m 53s\tremaining: 6m 58s\n",
      "293: learn: 0.7778206\ttest: 0.731349\tbestTest: 0.7315143 (275)\ttotal: 2m 54s\tremaining: 6m 58s\n",
      "294: learn: 0.7779061\ttest: 0.731371\tbestTest: 0.7315143 (275)\ttotal: 2m 54s\tremaining: 6m 57s\n",
      "295: learn: 0.7782206\ttest: 0.7312656\tbestTest: 0.7315143 (275)\ttotal: 2m 55s\tremaining: 6m 57s\n",
      "296: learn: 0.7783627\ttest: 0.7313017\tbestTest: 0.7315143 (275)\ttotal: 2m 55s\tremaining: 6m 56s\n",
      "297: learn: 0.7788094\ttest: 0.7311305\tbestTest: 0.7315143 (275)\ttotal: 2m 56s\tremaining: 6m 55s\n",
      "298: learn: 0.7788094\ttest: 0.7311299\tbestTest: 0.7315143 (275)\ttotal: 2m 57s\tremaining: 6m 54s\n",
      "299: learn: 0.778809\ttest: 0.7311274\tbestTest: 0.7315143 (275)\ttotal: 2m 57s\tremaining: 6m 54s\n",
      "300: learn: 0.7788086\ttest: 0.7311334\tbestTest: 0.7315143 (275)\ttotal: 2m 58s\tremaining: 6m 53s\n",
      "301: learn: 0.7788089\ttest: 0.7311337\tbestTest: 0.7315143 (275)\ttotal: 2m 58s\tremaining: 6m 52s\n",
      "302: learn: 0.7788658\ttest: 0.7311501\tbestTest: 0.7315143 (275)\ttotal: 2m 59s\tremaining: 6m 51s\n",
      "303: learn: 0.7788417\ttest: 0.7312648\tbestTest: 0.7315143 (275)\ttotal: 2m 59s\tremaining: 6m 51s\n",
      "304: learn: 0.778913\ttest: 0.7311943\tbestTest: 0.7315143 (275)\ttotal: 3m\tremaining: 6m 50s\n",
      "305: learn: 0.7789119\ttest: 0.7311898\tbestTest: 0.7315143 (275)\ttotal: 3m\tremaining: 6m 50s\n",
      "306: learn: 0.7789122\ttest: 0.7312483\tbestTest: 0.7315143 (275)\ttotal: 3m 1s\tremaining: 6m 49s\n",
      "307: learn: 0.7790536\ttest: 0.7313443\tbestTest: 0.7315143 (275)\ttotal: 3m 1s\tremaining: 6m 48s\n",
      "308: learn: 0.7790699\ttest: 0.7313842\tbestTest: 0.7315143 (275)\ttotal: 3m 2s\tremaining: 6m 47s\n",
      "309: learn: 0.7790561\ttest: 0.7313956\tbestTest: 0.7315143 (275)\ttotal: 3m 3s\tremaining: 6m 47s\n",
      "310: learn: 0.7790819\ttest: 0.7313495\tbestTest: 0.7315143 (275)\ttotal: 3m 3s\tremaining: 6m 46s\n",
      "311: learn: 0.7790936\ttest: 0.7313352\tbestTest: 0.7315143 (275)\ttotal: 3m 4s\tremaining: 6m 46s\n",
      "312: learn: 0.7793602\ttest: 0.7311039\tbestTest: 0.7315143 (275)\ttotal: 3m 4s\tremaining: 6m 45s\n",
      "313: learn: 0.779458\ttest: 0.7312123\tbestTest: 0.7315143 (275)\ttotal: 3m 5s\tremaining: 6m 44s\n",
      "314: learn: 0.7795882\ttest: 0.7311082\tbestTest: 0.7315143 (275)\ttotal: 3m 5s\tremaining: 6m 44s\n",
      "315: learn: 0.779632\ttest: 0.7311117\tbestTest: 0.7315143 (275)\ttotal: 3m 6s\tremaining: 6m 43s\n",
      "316: learn: 0.7797006\ttest: 0.7310498\tbestTest: 0.7315143 (275)\ttotal: 3m 6s\tremaining: 6m 42s\n",
      "317: learn: 0.7799915\ttest: 0.7311314\tbestTest: 0.7315143 (275)\ttotal: 3m 7s\tremaining: 6m 42s\n",
      "318: learn: 0.7803106\ttest: 0.730939\tbestTest: 0.7315143 (275)\ttotal: 3m 8s\tremaining: 6m 41s\n",
      "319: learn: 0.7803489\ttest: 0.7309416\tbestTest: 0.7315143 (275)\ttotal: 3m 8s\tremaining: 6m 41s\n",
      "320: learn: 0.7803569\ttest: 0.7309123\tbestTest: 0.7315143 (275)\ttotal: 3m 9s\tremaining: 6m 40s\n",
      "321: learn: 0.7803904\ttest: 0.7308853\tbestTest: 0.7315143 (275)\ttotal: 3m 9s\tremaining: 6m 39s\n",
      "322: learn: 0.7804328\ttest: 0.7308276\tbestTest: 0.7315143 (275)\ttotal: 3m 10s\tremaining: 6m 39s\n",
      "323: learn: 0.7804256\ttest: 0.7307683\tbestTest: 0.7315143 (275)\ttotal: 3m 11s\tremaining: 6m 38s\n",
      "324: learn: 0.7805434\ttest: 0.7306486\tbestTest: 0.7315143 (275)\ttotal: 3m 11s\tremaining: 6m 37s\n",
      "325: learn: 0.7805434\ttest: 0.7306592\tbestTest: 0.7315143 (275)\ttotal: 3m 12s\tremaining: 6m 37s\n",
      "326: learn: 0.7805394\ttest: 0.7306644\tbestTest: 0.7315143 (275)\ttotal: 3m 12s\tremaining: 6m 36s\n",
      "327: learn: 0.7806275\ttest: 0.7305939\tbestTest: 0.7315143 (275)\ttotal: 3m 13s\tremaining: 6m 36s\n",
      "328: learn: 0.7806267\ttest: 0.73058\tbestTest: 0.7315143 (275)\ttotal: 3m 13s\tremaining: 6m 35s\n",
      "329: learn: 0.7806569\ttest: 0.7305784\tbestTest: 0.7315143 (275)\ttotal: 3m 14s\tremaining: 6m 34s\n",
      "330: learn: 0.7806572\ttest: 0.7305848\tbestTest: 0.7315143 (275)\ttotal: 3m 14s\tremaining: 6m 34s\n",
      "331: learn: 0.7806575\ttest: 0.7305844\tbestTest: 0.7315143 (275)\ttotal: 3m 15s\tremaining: 6m 33s\n",
      "332: learn: 0.7806488\ttest: 0.7305701\tbestTest: 0.7315143 (275)\ttotal: 3m 16s\tremaining: 6m 32s\n",
      "333: learn: 0.7807913\ttest: 0.73047\tbestTest: 0.7315143 (275)\ttotal: 3m 16s\tremaining: 6m 32s\n",
      "334: learn: 0.7808086\ttest: 0.7304523\tbestTest: 0.7315143 (275)\ttotal: 3m 17s\tremaining: 6m 31s\n",
      "335: learn: 0.7810239\ttest: 0.730546\tbestTest: 0.7315143 (275)\ttotal: 3m 17s\tremaining: 6m 30s\n",
      "336: learn: 0.781023\ttest: 0.730556\tbestTest: 0.7315143 (275)\ttotal: 3m 18s\tremaining: 6m 30s\n",
      "337: learn: 0.7810237\ttest: 0.7305629\tbestTest: 0.7315143 (275)\ttotal: 3m 18s\tremaining: 6m 29s\n",
      "338: learn: 0.781023\ttest: 0.73056\tbestTest: 0.7315143 (275)\ttotal: 3m 19s\tremaining: 6m 28s\n",
      "339: learn: 0.7810315\ttest: 0.7305725\tbestTest: 0.7315143 (275)\ttotal: 3m 20s\tremaining: 6m 28s\n",
      "340: learn: 0.7814186\ttest: 0.7304364\tbestTest: 0.7315143 (275)\ttotal: 3m 20s\tremaining: 6m 27s\n",
      "341: learn: 0.7814461\ttest: 0.7303182\tbestTest: 0.7315143 (275)\ttotal: 3m 21s\tremaining: 6m 26s\n",
      "342: learn: 0.7814418\ttest: 0.730347\tbestTest: 0.7315143 (275)\ttotal: 3m 21s\tremaining: 6m 26s\n",
      "343: learn: 0.7814416\ttest: 0.7303466\tbestTest: 0.7315143 (275)\ttotal: 3m 22s\tremaining: 6m 25s\n",
      "344: learn: 0.7814227\ttest: 0.7302907\tbestTest: 0.7315143 (275)\ttotal: 3m 23s\tremaining: 6m 25s\n",
      "345: learn: 0.7815368\ttest: 0.7303907\tbestTest: 0.7315143 (275)\ttotal: 3m 23s\tremaining: 6m 24s\n",
      "346: learn: 0.7815375\ttest: 0.730391\tbestTest: 0.7315143 (275)\ttotal: 3m 24s\tremaining: 6m 24s\n",
      "347: learn: 0.7815865\ttest: 0.7304041\tbestTest: 0.7315143 (275)\ttotal: 3m 24s\tremaining: 6m 23s\n",
      "348: learn: 0.7815892\ttest: 0.7304119\tbestTest: 0.7315143 (275)\ttotal: 3m 25s\tremaining: 6m 22s\n",
      "349: learn: 0.7815879\ttest: 0.7304048\tbestTest: 0.7315143 (275)\ttotal: 3m 25s\tremaining: 6m 22s\n",
      "350: learn: 0.7816589\ttest: 0.7304336\tbestTest: 0.7315143 (275)\ttotal: 3m 26s\tremaining: 6m 21s\n",
      "351: learn: 0.7816606\ttest: 0.7304481\tbestTest: 0.7315143 (275)\ttotal: 3m 27s\tremaining: 6m 21s\n",
      "352: learn: 0.7817014\ttest: 0.730345\tbestTest: 0.7315143 (275)\ttotal: 3m 27s\tremaining: 6m 20s\n",
      "353: learn: 0.7816965\ttest: 0.730352\tbestTest: 0.7315143 (275)\ttotal: 3m 28s\tremaining: 6m 19s\n",
      "354: learn: 0.7817607\ttest: 0.7304587\tbestTest: 0.7315143 (275)\ttotal: 3m 28s\tremaining: 6m 19s\n",
      "355: learn: 0.7818088\ttest: 0.7305372\tbestTest: 0.7315143 (275)\ttotal: 3m 29s\tremaining: 6m 18s\n",
      "356: learn: 0.7818366\ttest: 0.7304886\tbestTest: 0.7315143 (275)\ttotal: 3m 29s\tremaining: 6m 18s\n",
      "357: learn: 0.7819069\ttest: 0.7305173\tbestTest: 0.7315143 (275)\ttotal: 3m 30s\tremaining: 6m 17s\n",
      "358: learn: 0.7819119\ttest: 0.7304991\tbestTest: 0.7315143 (275)\ttotal: 3m 31s\tremaining: 6m 16s\n",
      "359: learn: 0.7818981\ttest: 0.7305134\tbestTest: 0.7315143 (275)\ttotal: 3m 31s\tremaining: 6m 16s\n",
      "360: learn: 0.7820111\ttest: 0.7304255\tbestTest: 0.7315143 (275)\ttotal: 3m 32s\tremaining: 6m 16s\n",
      "361: learn: 0.7820612\ttest: 0.7303926\tbestTest: 0.7315143 (275)\ttotal: 3m 32s\tremaining: 6m 15s\n",
      "362: learn: 0.7821246\ttest: 0.7303486\tbestTest: 0.7315143 (275)\ttotal: 3m 33s\tremaining: 6m 14s\n",
      "363: learn: 0.7827374\ttest: 0.7302823\tbestTest: 0.7315143 (275)\ttotal: 3m 34s\tremaining: 6m 14s\n",
      "364: learn: 0.7828161\ttest: 0.7302206\tbestTest: 0.7315143 (275)\ttotal: 3m 34s\tremaining: 6m 13s\n",
      "365: learn: 0.7828594\ttest: 0.730269\tbestTest: 0.7315143 (275)\ttotal: 3m 35s\tremaining: 6m 12s\n",
      "366: learn: 0.7829134\ttest: 0.7303087\tbestTest: 0.7315143 (275)\ttotal: 3m 35s\tremaining: 6m 12s\n",
      "367: learn: 0.7831845\ttest: 0.7305301\tbestTest: 0.7315143 (275)\ttotal: 3m 36s\tremaining: 6m 11s\n",
      "368: learn: 0.7832343\ttest: 0.7305703\tbestTest: 0.7315143 (275)\ttotal: 3m 36s\tremaining: 6m 10s\n",
      "369: learn: 0.7832515\ttest: 0.7305869\tbestTest: 0.7315143 (275)\ttotal: 3m 37s\tremaining: 6m 10s\n",
      "370: learn: 0.7833466\ttest: 0.7305578\tbestTest: 0.7315143 (275)\ttotal: 3m 37s\tremaining: 6m 9s\n",
      "371: learn: 0.7833503\ttest: 0.7305576\tbestTest: 0.7315143 (275)\ttotal: 3m 38s\tremaining: 6m 8s\n",
      "372: learn: 0.7833425\ttest: 0.7305549\tbestTest: 0.7315143 (275)\ttotal: 3m 38s\tremaining: 6m 8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373: learn: 0.7833252\ttest: 0.7305231\tbestTest: 0.7315143 (275)\ttotal: 3m 39s\tremaining: 6m 7s\n",
      "374: learn: 0.7833725\ttest: 0.7305734\tbestTest: 0.7315143 (275)\ttotal: 3m 40s\tremaining: 6m 6s\n",
      "375: learn: 0.7836407\ttest: 0.7303301\tbestTest: 0.7315143 (275)\ttotal: 3m 40s\tremaining: 6m 6s\n",
      "376: learn: 0.783633\ttest: 0.7303341\tbestTest: 0.7315143 (275)\ttotal: 3m 41s\tremaining: 6m 5s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.7315143435\n",
      "bestIteration = 275\n",
      "\n",
      "Depth :  8\n",
      "Train and Test loss :  0.783632963102 0.730334079765\n",
      "[0.73173024442866696, 0.72712842450385662, 0.7303340797652994]\n",
      "0: learn: 0.6940769\ttest: 0.6889813\tbestTest: 0.6889813 (0)\ttotal: 467ms\tremaining: 7m 46s\n",
      "1: learn: 0.7047055\ttest: 0.6995002\tbestTest: 0.6995002 (1)\ttotal: 978ms\tremaining: 8m 7s\n",
      "2: learn: 0.7123612\ttest: 0.7076211\tbestTest: 0.7076211 (2)\ttotal: 1.48s\tremaining: 8m 10s\n",
      "3: learn: 0.7183592\ttest: 0.7152891\tbestTest: 0.7152891 (3)\ttotal: 2.03s\tremaining: 8m 26s\n",
      "4: learn: 0.7183256\ttest: 0.715013\tbestTest: 0.7152891 (3)\ttotal: 2.56s\tremaining: 8m 28s\n",
      "5: learn: 0.7214954\ttest: 0.7194004\tbestTest: 0.7194004 (5)\ttotal: 3.13s\tremaining: 8m 38s\n",
      "6: learn: 0.721675\ttest: 0.7194301\tbestTest: 0.7194301 (6)\ttotal: 3.82s\tremaining: 9m 2s\n",
      "7: learn: 0.7222852\ttest: 0.7205111\tbestTest: 0.7205111 (7)\ttotal: 4.38s\tremaining: 9m 2s\n",
      "8: learn: 0.7233744\ttest: 0.7211447\tbestTest: 0.7211447 (8)\ttotal: 4.97s\tremaining: 9m 6s\n",
      "9: learn: 0.7238425\ttest: 0.7209743\tbestTest: 0.7211447 (8)\ttotal: 5.67s\tremaining: 9m 21s\n",
      "10: learn: 0.7247251\ttest: 0.721829\tbestTest: 0.721829 (10)\ttotal: 6.33s\tremaining: 9m 29s\n",
      "11: learn: 0.7251747\ttest: 0.7223502\tbestTest: 0.7223502 (11)\ttotal: 6.92s\tremaining: 9m 29s\n",
      "12: learn: 0.7279929\ttest: 0.7239709\tbestTest: 0.7239709 (12)\ttotal: 7.51s\tremaining: 9m 30s\n",
      "13: learn: 0.7285912\ttest: 0.7244712\tbestTest: 0.7244712 (13)\ttotal: 8.2s\tremaining: 9m 37s\n",
      "14: learn: 0.7287484\ttest: 0.7242448\tbestTest: 0.7244712 (13)\ttotal: 8.88s\tremaining: 9m 43s\n",
      "15: learn: 0.7290344\ttest: 0.7243762\tbestTest: 0.7244712 (13)\ttotal: 9.53s\tremaining: 9m 46s\n",
      "16: learn: 0.7291891\ttest: 0.7241947\tbestTest: 0.7244712 (13)\ttotal: 10.2s\tremaining: 9m 48s\n",
      "17: learn: 0.7305241\ttest: 0.7246193\tbestTest: 0.7246193 (17)\ttotal: 10.7s\tremaining: 9m 44s\n",
      "18: learn: 0.7310005\ttest: 0.7248183\tbestTest: 0.7248183 (18)\ttotal: 11.3s\tremaining: 9m 45s\n",
      "19: learn: 0.7311908\ttest: 0.7248296\tbestTest: 0.7248296 (19)\ttotal: 12s\tremaining: 9m 46s\n",
      "20: learn: 0.7323678\ttest: 0.7251052\tbestTest: 0.7251052 (20)\ttotal: 12.5s\tremaining: 9m 41s\n",
      "21: learn: 0.7325861\ttest: 0.7253586\tbestTest: 0.7253586 (21)\ttotal: 13s\tremaining: 9m 38s\n",
      "22: learn: 0.732653\ttest: 0.7249571\tbestTest: 0.7253586 (21)\ttotal: 13.6s\tremaining: 9m 36s\n",
      "23: learn: 0.7331729\ttest: 0.7250069\tbestTest: 0.7253586 (21)\ttotal: 14.1s\tremaining: 9m 32s\n",
      "24: learn: 0.7336138\ttest: 0.7246597\tbestTest: 0.7253586 (21)\ttotal: 14.7s\tremaining: 9m 34s\n",
      "25: learn: 0.7336715\ttest: 0.7243684\tbestTest: 0.7253586 (21)\ttotal: 15.3s\tremaining: 9m 32s\n",
      "26: learn: 0.7341876\ttest: 0.7249178\tbestTest: 0.7253586 (21)\ttotal: 15.9s\tremaining: 9m 34s\n",
      "27: learn: 0.734249\ttest: 0.7247071\tbestTest: 0.7253586 (21)\ttotal: 16.5s\tremaining: 9m 32s\n",
      "28: learn: 0.734476\ttest: 0.7250082\tbestTest: 0.7253586 (21)\ttotal: 17s\tremaining: 9m 29s\n",
      "29: learn: 0.7348734\ttest: 0.7244606\tbestTest: 0.7253586 (21)\ttotal: 17.6s\tremaining: 9m 28s\n",
      "30: learn: 0.7350336\ttest: 0.724445\tbestTest: 0.7253586 (21)\ttotal: 18.1s\tremaining: 9m 26s\n",
      "31: learn: 0.7356583\ttest: 0.7254214\tbestTest: 0.7254214 (31)\ttotal: 18.7s\tremaining: 9m 25s\n",
      "32: learn: 0.7357664\ttest: 0.7253157\tbestTest: 0.7254214 (31)\ttotal: 19.3s\tremaining: 9m 24s\n",
      "33: learn: 0.7360519\ttest: 0.7249411\tbestTest: 0.7254214 (31)\ttotal: 19.8s\tremaining: 9m 22s\n",
      "34: learn: 0.7363227\ttest: 0.7250132\tbestTest: 0.7254214 (31)\ttotal: 20.5s\tremaining: 9m 24s\n",
      "35: learn: 0.7370615\ttest: 0.7248135\tbestTest: 0.7254214 (31)\ttotal: 20.9s\tremaining: 9m 20s\n",
      "36: learn: 0.7371547\ttest: 0.7247376\tbestTest: 0.7254214 (31)\ttotal: 21.4s\tremaining: 9m 18s\n",
      "37: learn: 0.7375548\ttest: 0.7249725\tbestTest: 0.7254214 (31)\ttotal: 22.1s\tremaining: 9m 19s\n",
      "38: learn: 0.7377557\ttest: 0.725147\tbestTest: 0.7254214 (31)\ttotal: 22.7s\tremaining: 9m 19s\n",
      "39: learn: 0.737854\ttest: 0.7247018\tbestTest: 0.7254214 (31)\ttotal: 23.2s\tremaining: 9m 17s\n",
      "40: learn: 0.7378404\ttest: 0.724919\tbestTest: 0.7254214 (31)\ttotal: 23.8s\tremaining: 9m 16s\n",
      "41: learn: 0.7382164\ttest: 0.7252624\tbestTest: 0.7254214 (31)\ttotal: 24.4s\tremaining: 9m 16s\n",
      "42: learn: 0.7383147\ttest: 0.725604\tbestTest: 0.725604 (42)\ttotal: 24.9s\tremaining: 9m 14s\n",
      "43: learn: 0.7383613\ttest: 0.7255793\tbestTest: 0.725604 (42)\ttotal: 25.5s\tremaining: 9m 14s\n",
      "44: learn: 0.738663\ttest: 0.725551\tbestTest: 0.725604 (42)\ttotal: 26.2s\tremaining: 9m 15s\n",
      "45: learn: 0.7391419\ttest: 0.7251929\tbestTest: 0.725604 (42)\ttotal: 26.7s\tremaining: 9m 13s\n",
      "46: learn: 0.7396649\ttest: 0.7251649\tbestTest: 0.725604 (42)\ttotal: 27.3s\tremaining: 9m 12s\n",
      "47: learn: 0.7398183\ttest: 0.7247971\tbestTest: 0.725604 (42)\ttotal: 27.7s\tremaining: 9m 10s\n",
      "48: learn: 0.7402996\ttest: 0.7246268\tbestTest: 0.725604 (42)\ttotal: 28.3s\tremaining: 9m 10s\n",
      "49: learn: 0.7406337\ttest: 0.7247633\tbestTest: 0.725604 (42)\ttotal: 28.9s\tremaining: 9m 8s\n",
      "50: learn: 0.7406673\ttest: 0.7248198\tbestTest: 0.725604 (42)\ttotal: 29.5s\tremaining: 9m 8s\n",
      "51: learn: 0.7406772\ttest: 0.7247699\tbestTest: 0.725604 (42)\ttotal: 30.1s\tremaining: 9m 8s\n",
      "52: learn: 0.7410215\ttest: 0.7247602\tbestTest: 0.725604 (42)\ttotal: 30.6s\tremaining: 9m 7s\n",
      "53: learn: 0.7411844\ttest: 0.7247727\tbestTest: 0.725604 (42)\ttotal: 31.3s\tremaining: 9m 7s\n",
      "54: learn: 0.7416823\ttest: 0.7252503\tbestTest: 0.725604 (42)\ttotal: 31.8s\tremaining: 9m 6s\n",
      "55: learn: 0.7418084\ttest: 0.7251776\tbestTest: 0.725604 (42)\ttotal: 32.3s\tremaining: 9m 4s\n",
      "56: learn: 0.7417929\ttest: 0.7251877\tbestTest: 0.725604 (42)\ttotal: 32.8s\tremaining: 9m 2s\n",
      "57: learn: 0.741946\ttest: 0.7251165\tbestTest: 0.725604 (42)\ttotal: 33.3s\tremaining: 9m 1s\n",
      "58: learn: 0.7428245\ttest: 0.7252465\tbestTest: 0.725604 (42)\ttotal: 33.9s\tremaining: 9m\n",
      "59: learn: 0.7431149\ttest: 0.7248574\tbestTest: 0.725604 (42)\ttotal: 34.4s\tremaining: 8m 59s\n",
      "60: learn: 0.7431166\ttest: 0.7248474\tbestTest: 0.725604 (42)\ttotal: 34.9s\tremaining: 8m 57s\n",
      "61: learn: 0.7434089\ttest: 0.7246552\tbestTest: 0.725604 (42)\ttotal: 35.5s\tremaining: 8m 56s\n",
      "62: learn: 0.7434769\ttest: 0.7245628\tbestTest: 0.725604 (42)\ttotal: 36s\tremaining: 8m 56s\n",
      "63: learn: 0.7438232\ttest: 0.724451\tbestTest: 0.725604 (42)\ttotal: 36.7s\tremaining: 8m 56s\n",
      "64: learn: 0.7441725\ttest: 0.7240932\tbestTest: 0.725604 (42)\ttotal: 37.2s\tremaining: 8m 55s\n",
      "65: learn: 0.744354\ttest: 0.7241298\tbestTest: 0.725604 (42)\ttotal: 37.8s\tremaining: 8m 54s\n",
      "66: learn: 0.7444862\ttest: 0.7237838\tbestTest: 0.725604 (42)\ttotal: 38.3s\tremaining: 8m 53s\n",
      "67: learn: 0.7445542\ttest: 0.7238809\tbestTest: 0.725604 (42)\ttotal: 39s\tremaining: 8m 54s\n",
      "68: learn: 0.7445808\ttest: 0.7239321\tbestTest: 0.725604 (42)\ttotal: 39.5s\tremaining: 8m 52s\n",
      "69: learn: 0.744828\ttest: 0.7242228\tbestTest: 0.725604 (42)\ttotal: 40s\tremaining: 8m 52s\n",
      "70: learn: 0.7448085\ttest: 0.7242221\tbestTest: 0.725604 (42)\ttotal: 40.6s\tremaining: 8m 51s\n",
      "71: learn: 0.7451437\ttest: 0.7244473\tbestTest: 0.725604 (42)\ttotal: 41.2s\tremaining: 8m 50s\n",
      "72: learn: 0.7453308\ttest: 0.7242662\tbestTest: 0.725604 (42)\ttotal: 41.8s\tremaining: 8m 51s\n",
      "73: learn: 0.7454775\ttest: 0.7242628\tbestTest: 0.725604 (42)\ttotal: 42.4s\tremaining: 8m 50s\n",
      "74: learn: 0.7461652\ttest: 0.7244153\tbestTest: 0.725604 (42)\ttotal: 43.1s\tremaining: 8m 51s\n",
      "75: learn: 0.7462081\ttest: 0.7243478\tbestTest: 0.725604 (42)\ttotal: 43.7s\tremaining: 8m 51s\n",
      "76: learn: 0.7462815\ttest: 0.7241809\tbestTest: 0.725604 (42)\ttotal: 44.4s\tremaining: 8m 52s\n",
      "77: learn: 0.7466122\ttest: 0.7239501\tbestTest: 0.725604 (42)\ttotal: 45.1s\tremaining: 8m 53s\n",
      "78: learn: 0.7466331\ttest: 0.7240826\tbestTest: 0.725604 (42)\ttotal: 45.6s\tremaining: 8m 52s\n",
      "79: learn: 0.746808\ttest: 0.7238429\tbestTest: 0.725604 (42)\ttotal: 46.2s\tremaining: 8m 51s\n",
      "80: learn: 0.7468749\ttest: 0.7238669\tbestTest: 0.725604 (42)\ttotal: 46.9s\tremaining: 8m 51s\n",
      "81: learn: 0.7469369\ttest: 0.7235879\tbestTest: 0.725604 (42)\ttotal: 47.5s\tremaining: 8m 51s\n",
      "82: learn: 0.746948\ttest: 0.723545\tbestTest: 0.725604 (42)\ttotal: 48.1s\tremaining: 8m 51s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83: learn: 0.7471244\ttest: 0.7236056\tbestTest: 0.725604 (42)\ttotal: 48.8s\tremaining: 8m 51s\n",
      "84: learn: 0.7473206\ttest: 0.7230353\tbestTest: 0.725604 (42)\ttotal: 49.5s\tremaining: 8m 53s\n",
      "85: learn: 0.7474035\ttest: 0.723124\tbestTest: 0.725604 (42)\ttotal: 50.1s\tremaining: 8m 52s\n",
      "86: learn: 0.7475613\ttest: 0.7232881\tbestTest: 0.725604 (42)\ttotal: 50.7s\tremaining: 8m 52s\n",
      "87: learn: 0.74775\ttest: 0.7233921\tbestTest: 0.725604 (42)\ttotal: 51.3s\tremaining: 8m 52s\n",
      "88: learn: 0.7479041\ttest: 0.7236599\tbestTest: 0.725604 (42)\ttotal: 52s\tremaining: 8m 52s\n",
      "89: learn: 0.7479665\ttest: 0.7237616\tbestTest: 0.725604 (42)\ttotal: 52.6s\tremaining: 8m 52s\n",
      "90: learn: 0.7481939\ttest: 0.7238355\tbestTest: 0.725604 (42)\ttotal: 53.2s\tremaining: 8m 51s\n",
      "91: learn: 0.7490404\ttest: 0.724277\tbestTest: 0.725604 (42)\ttotal: 53.9s\tremaining: 8m 52s\n",
      "92: learn: 0.7492324\ttest: 0.7244968\tbestTest: 0.725604 (42)\ttotal: 54.5s\tremaining: 8m 51s\n",
      "93: learn: 0.7494144\ttest: 0.7246459\tbestTest: 0.725604 (42)\ttotal: 55s\tremaining: 8m 50s\n",
      "94: learn: 0.7494678\ttest: 0.7246993\tbestTest: 0.725604 (42)\ttotal: 55.6s\tremaining: 8m 49s\n",
      "95: learn: 0.7495503\ttest: 0.7247775\tbestTest: 0.725604 (42)\ttotal: 56.2s\tremaining: 8m 49s\n",
      "96: learn: 0.7495622\ttest: 0.7248614\tbestTest: 0.725604 (42)\ttotal: 56.9s\tremaining: 8m 50s\n",
      "97: learn: 0.749611\ttest: 0.7248715\tbestTest: 0.725604 (42)\ttotal: 57.5s\tremaining: 8m 49s\n",
      "98: learn: 0.7496741\ttest: 0.7245597\tbestTest: 0.725604 (42)\ttotal: 58.1s\tremaining: 8m 48s\n",
      "99: learn: 0.7497288\ttest: 0.72437\tbestTest: 0.725604 (42)\ttotal: 58.6s\tremaining: 8m 47s\n",
      "100: learn: 0.7497639\ttest: 0.7244345\tbestTest: 0.725604 (42)\ttotal: 59.4s\tremaining: 8m 48s\n",
      "101: learn: 0.7499777\ttest: 0.7244302\tbestTest: 0.725604 (42)\ttotal: 60s\tremaining: 8m 47s\n",
      "102: learn: 0.7500402\ttest: 0.7243311\tbestTest: 0.725604 (42)\ttotal: 1m\tremaining: 8m 46s\n",
      "103: learn: 0.7502521\ttest: 0.7242757\tbestTest: 0.725604 (42)\ttotal: 1m\tremaining: 8m 45s\n",
      "104: learn: 0.7505464\ttest: 0.7243568\tbestTest: 0.725604 (42)\ttotal: 1m 1s\tremaining: 8m 44s\n",
      "105: learn: 0.7507864\ttest: 0.7245478\tbestTest: 0.725604 (42)\ttotal: 1m 2s\tremaining: 8m 44s\n",
      "106: learn: 0.7508735\ttest: 0.7244026\tbestTest: 0.725604 (42)\ttotal: 1m 2s\tremaining: 8m 44s\n",
      "107: learn: 0.7508696\ttest: 0.7244277\tbestTest: 0.725604 (42)\ttotal: 1m 3s\tremaining: 8m 44s\n",
      "108: learn: 0.7510449\ttest: 0.7244052\tbestTest: 0.725604 (42)\ttotal: 1m 4s\tremaining: 8m 43s\n",
      "109: learn: 0.7511186\ttest: 0.7242523\tbestTest: 0.725604 (42)\ttotal: 1m 4s\tremaining: 8m 44s\n",
      "110: learn: 0.7511058\ttest: 0.7241311\tbestTest: 0.725604 (42)\ttotal: 1m 5s\tremaining: 8m 43s\n",
      "111: learn: 0.7511055\ttest: 0.7241324\tbestTest: 0.725604 (42)\ttotal: 1m 5s\tremaining: 8m 42s\n",
      "112: learn: 0.7511257\ttest: 0.7241383\tbestTest: 0.725604 (42)\ttotal: 1m 6s\tremaining: 8m 41s\n",
      "113: learn: 0.7511196\ttest: 0.7241982\tbestTest: 0.725604 (42)\ttotal: 1m 6s\tremaining: 8m 40s\n",
      "114: learn: 0.7515961\ttest: 0.7238861\tbestTest: 0.725604 (42)\ttotal: 1m 7s\tremaining: 8m 39s\n",
      "115: learn: 0.751676\ttest: 0.7239461\tbestTest: 0.725604 (42)\ttotal: 1m 8s\tremaining: 8m 38s\n",
      "116: learn: 0.7517772\ttest: 0.7241906\tbestTest: 0.725604 (42)\ttotal: 1m 8s\tremaining: 8m 40s\n",
      "117: learn: 0.751834\ttest: 0.7242848\tbestTest: 0.725604 (42)\ttotal: 1m 9s\tremaining: 8m 39s\n",
      "118: learn: 0.752299\ttest: 0.724262\tbestTest: 0.725604 (42)\ttotal: 1m 9s\tremaining: 8m 38s\n",
      "119: learn: 0.7523252\ttest: 0.7243113\tbestTest: 0.725604 (42)\ttotal: 1m 10s\tremaining: 8m 37s\n",
      "120: learn: 0.7523314\ttest: 0.7243143\tbestTest: 0.725604 (42)\ttotal: 1m 11s\tremaining: 8m 36s\n",
      "121: learn: 0.7523938\ttest: 0.7243039\tbestTest: 0.725604 (42)\ttotal: 1m 11s\tremaining: 8m 36s\n",
      "122: learn: 0.7525215\ttest: 0.7244069\tbestTest: 0.725604 (42)\ttotal: 1m 12s\tremaining: 8m 35s\n",
      "123: learn: 0.7525853\ttest: 0.7243416\tbestTest: 0.725604 (42)\ttotal: 1m 12s\tremaining: 8m 35s\n",
      "124: learn: 0.7527489\ttest: 0.7243222\tbestTest: 0.725604 (42)\ttotal: 1m 13s\tremaining: 8m 35s\n",
      "125: learn: 0.752874\ttest: 0.7235535\tbestTest: 0.725604 (42)\ttotal: 1m 14s\tremaining: 8m 35s\n",
      "126: learn: 0.7531817\ttest: 0.7238597\tbestTest: 0.725604 (42)\ttotal: 1m 14s\tremaining: 8m 34s\n",
      "127: learn: 0.7531744\ttest: 0.72388\tbestTest: 0.725604 (42)\ttotal: 1m 15s\tremaining: 8m 34s\n",
      "128: learn: 0.7534592\ttest: 0.724152\tbestTest: 0.725604 (42)\ttotal: 1m 16s\tremaining: 8m 34s\n",
      "129: learn: 0.7539488\ttest: 0.7241799\tbestTest: 0.725604 (42)\ttotal: 1m 16s\tremaining: 8m 35s\n",
      "130: learn: 0.7539455\ttest: 0.7241691\tbestTest: 0.725604 (42)\ttotal: 1m 17s\tremaining: 8m 34s\n",
      "131: learn: 0.7544134\ttest: 0.7245819\tbestTest: 0.725604 (42)\ttotal: 1m 18s\tremaining: 8m 34s\n",
      "132: learn: 0.7548783\ttest: 0.7245571\tbestTest: 0.725604 (42)\ttotal: 1m 18s\tremaining: 8m 34s\n",
      "133: learn: 0.755207\ttest: 0.7245932\tbestTest: 0.725604 (42)\ttotal: 1m 19s\tremaining: 8m 33s\n",
      "134: learn: 0.7553908\ttest: 0.7246348\tbestTest: 0.725604 (42)\ttotal: 1m 20s\tremaining: 8m 33s\n",
      "135: learn: 0.7554494\ttest: 0.724575\tbestTest: 0.725604 (42)\ttotal: 1m 20s\tremaining: 8m 33s\n",
      "136: learn: 0.7557204\ttest: 0.724761\tbestTest: 0.725604 (42)\ttotal: 1m 21s\tremaining: 8m 32s\n",
      "137: learn: 0.7557379\ttest: 0.724756\tbestTest: 0.725604 (42)\ttotal: 1m 22s\tremaining: 8m 32s\n",
      "138: learn: 0.7559319\ttest: 0.7247164\tbestTest: 0.725604 (42)\ttotal: 1m 22s\tremaining: 8m 31s\n",
      "139: learn: 0.7560767\ttest: 0.724711\tbestTest: 0.725604 (42)\ttotal: 1m 23s\tremaining: 8m 31s\n",
      "140: learn: 0.7564296\ttest: 0.7250474\tbestTest: 0.725604 (42)\ttotal: 1m 23s\tremaining: 8m 30s\n",
      "141: learn: 0.7565622\ttest: 0.7249733\tbestTest: 0.725604 (42)\ttotal: 1m 24s\tremaining: 8m 29s\n",
      "142: learn: 0.7566681\ttest: 0.7247822\tbestTest: 0.725604 (42)\ttotal: 1m 25s\tremaining: 8m 30s\n",
      "143: learn: 0.7569412\ttest: 0.7252917\tbestTest: 0.725604 (42)\ttotal: 1m 25s\tremaining: 8m 29s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.7256040147\n",
      "bestIteration = 42\n",
      "\n",
      "Depth :  8\n",
      "Train and Test loss :  0.756941221148 0.725291702225\n",
      "[0.73173024442866696, 0.72712842450385662, 0.7303340797652994, 0.72529170222518347]\n",
      "0: learn: 0.6932128\ttest: 0.6924295\tbestTest: 0.6924295 (0)\ttotal: 454ms\tremaining: 7m 33s\n",
      "1: learn: 0.7111658\ttest: 0.7095681\tbestTest: 0.7095681 (1)\ttotal: 1.03s\tremaining: 8m 34s\n",
      "2: learn: 0.7149674\ttest: 0.7124572\tbestTest: 0.7124572 (2)\ttotal: 1.68s\tremaining: 9m 16s\n",
      "3: learn: 0.7190736\ttest: 0.7166194\tbestTest: 0.7166194 (3)\ttotal: 2.19s\tremaining: 9m 4s\n",
      "4: learn: 0.7216446\ttest: 0.7177292\tbestTest: 0.7177292 (4)\ttotal: 2.79s\tremaining: 9m 14s\n",
      "5: learn: 0.7232758\ttest: 0.7195161\tbestTest: 0.7195161 (5)\ttotal: 3.48s\tremaining: 9m 37s\n",
      "6: learn: 0.7230995\ttest: 0.7193265\tbestTest: 0.7195161 (5)\ttotal: 4.1s\tremaining: 9m 41s\n",
      "7: learn: 0.7236505\ttest: 0.7193948\tbestTest: 0.7195161 (5)\ttotal: 4.64s\tremaining: 9m 35s\n",
      "8: learn: 0.7250845\ttest: 0.7202018\tbestTest: 0.7202018 (8)\ttotal: 5.26s\tremaining: 9m 39s\n",
      "9: learn: 0.7256023\ttest: 0.7209367\tbestTest: 0.7209367 (9)\ttotal: 6.1s\tremaining: 10m 3s\n",
      "10: learn: 0.7258354\ttest: 0.7211423\tbestTest: 0.7211423 (10)\ttotal: 6.75s\tremaining: 10m 6s\n",
      "11: learn: 0.7264205\ttest: 0.7218657\tbestTest: 0.7218657 (11)\ttotal: 7.43s\tremaining: 10m 11s\n",
      "12: learn: 0.7282145\ttest: 0.7230902\tbestTest: 0.7230902 (12)\ttotal: 7.97s\tremaining: 10m 5s\n",
      "13: learn: 0.7284242\ttest: 0.723263\tbestTest: 0.723263 (13)\ttotal: 8.56s\tremaining: 10m 2s\n",
      "14: learn: 0.7286417\ttest: 0.7232697\tbestTest: 0.7232697 (14)\ttotal: 9.1s\tremaining: 9m 57s\n",
      "15: learn: 0.7289939\ttest: 0.7234629\tbestTest: 0.7234629 (15)\ttotal: 9.76s\tremaining: 10m\n",
      "16: learn: 0.7296668\ttest: 0.7233321\tbestTest: 0.7234629 (15)\ttotal: 10.2s\tremaining: 9m 51s\n",
      "17: learn: 0.7310237\ttest: 0.7238628\tbestTest: 0.7238628 (17)\ttotal: 10.8s\tremaining: 9m 48s\n",
      "18: learn: 0.7312324\ttest: 0.7241278\tbestTest: 0.7241278 (18)\ttotal: 11.4s\tremaining: 9m 50s\n",
      "19: learn: 0.7315123\ttest: 0.7241569\tbestTest: 0.7241569 (19)\ttotal: 12s\tremaining: 9m 46s\n",
      "20: learn: 0.7317425\ttest: 0.7243991\tbestTest: 0.7243991 (20)\ttotal: 12.6s\tremaining: 9m 48s\n",
      "21: learn: 0.7317746\ttest: 0.724659\tbestTest: 0.724659 (21)\ttotal: 13.2s\tremaining: 9m 45s\n",
      "22: learn: 0.7320883\ttest: 0.7249441\tbestTest: 0.7249441 (22)\ttotal: 13.7s\tremaining: 9m 40s\n",
      "23: learn: 0.7322901\ttest: 0.7250591\tbestTest: 0.7250591 (23)\ttotal: 14.2s\tremaining: 9m 38s\n",
      "24: learn: 0.7329335\ttest: 0.725311\tbestTest: 0.725311 (24)\ttotal: 14.9s\tremaining: 9m 40s\n",
      "25: learn: 0.7329075\ttest: 0.7252582\tbestTest: 0.725311 (24)\ttotal: 15.4s\tremaining: 9m 37s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26: learn: 0.7333634\ttest: 0.7258173\tbestTest: 0.7258173 (26)\ttotal: 16.1s\tremaining: 9m 38s\n",
      "27: learn: 0.7338004\ttest: 0.7260871\tbestTest: 0.7260871 (27)\ttotal: 16.6s\tremaining: 9m 36s\n",
      "28: learn: 0.7339469\ttest: 0.7269529\tbestTest: 0.7269529 (28)\ttotal: 17.2s\tremaining: 9m 35s\n",
      "29: learn: 0.7347019\ttest: 0.7276017\tbestTest: 0.7276017 (29)\ttotal: 17.7s\tremaining: 9m 32s\n",
      "30: learn: 0.7347297\ttest: 0.7272911\tbestTest: 0.7276017 (29)\ttotal: 18.4s\tremaining: 9m 33s\n",
      "31: learn: 0.7355336\ttest: 0.72719\tbestTest: 0.7276017 (29)\ttotal: 18.9s\tremaining: 9m 31s\n",
      "32: learn: 0.7358399\ttest: 0.7270979\tbestTest: 0.7276017 (29)\ttotal: 19.5s\tremaining: 9m 32s\n",
      "33: learn: 0.7359887\ttest: 0.7270616\tbestTest: 0.7276017 (29)\ttotal: 20.2s\tremaining: 9m 33s\n",
      "34: learn: 0.7360015\ttest: 0.7270608\tbestTest: 0.7276017 (29)\ttotal: 20.7s\tremaining: 9m 32s\n",
      "35: learn: 0.7361354\ttest: 0.7270323\tbestTest: 0.7276017 (29)\ttotal: 21.4s\tremaining: 9m 32s\n",
      "36: learn: 0.7365462\ttest: 0.7276033\tbestTest: 0.7276033 (36)\ttotal: 22s\tremaining: 9m 32s\n",
      "37: learn: 0.7369378\ttest: 0.727776\tbestTest: 0.727776 (37)\ttotal: 22.6s\tremaining: 9m 31s\n",
      "38: learn: 0.7370636\ttest: 0.7276939\tbestTest: 0.727776 (37)\ttotal: 23.1s\tremaining: 9m 29s\n",
      "39: learn: 0.7376251\ttest: 0.7276837\tbestTest: 0.727776 (37)\ttotal: 23.7s\tremaining: 9m 27s\n",
      "40: learn: 0.7380851\ttest: 0.7280464\tbestTest: 0.7280464 (40)\ttotal: 24.2s\tremaining: 9m 26s\n",
      "41: learn: 0.7383324\ttest: 0.7281321\tbestTest: 0.7281321 (41)\ttotal: 25s\tremaining: 9m 31s\n",
      "42: learn: 0.7385783\ttest: 0.7286775\tbestTest: 0.7286775 (42)\ttotal: 25.8s\tremaining: 9m 33s\n",
      "43: learn: 0.7386796\ttest: 0.7286962\tbestTest: 0.7286962 (43)\ttotal: 26.5s\tremaining: 9m 35s\n",
      "44: learn: 0.7386587\ttest: 0.7287949\tbestTest: 0.7287949 (44)\ttotal: 27.2s\tremaining: 9m 37s\n",
      "45: learn: 0.7389721\ttest: 0.7289264\tbestTest: 0.7289264 (45)\ttotal: 27.9s\tremaining: 9m 38s\n",
      "46: learn: 0.739142\ttest: 0.729224\tbestTest: 0.729224 (46)\ttotal: 28.6s\tremaining: 9m 39s\n",
      "47: learn: 0.7391826\ttest: 0.7291913\tbestTest: 0.729224 (46)\ttotal: 29.2s\tremaining: 9m 38s\n",
      "48: learn: 0.739261\ttest: 0.7291898\tbestTest: 0.729224 (46)\ttotal: 29.8s\tremaining: 9m 38s\n",
      "49: learn: 0.7398873\ttest: 0.7293365\tbestTest: 0.7293365 (49)\ttotal: 30.5s\tremaining: 9m 40s\n",
      "50: learn: 0.7399062\ttest: 0.7291989\tbestTest: 0.7293365 (49)\ttotal: 31.1s\tremaining: 9m 38s\n",
      "51: learn: 0.7399352\ttest: 0.729429\tbestTest: 0.729429 (51)\ttotal: 31.8s\tremaining: 9m 39s\n",
      "52: learn: 0.7405224\ttest: 0.7293733\tbestTest: 0.729429 (51)\ttotal: 32.4s\tremaining: 9m 39s\n",
      "53: learn: 0.7406309\ttest: 0.7294311\tbestTest: 0.7294311 (53)\ttotal: 33.1s\tremaining: 9m 39s\n",
      "54: learn: 0.7417825\ttest: 0.7289621\tbestTest: 0.7294311 (53)\ttotal: 33.7s\tremaining: 9m 39s\n",
      "55: learn: 0.7417438\ttest: 0.728803\tbestTest: 0.7294311 (53)\ttotal: 34.4s\tremaining: 9m 40s\n",
      "56: learn: 0.7422545\ttest: 0.7284161\tbestTest: 0.7294311 (53)\ttotal: 35s\tremaining: 9m 39s\n",
      "57: learn: 0.7424493\ttest: 0.7284587\tbestTest: 0.7294311 (53)\ttotal: 35.6s\tremaining: 9m 37s\n",
      "58: learn: 0.7426779\ttest: 0.7283163\tbestTest: 0.7294311 (53)\ttotal: 36.4s\tremaining: 9m 40s\n",
      "59: learn: 0.7430882\ttest: 0.7285671\tbestTest: 0.7294311 (53)\ttotal: 37.1s\tremaining: 9m 40s\n",
      "60: learn: 0.7431912\ttest: 0.7286862\tbestTest: 0.7294311 (53)\ttotal: 37.6s\tremaining: 9m 39s\n",
      "61: learn: 0.7434938\ttest: 0.7289354\tbestTest: 0.7294311 (53)\ttotal: 38.2s\tremaining: 9m 38s\n",
      "62: learn: 0.7437675\ttest: 0.7288027\tbestTest: 0.7294311 (53)\ttotal: 38.8s\tremaining: 9m 36s\n",
      "63: learn: 0.7438228\ttest: 0.728695\tbestTest: 0.7294311 (53)\ttotal: 39.3s\tremaining: 9m 34s\n",
      "64: learn: 0.7439904\ttest: 0.7288221\tbestTest: 0.7294311 (53)\ttotal: 39.9s\tremaining: 9m 33s\n",
      "65: learn: 0.7440592\ttest: 0.7285729\tbestTest: 0.7294311 (53)\ttotal: 40.5s\tremaining: 9m 33s\n",
      "66: learn: 0.7444937\ttest: 0.7286947\tbestTest: 0.7294311 (53)\ttotal: 41.2s\tremaining: 9m 33s\n",
      "67: learn: 0.7449022\ttest: 0.7286213\tbestTest: 0.7294311 (53)\ttotal: 41.7s\tremaining: 9m 31s\n",
      "68: learn: 0.7449887\ttest: 0.7286564\tbestTest: 0.7294311 (53)\ttotal: 42.3s\tremaining: 9m 30s\n",
      "69: learn: 0.7455228\ttest: 0.7287514\tbestTest: 0.7294311 (53)\ttotal: 43.1s\tremaining: 9m 32s\n",
      "70: learn: 0.7456357\ttest: 0.728817\tbestTest: 0.7294311 (53)\ttotal: 43.8s\tremaining: 9m 33s\n",
      "71: learn: 0.7457355\ttest: 0.728517\tbestTest: 0.7294311 (53)\ttotal: 44.4s\tremaining: 9m 32s\n",
      "72: learn: 0.745908\ttest: 0.7284341\tbestTest: 0.7294311 (53)\ttotal: 45.2s\tremaining: 9m 33s\n",
      "73: learn: 0.746094\ttest: 0.7282295\tbestTest: 0.7294311 (53)\ttotal: 45.8s\tremaining: 9m 33s\n",
      "74: learn: 0.7461958\ttest: 0.7283458\tbestTest: 0.7294311 (53)\ttotal: 46.4s\tremaining: 9m 32s\n",
      "75: learn: 0.7463362\ttest: 0.728253\tbestTest: 0.7294311 (53)\ttotal: 47.2s\tremaining: 9m 33s\n",
      "76: learn: 0.7463934\ttest: 0.7284251\tbestTest: 0.7294311 (53)\ttotal: 47.8s\tremaining: 9m 33s\n",
      "77: learn: 0.746835\ttest: 0.727911\tbestTest: 0.7294311 (53)\ttotal: 48.5s\tremaining: 9m 32s\n",
      "78: learn: 0.7471641\ttest: 0.7280893\tbestTest: 0.7294311 (53)\ttotal: 49s\tremaining: 9m 31s\n",
      "79: learn: 0.7483158\ttest: 0.728412\tbestTest: 0.7294311 (53)\ttotal: 49.6s\tremaining: 9m 30s\n",
      "80: learn: 0.7483583\ttest: 0.7284083\tbestTest: 0.7294311 (53)\ttotal: 50.3s\tremaining: 9m 30s\n",
      "81: learn: 0.7491578\ttest: 0.7286367\tbestTest: 0.7294311 (53)\ttotal: 50.9s\tremaining: 9m 30s\n",
      "82: learn: 0.7494197\ttest: 0.7288128\tbestTest: 0.7294311 (53)\ttotal: 51.6s\tremaining: 9m 30s\n",
      "83: learn: 0.749534\ttest: 0.7289231\tbestTest: 0.7294311 (53)\ttotal: 52.2s\tremaining: 9m 28s\n",
      "84: learn: 0.7496711\ttest: 0.7290623\tbestTest: 0.7294311 (53)\ttotal: 52.9s\tremaining: 9m 29s\n",
      "85: learn: 0.7497021\ttest: 0.7289346\tbestTest: 0.7294311 (53)\ttotal: 53.5s\tremaining: 9m 28s\n",
      "86: learn: 0.7502264\ttest: 0.7290242\tbestTest: 0.7294311 (53)\ttotal: 54.1s\tremaining: 9m 27s\n",
      "87: learn: 0.7503643\ttest: 0.7291111\tbestTest: 0.7294311 (53)\ttotal: 54.8s\tremaining: 9m 27s\n",
      "88: learn: 0.7503975\ttest: 0.7291482\tbestTest: 0.7294311 (53)\ttotal: 55.5s\tremaining: 9m 27s\n",
      "89: learn: 0.7504845\ttest: 0.7290521\tbestTest: 0.7294311 (53)\ttotal: 56.2s\tremaining: 9m 27s\n",
      "90: learn: 0.750696\ttest: 0.7292374\tbestTest: 0.7294311 (53)\ttotal: 56.9s\tremaining: 9m 27s\n",
      "91: learn: 0.7506757\ttest: 0.7292299\tbestTest: 0.7294311 (53)\ttotal: 57.4s\tremaining: 9m 26s\n",
      "92: learn: 0.7510297\ttest: 0.7294489\tbestTest: 0.7294489 (92)\ttotal: 58s\tremaining: 9m 25s\n",
      "93: learn: 0.7511342\ttest: 0.7293787\tbestTest: 0.7294489 (92)\ttotal: 58.6s\tremaining: 9m 24s\n",
      "94: learn: 0.7517499\ttest: 0.7292408\tbestTest: 0.7294489 (92)\ttotal: 59.1s\tremaining: 9m 23s\n",
      "95: learn: 0.7518542\ttest: 0.729283\tbestTest: 0.7294489 (92)\ttotal: 59.8s\tremaining: 9m 22s\n",
      "96: learn: 0.7522722\ttest: 0.7293181\tbestTest: 0.7294489 (92)\ttotal: 1m\tremaining: 9m 21s\n",
      "97: learn: 0.7524644\ttest: 0.7294242\tbestTest: 0.7294489 (92)\ttotal: 1m 1s\tremaining: 9m 24s\n",
      "98: learn: 0.7527058\ttest: 0.7293905\tbestTest: 0.7294489 (92)\ttotal: 1m 2s\tremaining: 9m 28s\n",
      "99: learn: 0.7529353\ttest: 0.7292299\tbestTest: 0.7294489 (92)\ttotal: 1m 3s\tremaining: 9m 29s\n",
      "100: learn: 0.7529808\ttest: 0.7292043\tbestTest: 0.7294489 (92)\ttotal: 1m 3s\tremaining: 9m 29s\n",
      "101: learn: 0.7532193\ttest: 0.7289514\tbestTest: 0.7294489 (92)\ttotal: 1m 4s\tremaining: 9m 28s\n",
      "102: learn: 0.7532075\ttest: 0.7288595\tbestTest: 0.7294489 (92)\ttotal: 1m 5s\tremaining: 9m 28s\n",
      "103: learn: 0.7534539\ttest: 0.7288961\tbestTest: 0.7294489 (92)\ttotal: 1m 6s\tremaining: 9m 28s\n",
      "104: learn: 0.7535028\ttest: 0.7287353\tbestTest: 0.7294489 (92)\ttotal: 1m 6s\tremaining: 9m 27s\n",
      "105: learn: 0.7534957\ttest: 0.7287423\tbestTest: 0.7294489 (92)\ttotal: 1m 7s\tremaining: 9m 26s\n",
      "106: learn: 0.7538187\ttest: 0.7286227\tbestTest: 0.7294489 (92)\ttotal: 1m 7s\tremaining: 9m 24s\n",
      "107: learn: 0.7539834\ttest: 0.728648\tbestTest: 0.7294489 (92)\ttotal: 1m 8s\tremaining: 9m 24s\n",
      "108: learn: 0.7541954\ttest: 0.7285045\tbestTest: 0.7294489 (92)\ttotal: 1m 8s\tremaining: 9m 23s\n",
      "109: learn: 0.7544067\ttest: 0.7284834\tbestTest: 0.7294489 (92)\ttotal: 1m 9s\tremaining: 9m 21s\n",
      "110: learn: 0.7543965\ttest: 0.7284658\tbestTest: 0.7294489 (92)\ttotal: 1m 9s\tremaining: 9m 19s\n",
      "111: learn: 0.7546039\ttest: 0.7284669\tbestTest: 0.7294489 (92)\ttotal: 1m 10s\tremaining: 9m 18s\n",
      "112: learn: 0.7550052\ttest: 0.7283177\tbestTest: 0.7294489 (92)\ttotal: 1m 11s\tremaining: 9m 18s\n",
      "113: learn: 0.7551477\ttest: 0.7282964\tbestTest: 0.7294489 (92)\ttotal: 1m 11s\tremaining: 9m 17s\n",
      "114: learn: 0.7555522\ttest: 0.7285154\tbestTest: 0.7294489 (92)\ttotal: 1m 12s\tremaining: 9m 17s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115: learn: 0.7560248\ttest: 0.72848\tbestTest: 0.7294489 (92)\ttotal: 1m 13s\tremaining: 9m 17s\n",
      "116: learn: 0.7563055\ttest: 0.7286533\tbestTest: 0.7294489 (92)\ttotal: 1m 13s\tremaining: 9m 17s\n",
      "117: learn: 0.756633\ttest: 0.7286242\tbestTest: 0.7294489 (92)\ttotal: 1m 14s\tremaining: 9m 16s\n",
      "118: learn: 0.756834\ttest: 0.728571\tbestTest: 0.7294489 (92)\ttotal: 1m 15s\tremaining: 9m 15s\n",
      "119: learn: 0.757009\ttest: 0.7285168\tbestTest: 0.7294489 (92)\ttotal: 1m 15s\tremaining: 9m 15s\n",
      "120: learn: 0.7574434\ttest: 0.7282824\tbestTest: 0.7294489 (92)\ttotal: 1m 16s\tremaining: 9m 15s\n",
      "121: learn: 0.7575935\ttest: 0.7283632\tbestTest: 0.7294489 (92)\ttotal: 1m 17s\tremaining: 9m 14s\n",
      "122: learn: 0.7577247\ttest: 0.7283805\tbestTest: 0.7294489 (92)\ttotal: 1m 17s\tremaining: 9m 14s\n",
      "123: learn: 0.7577952\ttest: 0.7283234\tbestTest: 0.7294489 (92)\ttotal: 1m 18s\tremaining: 9m 13s\n",
      "124: learn: 0.758034\ttest: 0.7281469\tbestTest: 0.7294489 (92)\ttotal: 1m 18s\tremaining: 9m 12s\n",
      "125: learn: 0.7587283\ttest: 0.7279918\tbestTest: 0.7294489 (92)\ttotal: 1m 19s\tremaining: 9m 11s\n",
      "126: learn: 0.7588642\ttest: 0.7277906\tbestTest: 0.7294489 (92)\ttotal: 1m 20s\tremaining: 9m 10s\n",
      "127: learn: 0.7588857\ttest: 0.7278961\tbestTest: 0.7294489 (92)\ttotal: 1m 20s\tremaining: 9m 10s\n",
      "128: learn: 0.7589711\ttest: 0.7279967\tbestTest: 0.7294489 (92)\ttotal: 1m 21s\tremaining: 9m 8s\n",
      "129: learn: 0.7592759\ttest: 0.7283214\tbestTest: 0.7294489 (92)\ttotal: 1m 21s\tremaining: 9m 7s\n",
      "130: learn: 0.7592875\ttest: 0.7282598\tbestTest: 0.7294489 (92)\ttotal: 1m 22s\tremaining: 9m 6s\n",
      "131: learn: 0.7593651\ttest: 0.7281002\tbestTest: 0.7294489 (92)\ttotal: 1m 23s\tremaining: 9m 6s\n",
      "132: learn: 0.7600212\ttest: 0.7284325\tbestTest: 0.7294489 (92)\ttotal: 1m 23s\tremaining: 9m 6s\n",
      "133: learn: 0.7600176\ttest: 0.7284224\tbestTest: 0.7294489 (92)\ttotal: 1m 24s\tremaining: 9m 5s\n",
      "134: learn: 0.7602019\ttest: 0.7284992\tbestTest: 0.7294489 (92)\ttotal: 1m 24s\tremaining: 9m 4s\n",
      "135: learn: 0.7605427\ttest: 0.7284571\tbestTest: 0.7294489 (92)\ttotal: 1m 25s\tremaining: 9m 3s\n",
      "136: learn: 0.7607868\ttest: 0.7285469\tbestTest: 0.7294489 (92)\ttotal: 1m 26s\tremaining: 9m 2s\n",
      "137: learn: 0.7608005\ttest: 0.7285739\tbestTest: 0.7294489 (92)\ttotal: 1m 26s\tremaining: 9m 1s\n",
      "138: learn: 0.7612596\ttest: 0.728558\tbestTest: 0.7294489 (92)\ttotal: 1m 27s\tremaining: 9m\n",
      "139: learn: 0.7612925\ttest: 0.7285315\tbestTest: 0.7294489 (92)\ttotal: 1m 28s\tremaining: 9m 1s\n",
      "140: learn: 0.7615434\ttest: 0.7286794\tbestTest: 0.7294489 (92)\ttotal: 1m 28s\tremaining: 9m\n",
      "141: learn: 0.7615519\ttest: 0.7287424\tbestTest: 0.7294489 (92)\ttotal: 1m 29s\tremaining: 8m 59s\n",
      "142: learn: 0.7616101\ttest: 0.7286783\tbestTest: 0.7294489 (92)\ttotal: 1m 30s\tremaining: 8m 59s\n",
      "143: learn: 0.761955\ttest: 0.7285995\tbestTest: 0.7294489 (92)\ttotal: 1m 30s\tremaining: 8m 58s\n",
      "144: learn: 0.7619647\ttest: 0.728539\tbestTest: 0.7294489 (92)\ttotal: 1m 31s\tremaining: 8m 58s\n",
      "145: learn: 0.7621581\ttest: 0.7285501\tbestTest: 0.7294489 (92)\ttotal: 1m 31s\tremaining: 8m 57s\n",
      "146: learn: 0.7621936\ttest: 0.7287093\tbestTest: 0.7294489 (92)\ttotal: 1m 32s\tremaining: 8m 56s\n",
      "147: learn: 0.7621941\ttest: 0.7287022\tbestTest: 0.7294489 (92)\ttotal: 1m 33s\tremaining: 8m 55s\n",
      "148: learn: 0.762201\ttest: 0.728692\tbestTest: 0.7294489 (92)\ttotal: 1m 33s\tremaining: 8m 55s\n",
      "149: learn: 0.7622829\ttest: 0.7286593\tbestTest: 0.7294489 (92)\ttotal: 1m 34s\tremaining: 8m 54s\n",
      "150: learn: 0.7624765\ttest: 0.7287813\tbestTest: 0.7294489 (92)\ttotal: 1m 35s\tremaining: 8m 54s\n",
      "151: learn: 0.7624694\ttest: 0.7287677\tbestTest: 0.7294489 (92)\ttotal: 1m 35s\tremaining: 8m 54s\n",
      "152: learn: 0.7628495\ttest: 0.7289059\tbestTest: 0.7294489 (92)\ttotal: 1m 36s\tremaining: 8m 54s\n",
      "153: learn: 0.7628612\ttest: 0.7287792\tbestTest: 0.7294489 (92)\ttotal: 1m 37s\tremaining: 8m 53s\n",
      "154: learn: 0.7629409\ttest: 0.728626\tbestTest: 0.7294489 (92)\ttotal: 1m 37s\tremaining: 8m 52s\n",
      "155: learn: 0.7632213\ttest: 0.728527\tbestTest: 0.7294489 (92)\ttotal: 1m 38s\tremaining: 8m 53s\n",
      "156: learn: 0.763344\ttest: 0.7286997\tbestTest: 0.7294489 (92)\ttotal: 1m 39s\tremaining: 8m 52s\n",
      "157: learn: 0.7636161\ttest: 0.7285473\tbestTest: 0.7294489 (92)\ttotal: 1m 39s\tremaining: 8m 51s\n",
      "158: learn: 0.7637705\ttest: 0.7285653\tbestTest: 0.7294489 (92)\ttotal: 1m 40s\tremaining: 8m 51s\n",
      "159: learn: 0.7637823\ttest: 0.7285808\tbestTest: 0.7294489 (92)\ttotal: 1m 41s\tremaining: 8m 50s\n",
      "160: learn: 0.7639684\ttest: 0.7286832\tbestTest: 0.7294489 (92)\ttotal: 1m 41s\tremaining: 8m 49s\n",
      "161: learn: 0.7641205\ttest: 0.7287304\tbestTest: 0.7294489 (92)\ttotal: 1m 42s\tremaining: 8m 49s\n",
      "162: learn: 0.7641741\ttest: 0.7287787\tbestTest: 0.7294489 (92)\ttotal: 1m 43s\tremaining: 8m 49s\n",
      "163: learn: 0.7642342\ttest: 0.7286646\tbestTest: 0.7294489 (92)\ttotal: 1m 43s\tremaining: 8m 48s\n",
      "164: learn: 0.7642356\ttest: 0.7286654\tbestTest: 0.7294489 (92)\ttotal: 1m 44s\tremaining: 8m 48s\n",
      "165: learn: 0.7644282\ttest: 0.7287138\tbestTest: 0.7294489 (92)\ttotal: 1m 44s\tremaining: 8m 47s\n",
      "166: learn: 0.7645725\ttest: 0.7285041\tbestTest: 0.7294489 (92)\ttotal: 1m 45s\tremaining: 8m 47s\n",
      "167: learn: 0.7645949\ttest: 0.7285274\tbestTest: 0.7294489 (92)\ttotal: 1m 46s\tremaining: 8m 46s\n",
      "168: learn: 0.7647721\ttest: 0.7285146\tbestTest: 0.7294489 (92)\ttotal: 1m 46s\tremaining: 8m 46s\n",
      "169: learn: 0.7651078\ttest: 0.7286644\tbestTest: 0.7294489 (92)\ttotal: 1m 47s\tremaining: 8m 45s\n",
      "170: learn: 0.7653291\ttest: 0.7288712\tbestTest: 0.7294489 (92)\ttotal: 1m 48s\tremaining: 8m 44s\n",
      "171: learn: 0.7653361\ttest: 0.7288769\tbestTest: 0.7294489 (92)\ttotal: 1m 48s\tremaining: 8m 44s\n",
      "172: learn: 0.7656323\ttest: 0.7288239\tbestTest: 0.7294489 (92)\ttotal: 1m 49s\tremaining: 8m 43s\n",
      "173: learn: 0.7657507\ttest: 0.7287904\tbestTest: 0.7294489 (92)\ttotal: 1m 50s\tremaining: 8m 43s\n",
      "174: learn: 0.7659089\ttest: 0.7290835\tbestTest: 0.7294489 (92)\ttotal: 1m 50s\tremaining: 8m 42s\n",
      "175: learn: 0.7659148\ttest: 0.7290097\tbestTest: 0.7294489 (92)\ttotal: 1m 51s\tremaining: 8m 41s\n",
      "176: learn: 0.7661382\ttest: 0.7288743\tbestTest: 0.7294489 (92)\ttotal: 1m 52s\tremaining: 8m 41s\n",
      "177: learn: 0.7663399\ttest: 0.7287588\tbestTest: 0.7294489 (92)\ttotal: 1m 52s\tremaining: 8m 40s\n",
      "178: learn: 0.7663382\ttest: 0.7287182\tbestTest: 0.7294489 (92)\ttotal: 1m 53s\tremaining: 8m 39s\n",
      "179: learn: 0.7663685\ttest: 0.728619\tbestTest: 0.7294489 (92)\ttotal: 1m 53s\tremaining: 8m 38s\n",
      "180: learn: 0.7665509\ttest: 0.7287037\tbestTest: 0.7294489 (92)\ttotal: 1m 54s\tremaining: 8m 37s\n",
      "181: learn: 0.766772\ttest: 0.7288804\tbestTest: 0.7294489 (92)\ttotal: 1m 55s\tremaining: 8m 37s\n",
      "182: learn: 0.7668203\ttest: 0.7288218\tbestTest: 0.7294489 (92)\ttotal: 1m 55s\tremaining: 8m 37s\n",
      "183: learn: 0.766824\ttest: 0.7287999\tbestTest: 0.7294489 (92)\ttotal: 1m 56s\tremaining: 8m 37s\n",
      "184: learn: 0.766929\ttest: 0.7286623\tbestTest: 0.7294489 (92)\ttotal: 1m 57s\tremaining: 8m 37s\n",
      "185: learn: 0.7669052\ttest: 0.7286559\tbestTest: 0.7294489 (92)\ttotal: 1m 58s\tremaining: 8m 36s\n",
      "186: learn: 0.7670735\ttest: 0.7285877\tbestTest: 0.7294489 (92)\ttotal: 1m 59s\tremaining: 8m 38s\n",
      "187: learn: 0.7671271\ttest: 0.7285597\tbestTest: 0.7294489 (92)\ttotal: 2m\tremaining: 8m 38s\n",
      "188: learn: 0.7671735\ttest: 0.7285174\tbestTest: 0.7294489 (92)\ttotal: 2m\tremaining: 8m 38s\n",
      "189: learn: 0.7673252\ttest: 0.7287149\tbestTest: 0.7294489 (92)\ttotal: 2m 1s\tremaining: 8m 37s\n",
      "190: learn: 0.7673357\ttest: 0.728721\tbestTest: 0.7294489 (92)\ttotal: 2m 2s\tremaining: 8m 37s\n",
      "191: learn: 0.7676688\ttest: 0.7288206\tbestTest: 0.7294489 (92)\ttotal: 2m 2s\tremaining: 8m 36s\n",
      "192: learn: 0.7677224\ttest: 0.7287558\tbestTest: 0.7294489 (92)\ttotal: 2m 3s\tremaining: 8m 35s\n",
      "193: learn: 0.7679938\ttest: 0.7287158\tbestTest: 0.7294489 (92)\ttotal: 2m 3s\tremaining: 8m 35s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.7294488897\n",
      "bestIteration = 92\n",
      "\n",
      "Depth :  8\n",
      "Train and Test loss :  0.767993816116 0.728715817279\n",
      "[0.73173024442866696, 0.72712842450385662, 0.7303340797652994, 0.72529170222518347, 0.72871581727894985]\n",
      "0.728532365207\n"
     ]
    }
   ],
   "source": [
    "kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "cv_scores = []\n",
    "pred_test_full = 0\n",
    "pred_val_full = np.zeros(train_X.shape[0])\n",
    "for dev_index, val_index in kf.split(train_X, train_y):\n",
    "    dev_X, val_X = train_X.iloc[dev_index,:], train_X.iloc[val_index,:]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "\n",
    "        \n",
    "    pred_val, loss, pred_test = runCatB(dev_X, dev_y, val_X, val_y, test_X)\n",
    "            \n",
    "    pred_val_full[val_index] = pred_val\n",
    "    pred_test_full = pred_test_full + pred_test\n",
    "    cv_scores.append(loss)\n",
    "    print(cv_scores)\n",
    "pred_test_full /= 5.\n",
    "print(metrics.roc_auc_score(train_y, pred_val_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#0.73409\n",
    "Depth :  8\n",
    "learning_rate = 0.2\n",
    "0.728183167395\n",
    "#0.73516\n",
    "Depth :  8\n",
    "learning_rate = 0.2\n",
    "mse=0.7\n",
    "0.729139632577\n",
    "#0.73356\n",
    "Depth :  7\n",
    "learning_rate = 0.2\n",
    "mse=0.7\n",
    "0.727873109999\n",
    "#\n",
    "Depth :  6\n",
    "learning_rate = 0.2\n",
    "mse=0.7\n",
    "0.728397722185"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame({\"transaction_id\":test_id})\n",
    "out_df[\"target\"] = pred_test_full\n",
    "out_df.to_csv(\"pred_test_v5_CatB_freq_Str(depth8-0.2-0.7).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runET(train_X, train_y, test_X, test_y=None, test_X2=None, depth=20, leaf=10, feat=0.8):\n",
    "    model = ensemble.ExtraTreesClassifier(\n",
    "                                        n_estimators = 100,\n",
    "                                        criterion = 'gini',\n",
    "                                        max_depth = depth,\n",
    "                                        min_samples_split = 10,\n",
    "                                        min_samples_leaf = leaf,\n",
    "                                        warm_start = True,\n",
    "                                        max_features =  feat,\n",
    "                                        #min_impurity_split = 0.1,\n",
    "                                        n_jobs = -1,\n",
    "                                        verbose = 3,\n",
    "                                        random_state = 0)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:,1]\n",
    "    test_preds = model.predict_proba(test_X)[:,1]\n",
    "    test_preds2 = model.predict_proba(test_X2)[:,1]\n",
    "    test_loss = 0\n",
    "    if test_y is not None:\n",
    "        train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "        test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "        print(\"Depth, leaf, feat : \", depth, leaf, feat)\n",
    "        print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 2 of 100\n",
      "building tree 1 of 100building tree 3 of 100\n",
      "\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   29.9s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    2.6s finished\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    4.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth, leaf, feat :  20 10 0.8\n",
      "Train and Test loss :  0.858975958905 0.732088511145\n",
      "[0.73208851114492313]\n",
      "building tree 2 of 100building tree 1 of 100building tree 3 of 100\n",
      "\n",
      "\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   28.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.9min finished\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    5.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth, leaf, feat :  20 10 0.8\n",
      "Train and Test loss :  0.8549893778 0.729937396015\n",
      "[0.73208851114492313, 0.72993739601453644]\n",
      "building tree 1 of 100building tree 2 of 100building tree 3 of 100\n",
      "\n",
      "\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n",
      "building tree 28 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   32.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.1min finished\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    2.7s finished\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    4.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth, leaf, feat :  20 10 0.8\n",
      "Train and Test loss :  0.85424204951 0.723315051056\n",
      "[0.73208851114492313, 0.72993739601453644, 0.72331505105579152]\n",
      "building tree 1 of 100building tree 2 of 100building tree 3 of 100\n",
      "\n",
      "\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   32.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    5.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth, leaf, feat :  20 10 0.8\n",
      "Train and Test loss :  0.855778978686 0.725414337358\n",
      "[0.73208851114492313, 0.72993739601453644, 0.72331505105579152, 0.72541433735838246]\n",
      "building tree 1 of 100building tree 2 of 100\n",
      "\n",
      "building tree 3 of 100\n",
      "building tree 4 of 100\n",
      "building tree 5 of 100\n",
      "building tree 6 of 100\n",
      "building tree 7 of 100\n",
      "building tree 8 of 100\n",
      "building tree 9 of 100\n",
      "building tree 10 of 100\n",
      "building tree 11 of 100\n",
      "building tree 12 of 100\n",
      "building tree 13 of 100\n",
      "building tree 14 of 100\n",
      "building tree 15 of 100\n",
      "building tree 16 of 100\n",
      "building tree 17 of 100\n",
      "building tree 18 of 100\n",
      "building tree 19 of 100\n",
      "building tree 20 of 100\n",
      "building tree 21 of 100\n",
      "building tree 22 of 100\n",
      "building tree 23 of 100\n",
      "building tree 24 of 100\n",
      "building tree 25 of 100\n",
      "building tree 26 of 100\n",
      "building tree 27 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   30.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tree 28 of 100\n",
      "building tree 29 of 100\n",
      "building tree 30 of 100\n",
      "building tree 31 of 100\n",
      "building tree 32 of 100\n",
      "building tree 33 of 100\n",
      "building tree 34 of 100\n",
      "building tree 35 of 100\n",
      "building tree 36 of 100\n",
      "building tree 37 of 100\n",
      "building tree 38 of 100\n",
      "building tree 39 of 100\n",
      "building tree 40 of 100\n",
      "building tree 41 of 100\n",
      "building tree 42 of 100\n",
      "building tree 43 of 100\n",
      "building tree 44 of 100\n",
      "building tree 45 of 100\n",
      "building tree 46 of 100\n",
      "building tree 47 of 100\n",
      "building tree 48 of 100\n",
      "building tree 49 of 100\n",
      "building tree 50 of 100\n",
      "building tree 51 of 100\n",
      "building tree 52 of 100\n",
      "building tree 53 of 100\n",
      "building tree 54 of 100\n",
      "building tree 55 of 100\n",
      "building tree 56 of 100\n",
      "building tree 57 of 100\n",
      "building tree 58 of 100\n",
      "building tree 59 of 100\n",
      "building tree 60 of 100\n",
      "building tree 61 of 100\n",
      "building tree 62 of 100\n",
      "building tree 63 of 100\n",
      "building tree 64 of 100\n",
      "building tree 65 of 100\n",
      "building tree 66 of 100\n",
      "building tree 67 of 100\n",
      "building tree 68 of 100\n",
      "building tree 69 of 100\n",
      "building tree 70 of 100\n",
      "building tree 71 of 100\n",
      "building tree 72 of 100\n",
      "building tree 73 of 100\n",
      "building tree 74 of 100\n",
      "building tree 75 of 100\n",
      "building tree 76 of 100\n",
      "building tree 77 of 100\n",
      "building tree 78 of 100\n",
      "building tree 79 of 100\n",
      "building tree 80 of 100\n",
      "building tree 81 of 100\n",
      "building tree 82 of 100\n",
      "building tree 83 of 100\n",
      "building tree 84 of 100\n",
      "building tree 85 of 100\n",
      "building tree 86 of 100\n",
      "building tree 87 of 100\n",
      "building tree 88 of 100\n",
      "building tree 89 of 100\n",
      "building tree 90 of 100\n",
      "building tree 91 of 100\n",
      "building tree 92 of 100\n",
      "building tree 93 of 100\n",
      "building tree 94 of 100\n",
      "building tree 95 of 100\n",
      "building tree 96 of 100\n",
      "building tree 97 of 100\n",
      "building tree 98 of 100\n",
      "building tree 99 of 100\n",
      "building tree 100 of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.0min finished\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    2.5s finished\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:    1.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth, leaf, feat :  20 10 0.8\n",
      "Train and Test loss :  0.853413704547 0.72798137251\n",
      "[0.73208851114492313, 0.72993739601453644, 0.72331505105579152, 0.72541433735838246, 0.72798137251044004]\n",
      "0.727715212136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    4.9s finished\n"
     ]
    }
   ],
   "source": [
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "cv_scores = []\n",
    "pred_test_full = 0\n",
    "pred_val_full = np.zeros(train_X.shape[0])\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_X.iloc[dev_index,:], train_X.iloc[val_index,:]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "\n",
    "        \n",
    "    pred_val, loss, pred_test = runET(dev_X, dev_y, val_X, val_y, test_X)\n",
    "            \n",
    "    pred_val_full[val_index] = pred_val\n",
    "    pred_test_full = pred_test_full + pred_test\n",
    "    cv_scores.append(loss)\n",
    "    print(cv_scores)\n",
    "pred_test_full /= 5.\n",
    "print(metrics.roc_auc_score(train_y, pred_val_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame({\"transaction_id\":test_id})\n",
    "out_df[\"target\"] = pred_test_full\n",
    "out_df.to_csv(\"pred_test_v5_ET.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def runADA(train_X, train_y, test_X, test_y=None, test_X2=None, depth=6):\n",
    "    model = AdaBoostClassifier(DecisionTreeClassifier(max_depth=depth, max_features=0.7, min_samples_leaf=5),\n",
    "                               n_estimators = 300,\n",
    "                               learning_rate = 0.2,\n",
    "                               random_state=42)\n",
    "    \n",
    "    model.fit(train_X, train_y)#, eval_set=(test_X, test_y))#, plot=True)\n",
    "    train_preds = model.predict_proba(train_X)[:,1]\n",
    "    test_preds = model.predict_proba(test_X)[:,1]\n",
    "    test_preds2 = model.predict_proba(test_X2)[:,1]\n",
    "    test_loss = 0\n",
    "    if test_y is not None:\n",
    "        train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "        test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "        print(\"Depth : \", depth)\n",
    "        print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth :  6\n",
      "Train and Test loss :  0.832458583785 0.710091525144\n",
      "[0.7100915251436426]\n",
      "Depth :  6\n",
      "Train and Test loss :  0.834312921178 0.709692699578\n",
      "[0.7100915251436426, 0.70969269957816228]\n",
      "Depth :  6\n",
      "Train and Test loss :  0.833311675136 0.709264199389\n",
      "[0.7100915251436426, 0.70969269957816228, 0.70926419938928842]\n",
      "Depth :  6\n",
      "Train and Test loss :  0.83326242463 0.70249562109\n",
      "[0.7100915251436426, 0.70969269957816228, 0.70926419938928842, 0.70249562108972763]\n",
      "Depth :  6\n",
      "Train and Test loss :  0.835442925819 0.707837409163\n",
      "[0.7100915251436426, 0.70969269957816228, 0.70926419938928842, 0.70249562108972763, 0.70783740916310123]\n",
      "0.7078584866\n"
     ]
    }
   ],
   "source": [
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2018)\n",
    "cv_scores = []\n",
    "pred_test_full = 0\n",
    "pred_val_full = np.zeros(train_X.shape[0])\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_X.iloc[dev_index,:], train_X.iloc[val_index,:]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "\n",
    "        \n",
    "    pred_val, loss, pred_test = runADA(dev_X, dev_y, val_X, val_y, test_X)\n",
    "            \n",
    "    pred_val_full[val_index] = pred_val\n",
    "    pred_test_full = pred_test_full + pred_test\n",
    "    cv_scores.append(loss)\n",
    "    print(cv_scores)\n",
    "pred_test_full /= 5.\n",
    "print(metrics.roc_auc_score(train_y, pred_val_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
